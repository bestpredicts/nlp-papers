{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import struct\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from multiprocessing import Pool, Value, Array\n",
    "\n",
    "class VocabItem:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.count = 0\n",
    "        self.path = None # Path (list of indices) from the root to the word (leaf)\n",
    "        self.code = None # Huffman encoding\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, fi, min_count):\n",
    "        vocab_items = []\n",
    "        vocab_hash = {}\n",
    "        word_count = 0\n",
    "        fi = open(fi, 'r')\n",
    "\n",
    "        # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "        for token in ['<bol>', '<eol>']:\n",
    "            vocab_hash[token] = len(vocab_items)\n",
    "            vocab_items.append(VocabItem(token))\n",
    "\n",
    "        for line in fi:\n",
    "            tokens = line.split()\n",
    "            for token in tokens:\n",
    "                if token not in vocab_hash:\n",
    "                    vocab_hash[token] = len(vocab_items)\n",
    "                    vocab_items.append(VocabItem(token))\n",
    "                    \n",
    "                #assert vocab_items[vocab_hash[token]].word == token, 'Wrong vocab_hash index'\n",
    "                vocab_items[vocab_hash[token]].count += 1\n",
    "                word_count += 1\n",
    "            \n",
    "                if word_count % 10000 == 0:\n",
    "                    sys.stdout.write(\"\\rReading word %d\" % word_count)\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "            vocab_items[vocab_hash['<bol>']].count += 1\n",
    "            vocab_items[vocab_hash['<eol>']].count += 1\n",
    "            word_count += 2\n",
    "\n",
    "        self.bytes = fi.tell()\n",
    "        self.vocab_items = vocab_items         # List of VocabItem objects\n",
    "        self.vocab_hash = vocab_hash           # Mapping from each token to its index in vocab\n",
    "        self.word_count = word_count           # Total number of words in train file\n",
    "\n",
    "        # Add special token <unk> (unknown),\n",
    "        # merge words occurring less than min_count into <unk>, and\n",
    "        # sort vocab in descending order by frequency in train file\n",
    "        self.__sort(min_count)\n",
    "\n",
    "        #assert self.word_count == sum([t.count for t in self.vocab_items]), 'word_count and sum of t.count do not agree'\n",
    "        print 'Total words in training file: %d' % self.word_count\n",
    "        print 'Total bytes in training file: %d' % self.bytes\n",
    "        print 'Vocab size: %d' % len(self)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.vocab_items[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_items)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vocab_items)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.vocab_hash\n",
    "\n",
    "    def __sort(self, min_count):\n",
    "        tmp = []\n",
    "        tmp.append(VocabItem('<unk>'))\n",
    "        unk_hash = 0\n",
    "        \n",
    "        count_unk = 0\n",
    "        for token in self.vocab_items:\n",
    "            if token.count < min_count:\n",
    "                count_unk += 1\n",
    "                tmp[unk_hash].count += token.count\n",
    "            else:\n",
    "                tmp.append(token)\n",
    "\n",
    "        tmp.sort(key=lambda token : token.count, reverse=True)\n",
    "\n",
    "        # Update vocab_hash\n",
    "        vocab_hash = {}\n",
    "        for i, token in enumerate(tmp):\n",
    "            vocab_hash[token.word] = i\n",
    "\n",
    "        self.vocab_items = tmp\n",
    "        self.vocab_hash = vocab_hash\n",
    "\n",
    "        print\n",
    "        print 'Unknown vocab size:', count_unk\n",
    "\n",
    "    def indices(self, tokens):\n",
    "        return [self.vocab_hash[token] if token in self else self.vocab_hash['<unk>'] for token in tokens]\n",
    "\n",
    "    def encode_huffman(self):\n",
    "        # Build a Huffman tree\n",
    "        vocab_size = len(self)\n",
    "        count = [t.count for t in self] + [1e15] * (vocab_size - 1)\n",
    "        parent = [0] * (2 * vocab_size - 2)\n",
    "        binary = [0] * (2 * vocab_size - 2)\n",
    "        \n",
    "        pos1 = vocab_size - 1\n",
    "        pos2 = vocab_size\n",
    "\n",
    "        for i in xrange(vocab_size - 1):\n",
    "            # Find min1\n",
    "            if pos1 >= 0:\n",
    "                if count[pos1] < count[pos2]:\n",
    "                    min1 = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min1 = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min1 = pos2\n",
    "                pos2 += 1\n",
    "\n",
    "            # Find min2\n",
    "            if pos1 >= 0:\n",
    "                if count[pos1] < count[pos2]:\n",
    "                    min2 = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min2 = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min2 = pos2\n",
    "                pos2 += 1\n",
    "\n",
    "            count[vocab_size + i] = count[min1] + count[min2]\n",
    "            parent[min1] = vocab_size + i\n",
    "            parent[min2] = vocab_size + i\n",
    "            binary[min2] = 1\n",
    "\n",
    "        # Assign binary code and path pointers to each vocab word\n",
    "        root_idx = 2 * vocab_size - 2\n",
    "        for i, token in enumerate(self):\n",
    "            path = [] # List of indices from the leaf to the root\n",
    "            code = [] # Binary Huffman encoding from the leaf to the root\n",
    "\n",
    "            node_idx = i\n",
    "            while node_idx < root_idx:\n",
    "                if node_idx >= vocab_size: path.append(node_idx)\n",
    "                code.append(binary[node_idx])\n",
    "                node_idx = parent[node_idx]\n",
    "            path.append(root_idx)\n",
    "\n",
    "            # These are path and code from the root to the leaf\n",
    "            token.path = [j - vocab_size for j in path[::-1]]\n",
    "            token.code = code[::-1]\n",
    "\n",
    "class UnigramTable:\n",
    "    \"\"\"\n",
    "    A list of indices of tokens in the vocab following a power law distribution,\n",
    "    used to draw negative samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        vocab_size = len(vocab)\n",
    "        power = 0.75\n",
    "        norm = sum([math.pow(t.count, power) for t in vocab]) # Normalizing constant\n",
    "\n",
    "        table_size = 1e8 # Length of the unigram table\n",
    "        table = np.zeros(table_size, dtype=np.uint32)\n",
    "\n",
    "        print 'Filling unigram table'\n",
    "        p = 0 # Cumulative probability\n",
    "        i = 0\n",
    "        for j, unigram in enumerate(vocab):\n",
    "            p += float(math.pow(unigram.count, power))/norm\n",
    "            while i < table_size and float(i) / table_size < p:\n",
    "                table[i] = j\n",
    "                i += 1\n",
    "        self.table = table\n",
    "\n",
    "    def sample(self, count):\n",
    "        indices = np.random.randint(low=0, high=len(self.table), size=count)\n",
    "        return [self.table[i] for i in indices]\n",
    "\n",
    "def sigmoid(z):\n",
    "    if z > 6:\n",
    "        return 1.0\n",
    "    elif z < -6:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "\n",
    "def init_net(dim, vocab_size):\n",
    "    # Init syn0 with random numbers from a uniform distribution on the interval [-0.5, 0.5]/dim\n",
    "    tmp = np.random.uniform(low=-0.5/dim, high=0.5/dim, size=(vocab_size, dim))\n",
    "    syn0 = np.ctypeslib.as_ctypes(tmp)\n",
    "    syn0 = Array(syn0._type_, syn0, lock=False)\n",
    "\n",
    "    # Init syn1 with zeros\n",
    "    tmp = np.zeros(shape=(vocab_size, dim))\n",
    "    syn1 = np.ctypeslib.as_ctypes(tmp)\n",
    "    syn1 = Array(syn1._type_, syn1, lock=False)\n",
    "\n",
    "    return (syn0, syn1)\n",
    "\n",
    "def train_process(pid):\n",
    "    # Set fi to point to the right chunk of training file\n",
    "    start = vocab.bytes / num_processes * pid\n",
    "    end = vocab.bytes if pid == num_processes - 1 else vocab.bytes / num_processes * (pid + 1)\n",
    "    fi.seek(start)\n",
    "    #print 'Worker %d beginning training at %d, ending at %d' % (pid, start, end)\n",
    "\n",
    "    alpha = starting_alpha\n",
    "\n",
    "    word_count = 0\n",
    "    last_word_count = 0\n",
    "\n",
    "    while fi.tell() < end:\n",
    "        line = fi.readline().strip()\n",
    "        # Skip blank lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Init sent, a list of indices of words in line\n",
    "        sent = vocab.indices(['<bol>'] + line.split() + ['<eol>'])\n",
    "\n",
    "        for sent_pos, token in enumerate(sent):\n",
    "            if word_count % 10000 == 0:\n",
    "                global_word_count.value += (word_count - last_word_count)\n",
    "                last_word_count = word_count\n",
    "\n",
    "                # Recalculate alpha\n",
    "                alpha = starting_alpha * (1 - float(global_word_count.value) / vocab.word_count)\n",
    "                if alpha < starting_alpha * 0.0001: alpha = starting_alpha * 0.0001\n",
    "\n",
    "                # Print progress info\n",
    "                sys.stdout.write(\"\\rAlpha: %f Progress: %d of %d (%.2f%%)\" %\n",
    "                                 (alpha, global_word_count.value, vocab.word_count,\n",
    "                                  float(global_word_count.value) / vocab.word_count * 100))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # Randomize window size, where win is the max window size\n",
    "            current_win = np.random.randint(low=1, high=win+1)\n",
    "            context_start = max(sent_pos - current_win, 0)\n",
    "            context_end = min(sent_pos + current_win + 1, len(sent))\n",
    "            context = sent[context_start:sent_pos] + sent[sent_pos+1:context_end] # Turn into an iterator?\n",
    "\n",
    "            # CBOW\n",
    "            if cbow:\n",
    "                # Compute neu1\n",
    "                neu1 = np.mean(np.array([syn0[c] for c in context]), axis=0)\n",
    "                assert len(neu1) == dim, 'neu1 and dim do not agree'\n",
    "\n",
    "                # Init neu1e with zeros\n",
    "                neu1e = np.zeros(dim)\n",
    "\n",
    "                # Compute neu1e and update syn1\n",
    "                if neg > 0:\n",
    "                    classifiers = [(token, 1)] + [(target, 0) for target in table.sample(neg)]\n",
    "                else:\n",
    "                    classifiers = zip(vocab[token].path, vocab[token].code)\n",
    "                for target, label in classifiers:\n",
    "                    z = np.dot(neu1, syn1[target])\n",
    "                    p = sigmoid(z)\n",
    "                    g = alpha * (label - p)\n",
    "                    neu1e += g * syn1[target] # Error to backpropagate to syn0\n",
    "                    syn1[target] += g * neu1  # Update syn1\n",
    "\n",
    "                # Update syn0\n",
    "                for context_word in context:\n",
    "                    syn0[context_word] += neu1e\n",
    "\n",
    "            # Skip-gram\n",
    "            else:\n",
    "                for context_word in context:\n",
    "                    # Init neu1e with zeros\n",
    "                    neu1e = np.zeros(dim)\n",
    "\n",
    "                    # Compute neu1e and update syn1\n",
    "                    if neg > 0:\n",
    "                        classifiers = [(token, 1)] + [(target, 0) for target in table.sample(neg)]\n",
    "                    else:\n",
    "                        classifiers = zip(vocab[token].path, vocab[token].code)\n",
    "                    for target, label in classifiers:\n",
    "                        z = np.dot(syn0[context_word], syn1[target])\n",
    "                        p = sigmoid(z)\n",
    "                        g = alpha * (label - p)\n",
    "                        neu1e += g * syn1[target]              # Error to backpropagate to syn0\n",
    "                        syn1[target] += g * syn0[context_word] # Update syn1\n",
    "\n",
    "                    # Update syn0\n",
    "                    syn0[context_word] += neu1e\n",
    "\n",
    "            word_count += 1\n",
    "\n",
    "    # Print progress info\n",
    "    global_word_count.value += (word_count - last_word_count)\n",
    "    sys.stdout.write(\"\\rAlpha: %f Progress: %d of %d (%.2f%%)\" %\n",
    "                     (alpha, global_word_count.value, vocab.word_count,\n",
    "                      float(global_word_count.value)/vocab.word_count * 100))\n",
    "    sys.stdout.flush()\n",
    "    fi.close()\n",
    "\n",
    "def save(vocab, syn0, fo, binary):\n",
    "    print 'Saving model to', fo\n",
    "    dim = len(syn0[0])\n",
    "    if binary:\n",
    "        fo = open(fo, 'wb')\n",
    "        fo.write('%d %d\\n' % (len(syn0), dim))\n",
    "        fo.write('\\n')\n",
    "        for token, vector in zip(vocab, syn0):\n",
    "            fo.write('%s ' % token.word)\n",
    "            for s in vector:\n",
    "                fo.write(struct.pack('f', s))\n",
    "            fo.write('\\n')\n",
    "    else:\n",
    "        fo = open(fo, 'w')\n",
    "        fo.write('%d %d\\n' % (len(syn0), dim))\n",
    "        for token, vector in zip(vocab, syn0):\n",
    "            word = token.word\n",
    "            vector_str = ' '.join([str(s) for s in vector])\n",
    "            fo.write('%s %s\\n' % (word, vector_str))\n",
    "\n",
    "    fo.close()\n",
    "\n",
    "def __init_process(*args):\n",
    "    global vocab, syn0, syn1, table, cbow, neg, dim, starting_alpha\n",
    "    global win, num_processes, global_word_count, fi\n",
    "    \n",
    "    vocab, syn0_tmp, syn1_tmp, table, cbow, neg, dim, starting_alpha, win, num_processes, global_word_count = args[:-1]\n",
    "    fi = open(args[-1], 'r')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', RuntimeWarning)\n",
    "        syn0 = np.ctypeslib.as_array(syn0_tmp)\n",
    "        syn1 = np.ctypeslib.as_array(syn1_tmp)\n",
    "\n",
    "def train(fi, fo, cbow, neg, dim, alpha, win, min_count, num_processes, binary):\n",
    "    # Read train file to init vocab\n",
    "    vocab = Vocab(fi, min_count)\n",
    "\n",
    "    # Init net\n",
    "    syn0, syn1 = init_net(dim, len(vocab))\n",
    "\n",
    "    global_word_count = Value('i', 0)\n",
    "    table = None\n",
    "    if neg > 0:\n",
    "        print 'Initializing unigram table'\n",
    "        table = UnigramTable(vocab)\n",
    "    else:\n",
    "        print 'Initializing Huffman tree'\n",
    "        vocab.encode_huffman()\n",
    "\n",
    "    # Begin training using num_processes workers\n",
    "    t0 = time.time()\n",
    "    pool = Pool(processes=num_processes, initializer=__init_process,\n",
    "                initargs=(vocab, syn0, syn1, table, cbow, neg, dim, alpha,\n",
    "                          win, num_processes, global_word_count, fi))\n",
    "    pool.map(train_process, range(num_processes))\n",
    "    t1 = time.time()\n",
    "    print\n",
    "    print 'Completed training. Training took', (t1 - t0) / 60, 'minutes'\n",
    "\n",
    "    # Save model to file\n",
    "    save(vocab, syn0, fo, binary)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-train', help='Training file', dest='fi', required=True)\n",
    "    parser.add_argument('-model', help='Output model file', dest='fo', required=True)\n",
    "    parser.add_argument('-cbow', help='1 for CBOW, 0 for skip-gram', dest='cbow', default=1, type=int)\n",
    "    parser.add_argument('-negative', help='Number of negative examples (>0) for negative sampling, 0 for hierarchical softmax', dest='neg', default=5, type=int)\n",
    "    parser.add_argument('-dim', help='Dimensionality of word embeddings', dest='dim', default=100, type=int)\n",
    "    parser.add_argument('-alpha', help='Starting alpha', dest='alpha', default=0.025, type=float)\n",
    "    parser.add_argument('-window', help='Max window length', dest='win', default=5, type=int) \n",
    "    parser.add_argument('-min-count', help='Min count for words used to learn <unk>', dest='min_count', default=5, type=int)\n",
    "    parser.add_argument('-processes', help='Number of processes', dest='num_processes', default=1, type=int)\n",
    "    parser.add_argument('-binary', help='1 for output model in binary format, 0 otherwise', dest='binary', default=0, type=int)\n",
    "    #TO DO: parser.add_argument('-epoch', help='Number of training epochs', dest='epoch', default=1, type=int)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train(args.fi, args.fo, bool(args.cbow), args.neg, args.dim, args.alpha, args.win,\n",
    "          args.min_count, args.num_processes, bool(args.binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
