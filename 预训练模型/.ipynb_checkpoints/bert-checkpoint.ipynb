{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hf api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:26.245376Z",
     "start_time": "2021-02-10T14:49:26.129660Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019-present, the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "__version__ = \"4.3.2\"\n",
    "\n",
    "\n",
    "import io\n",
    "import os\n",
    "from os.path import expanduser\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "ENDPOINT = \"https://huggingface.co\"\n",
    "\n",
    "\n",
    "class RepoObj:\n",
    "    \"\"\"\n",
    "    HuggingFace git-based system, data structure that represents a file belonging to the current user.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, lastModified: str, commit: str, size: int, **kwargs):\n",
    "        self.filename = filename\n",
    "        self.lastModified = lastModified\n",
    "        self.commit = commit\n",
    "        self.size = size\n",
    "\n",
    "\n",
    "class S3Obj:\n",
    "    \"\"\"\n",
    "    HuggingFace S3-based system, data structure that represents a file belonging to the current user.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, LastModified: str, ETag: str, Size: int, **kwargs):\n",
    "        self.filename = filename\n",
    "        self.LastModified = LastModified\n",
    "        self.ETag = ETag\n",
    "        self.Size = Size\n",
    "\n",
    "\n",
    "class PresignedUrl:\n",
    "    def __init__(self, write: str, access: str, type: str, **kwargs):\n",
    "        self.write = write\n",
    "        self.access = access\n",
    "        self.type = type  # mime-type to send to S3.\n",
    "\n",
    "\n",
    "class ModelSibling:\n",
    "    \"\"\"\n",
    "    Data structure that represents a public file inside a model, accessible from huggingface.co\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rfilename: str, **kwargs):\n",
    "        self.rfilename = rfilename  # filename relative to the model root\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class ModelInfo:\n",
    "    \"\"\"\n",
    "    Info about a public model accessible from huggingface.co\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modelId: Optional[str] = None,  # id of model\n",
    "        author: Optional[str] = None,\n",
    "        downloads: Optional[int] = None,\n",
    "        tags: List[str] = [],\n",
    "        pipeline_tag: Optional[str] = None,\n",
    "        siblings: Optional[List[Dict]] = None,  # list of files that constitute the model\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.modelId = modelId\n",
    "        self.author = author\n",
    "        self.downloads = downloads\n",
    "        self.tags = tags\n",
    "        self.pipeline_tag = pipeline_tag\n",
    "        self.siblings = [ModelSibling(**x) for x in siblings] if siblings is not None else None\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class HfApi:\n",
    "    ALLOWED_S3_FILE_TYPES = [\"datasets\", \"metrics\"]\n",
    "\n",
    "    def __init__(self, endpoint=None):\n",
    "        self.endpoint = endpoint if endpoint is not None else ENDPOINT\n",
    "\n",
    "    def login(self, username: str, password: str) -> str:\n",
    "        \"\"\"\n",
    "        Call HF API to sign in a user and get a token if credentials are valid.\n",
    "\n",
    "        Outputs: token if credentials are valid\n",
    "\n",
    "        Throws: requests.exceptions.HTTPError if credentials are invalid\n",
    "        \"\"\"\n",
    "        path = \"{}/api/login\".format(self.endpoint)\n",
    "        r = requests.post(path, json={\"username\": username, \"password\": password})\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return d[\"token\"]\n",
    "\n",
    "    def whoami(self, token: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Call HF API to know \"whoami\"\n",
    "        \"\"\"\n",
    "        path = \"{}/api/whoami\".format(self.endpoint)\n",
    "        r = requests.get(path, headers={\"authorization\": \"Bearer {}\".format(token)})\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return d[\"user\"], d[\"orgs\"]\n",
    "\n",
    "    def logout(self, token: str) -> None:\n",
    "        \"\"\"\n",
    "        Call HF API to log out.\n",
    "        \"\"\"\n",
    "        path = \"{}/api/logout\".format(self.endpoint)\n",
    "        r = requests.post(path, headers={\"authorization\": \"Bearer {}\".format(token)})\n",
    "        r.raise_for_status()\n",
    "\n",
    "    def presign(self, token: str, filetype: str, filename: str, organization: Optional[str] = None) -> PresignedUrl:\n",
    "        \"\"\"\n",
    "        HuggingFace S3-based system, used for datasets and metrics.\n",
    "\n",
    "        Call HF API to get a presigned url to upload `filename` to S3.\n",
    "        \"\"\"\n",
    "        assert filetype in self.ALLOWED_S3_FILE_TYPES, f\"Please specify filetype from {self.ALLOWED_S3_FILE_TYPES}\"\n",
    "        path = f\"{self.endpoint}/api/{filetype}/presign\"\n",
    "        r = requests.post(\n",
    "            path,\n",
    "            headers={\"authorization\": \"Bearer {}\".format(token)},\n",
    "            json={\"filename\": filename, \"organization\": organization},\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return PresignedUrl(**d)\n",
    "\n",
    "    def presign_and_upload(\n",
    "        self, token: str, filetype: str, filename: str, filepath: str, organization: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HuggingFace S3-based system, used for datasets and metrics.\n",
    "\n",
    "        Get a presigned url, then upload file to S3.\n",
    "\n",
    "        Outputs: url: Read-only url for the stored file on S3.\n",
    "        \"\"\"\n",
    "        assert filetype in self.ALLOWED_S3_FILE_TYPES, f\"Please specify filetype from {self.ALLOWED_S3_FILE_TYPES}\"\n",
    "        urls = self.presign(token, filetype=filetype, filename=filename, organization=organization)\n",
    "        # streaming upload:\n",
    "        # https://2.python-requests.org/en/master/user/advanced/#streaming-uploads\n",
    "        #\n",
    "        # Even though we presign with the correct content-type,\n",
    "        # the client still has to specify it when uploading the file.\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            pf = TqdmProgressFileReader(f)\n",
    "            data = f if pf.total_size > 0 else \"\"\n",
    "\n",
    "            r = requests.put(urls.write, data=data, headers={\"content-type\": urls.type})\n",
    "            r.raise_for_status()\n",
    "            pf.close()\n",
    "        return urls.access\n",
    "\n",
    "    def list_objs(self, token: str, filetype: str, organization: Optional[str] = None) -> List[S3Obj]:\n",
    "        \"\"\"\n",
    "        HuggingFace S3-based system, used for datasets and metrics.\n",
    "\n",
    "        Call HF API to list all stored files for user (or one of their organizations).\n",
    "        \"\"\"\n",
    "        assert filetype in self.ALLOWED_S3_FILE_TYPES, f\"Please specify filetype from {self.ALLOWED_S3_FILE_TYPES}\"\n",
    "        path = \"{}/api/{}/listObjs\".format(self.endpoint, filetype)\n",
    "        params = {\"organization\": organization} if organization is not None else None\n",
    "        r = requests.get(path, params=params, headers={\"authorization\": \"Bearer {}\".format(token)})\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return [S3Obj(**x) for x in d]\n",
    "\n",
    "    def delete_obj(self, token: str, filetype: str, filename: str, organization: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        HuggingFace S3-based system, used for datasets and metrics.\n",
    "\n",
    "        Call HF API to delete a file stored by user\n",
    "        \"\"\"\n",
    "        assert filetype in self.ALLOWED_S3_FILE_TYPES, f\"Please specify filetype from {self.ALLOWED_S3_FILE_TYPES}\"\n",
    "        path = \"{}/api/{}/deleteObj\".format(self.endpoint, filetype)\n",
    "        r = requests.delete(\n",
    "            path,\n",
    "            headers={\"authorization\": \"Bearer {}\".format(token)},\n",
    "            json={\"filename\": filename, \"organization\": organization},\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "\n",
    "    def model_list(self) -> List[ModelInfo]:\n",
    "        \"\"\"\n",
    "        Get the public list of all the models on huggingface.co\n",
    "        \"\"\"\n",
    "        path = \"{}/api/models\".format(self.endpoint)\n",
    "        r = requests.get(path)\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return [ModelInfo(**x) for x in d]\n",
    "\n",
    "    def list_repos_objs(self, token: str, organization: Optional[str] = None) -> List[RepoObj]:\n",
    "        \"\"\"\n",
    "        HuggingFace git-based system, used for models.\n",
    "\n",
    "        Call HF API to list all stored files for user (or one of their organizations).\n",
    "        \"\"\"\n",
    "        path = \"{}/api/repos/ls\".format(self.endpoint)\n",
    "        params = {\"organization\": organization} if organization is not None else None\n",
    "        r = requests.get(path, params=params, headers={\"authorization\": \"Bearer {}\".format(token)})\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return [RepoObj(**x) for x in d]\n",
    "\n",
    "    def create_repo(\n",
    "        self,\n",
    "        token: str,\n",
    "        name: str,\n",
    "        organization: Optional[str] = None,\n",
    "        private: Optional[bool] = None,\n",
    "        exist_ok=False,\n",
    "        lfsmultipartthresh: Optional[int] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HuggingFace git-based system, used for models.\n",
    "\n",
    "        Call HF API to create a whole repo.\n",
    "\n",
    "        Params:\n",
    "            private: Whether the model repo should be private (requires a paid huggingface.co account)\n",
    "\n",
    "            exist_ok: Do not raise an error if repo already exists\n",
    "\n",
    "            lfsmultipartthresh: Optional: internal param for testing purposes.\n",
    "        \"\"\"\n",
    "        path = \"{}/api/repos/create\".format(self.endpoint)\n",
    "        json = {\"name\": name, \"organization\": organization, \"private\": private}\n",
    "        if lfsmultipartthresh is not None:\n",
    "            json[\"lfsmultipartthresh\"] = lfsmultipartthresh\n",
    "        r = requests.post(\n",
    "            path,\n",
    "            headers={\"authorization\": \"Bearer {}\".format(token)},\n",
    "            json=json,\n",
    "        )\n",
    "        if exist_ok and r.status_code == 409:\n",
    "            return \"\"\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return d[\"url\"]\n",
    "\n",
    "    def delete_repo(self, token: str, name: str, organization: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        HuggingFace git-based system, used for models.\n",
    "\n",
    "        Call HF API to delete a whole repo.\n",
    "\n",
    "        CAUTION(this is irreversible).\n",
    "        \"\"\"\n",
    "        path = \"{}/api/repos/delete\".format(self.endpoint)\n",
    "        r = requests.delete(\n",
    "            path,\n",
    "            headers={\"authorization\": \"Bearer {}\".format(token)},\n",
    "            json={\"name\": name, \"organization\": organization},\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "class TqdmProgressFileReader:\n",
    "    \"\"\"\n",
    "    Wrap an io.BufferedReader `f` (such as the output of `open(…, \"rb\")`) and override `f.read()` so as to display a\n",
    "    tqdm progress bar.\n",
    "\n",
    "    see github.com/huggingface/transformers/pull/2078#discussion_r354739608 for implementation details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f: io.BufferedReader):\n",
    "        self.f = f\n",
    "        self.total_size = os.fstat(f.fileno()).st_size\n",
    "        self.pbar = tqdm(total=self.total_size, leave=False)\n",
    "        self.read = f.read\n",
    "        f.read = self._read\n",
    "\n",
    "    def _read(self, n=-1):\n",
    "        self.pbar.update(n)\n",
    "        return self.read(n)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "class HfFolder:\n",
    "    path_token = expanduser(\"~/.huggingface/token\")\n",
    "\n",
    "    @classmethod\n",
    "    def save_token(cls, token):\n",
    "        \"\"\"\n",
    "        Save token, creating folder as needed.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(cls.path_token), exist_ok=True)\n",
    "        with open(cls.path_token, \"w+\") as f:\n",
    "            f.write(token)\n",
    "\n",
    "    @classmethod\n",
    "    def get_token(cls):\n",
    "        \"\"\"\n",
    "        Get token or None if not existent.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(cls.path_token, \"r\") as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    @classmethod\n",
    "    def delete_token(cls):\n",
    "        \"\"\"\n",
    "        Delete token. Do not fail if token does not exist.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.remove(cls.path_token)\n",
    "        except FileNotFoundError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:29.450972Z",
     "start_time": "2021-02-10T14:49:29.434974Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 Optuna, Hugging Face\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Logging utilities. \"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "from logging import CRITICAL  # NOQA\n",
    "from logging import DEBUG  # NOQA\n",
    "from logging import ERROR  # NOQA\n",
    "from logging import FATAL  # NOQA\n",
    "from logging import INFO  # NOQA\n",
    "from logging import NOTSET  # NOQA\n",
    "from logging import WARN  # NOQA\n",
    "from logging import WARNING  # NOQA\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "_lock = threading.Lock()\n",
    "_default_handler: Optional[logging.Handler] = None\n",
    "\n",
    "log_levels = {\n",
    "    \"debug\": logging.DEBUG,\n",
    "    \"info\": logging.INFO,\n",
    "    \"warning\": logging.WARNING,\n",
    "    \"error\": logging.ERROR,\n",
    "    \"critical\": logging.CRITICAL,\n",
    "}\n",
    "\n",
    "_default_log_level = logging.WARNING\n",
    "\n",
    "\n",
    "def _get_default_logging_level():\n",
    "    \"\"\"\n",
    "    If TRANSFORMERS_VERBOSITY env var is set to one of the valid choices return that as the new default level. If it is\n",
    "    not - fall back to ``_default_log_level``\n",
    "    \"\"\"\n",
    "    env_level_str = os.getenv(\"TRANSFORMERS_VERBOSITY\", None)\n",
    "    if env_level_str:\n",
    "        if env_level_str in log_levels:\n",
    "            return log_levels[env_level_str]\n",
    "        else:\n",
    "            logging.getLogger().warning(\n",
    "                f\"Unknown option TRANSFORMERS_VERBOSITY={env_level_str}, \"\n",
    "                f\"has to be one of: { ', '.join(log_levels.keys()) }\"\n",
    "            )\n",
    "    return _default_log_level\n",
    "\n",
    "\n",
    "def _get_library_name() -> str:\n",
    "\n",
    "    return __name__.split(\".\")[0]\n",
    "\n",
    "\n",
    "def _get_library_root_logger() -> logging.Logger:\n",
    "\n",
    "    return logging.getLogger(_get_library_name())\n",
    "\n",
    "\n",
    "def _configure_library_root_logger() -> None:\n",
    "\n",
    "    global _default_handler\n",
    "\n",
    "    with _lock:\n",
    "        if _default_handler:\n",
    "            # This library has already configured the library root logger.\n",
    "            return\n",
    "        _default_handler = logging.StreamHandler()  # Set sys.stderr as stream.\n",
    "        _default_handler.flush = sys.stderr.flush\n",
    "\n",
    "        # Apply our default configuration to the library root logger.\n",
    "        library_root_logger = _get_library_root_logger()\n",
    "        library_root_logger.addHandler(_default_handler)\n",
    "        library_root_logger.setLevel(_get_default_logging_level())\n",
    "        library_root_logger.propagate = False\n",
    "\n",
    "\n",
    "def _reset_library_root_logger() -> None:\n",
    "\n",
    "    global _default_handler\n",
    "\n",
    "    with _lock:\n",
    "        if not _default_handler:\n",
    "            return\n",
    "\n",
    "        library_root_logger = _get_library_root_logger()\n",
    "        library_root_logger.removeHandler(_default_handler)\n",
    "        library_root_logger.setLevel(logging.NOTSET)\n",
    "        _default_handler = None\n",
    "\n",
    "\n",
    "def get_logger(name: Optional[str] = None) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Return a logger with the specified name.\n",
    "\n",
    "    This function is not supposed to be directly accessed unless you are writing a custom transformers module.\n",
    "    \"\"\"\n",
    "\n",
    "    if name is None:\n",
    "        name = _get_library_name()\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    return logging.getLogger(name)\n",
    "\n",
    "\n",
    "def get_verbosity() -> int:\n",
    "    \"\"\"\n",
    "    Return the current level for the 🤗 Transformers's root logger as an int.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`int`: The logging level.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        🤗 Transformers has following logging levels:\n",
    "\n",
    "        - 50: ``transformers.logging.CRITICAL`` or ``transformers.logging.FATAL``\n",
    "        - 40: ``transformers.logging.ERROR``\n",
    "        - 30: ``transformers.logging.WARNING`` or ``transformers.logging.WARN``\n",
    "        - 20: ``transformers.logging.INFO``\n",
    "        - 10: ``transformers.logging.DEBUG``\n",
    "    \"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    return _get_library_root_logger().getEffectiveLevel()\n",
    "\n",
    "\n",
    "def set_verbosity(verbosity: int) -> None:\n",
    "    \"\"\"\n",
    "    Set the vebosity level for the 🤗 Transformers's root logger.\n",
    "\n",
    "    Args:\n",
    "        verbosity (:obj:`int`):\n",
    "            Logging level, e.g., one of:\n",
    "\n",
    "            - ``transformers.logging.CRITICAL`` or ``transformers.logging.FATAL``\n",
    "            - ``transformers.logging.ERROR``\n",
    "            - ``transformers.logging.WARNING`` or ``transformers.logging.WARN``\n",
    "            - ``transformers.logging.INFO``\n",
    "            - ``transformers.logging.DEBUG``\n",
    "    \"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    _get_library_root_logger().setLevel(verbosity)\n",
    "\n",
    "\n",
    "def set_verbosity_info():\n",
    "    \"\"\"Set the verbosity to the :obj:`INFO` level.\"\"\"\n",
    "    return set_verbosity(INFO)\n",
    "\n",
    "\n",
    "def set_verbosity_warning():\n",
    "    \"\"\"Set the verbosity to the :obj:`WARNING` level.\"\"\"\n",
    "    return set_verbosity(WARNING)\n",
    "\n",
    "\n",
    "def set_verbosity_debug():\n",
    "    \"\"\"Set the verbosity to the :obj:`DEBUG` level.\"\"\"\n",
    "    return set_verbosity(DEBUG)\n",
    "\n",
    "\n",
    "def set_verbosity_error():\n",
    "    \"\"\"Set the verbosity to the :obj:`ERROR` level.\"\"\"\n",
    "    return set_verbosity(ERROR)\n",
    "\n",
    "\n",
    "def disable_default_handler() -> None:\n",
    "    \"\"\"Disable the default handler of the HuggingFace Transformers's root logger.\"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "\n",
    "    assert _default_handler is not None\n",
    "    _get_library_root_logger().removeHandler(_default_handler)\n",
    "\n",
    "\n",
    "def enable_default_handler() -> None:\n",
    "    \"\"\"Enable the default handler of the HuggingFace Transformers's root logger.\"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "\n",
    "    assert _default_handler is not None\n",
    "    _get_library_root_logger().addHandler(_default_handler)\n",
    "\n",
    "\n",
    "def disable_propagation() -> None:\n",
    "    \"\"\"\n",
    "    Disable propagation of the library log outputs. Note that log propagation is disabled by default.\n",
    "    \"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    _get_library_root_logger().propagate = False\n",
    "\n",
    "\n",
    "def enable_propagation() -> None:\n",
    "    \"\"\"\n",
    "    Enable propagation of the library log outputs. Please disable the HuggingFace Transformers's default handler to\n",
    "    prevent double logging if the root logger has been configured.\n",
    "    \"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    _get_library_root_logger().propagate = True\n",
    "\n",
    "\n",
    "def enable_explicit_format() -> None:\n",
    "    \"\"\"\n",
    "    Enable explicit formatting for every HuggingFace Transformers's logger. The explicit formatter is as follows:\n",
    "\n",
    "    ::\n",
    "\n",
    "        [LEVELNAME|FILENAME|LINE NUMBER] TIME >> MESSAGE\n",
    "\n",
    "    All handlers currently bound to the root logger are affected by this method.\n",
    "    \"\"\"\n",
    "    handlers = _get_library_root_logger().handlers\n",
    "\n",
    "    for handler in handlers:\n",
    "        formatter = logging.Formatter(\"[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s\")\n",
    "        handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "def reset_format() -> None:\n",
    "    \"\"\"\n",
    "    Resets the formatting for HuggingFace Transformers's loggers.\n",
    "\n",
    "    All handlers currently bound to the root logger are affected by this method.\n",
    "    \"\"\"\n",
    "    handlers = _get_library_root_logger().handlers\n",
    "\n",
    "    for handler in handlers:\n",
    "        handler.setFormatter(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  file util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:33.081828Z",
     "start_time": "2021-02-10T14:49:32.716580Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 The HuggingFace Team, the AllenNLP library authors. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Utilities for working with the local dataset cache. Parts of this file is adapted from the AllenNLP library at\n",
    "https://github.com/allenai/allennlp.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import fnmatch\n",
    "import importlib.util\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "import tempfile\n",
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import fields\n",
    "from functools import partial, wraps\n",
    "from hashlib import sha256\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "from typing import Any, BinaryIO, Dict, List, Optional, Tuple, Union\n",
    "from urllib.parse import urlparse\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "\n",
    "import numpy as np\n",
    "from packaging import version\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import requests\n",
    "from filelock import FileLock\n",
    "\n",
    "# from . import __version__\n",
    "# from .hf_api import HfFolder\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "# The package importlib_metadata is in a different place, depending on the python version.\n",
    "if sys.version_info < (3, 8):\n",
    "    import importlib_metadata\n",
    "else:\n",
    "    import importlib.metadata as importlib_metadata\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "ENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}\n",
    "ENV_VARS_TRUE_AND_AUTO_VALUES = ENV_VARS_TRUE_VALUES.union({\"AUTO\"})\n",
    "\n",
    "USE_TF = os.environ.get(\"USE_TF\", \"AUTO\").upper()\n",
    "USE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\n",
    "USE_JAX = os.environ.get(\"USE_FLAX\", \"AUTO\").upper()\n",
    "\n",
    "if USE_TORCH in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TF not in ENV_VARS_TRUE_VALUES:\n",
    "    _torch_available = importlib.util.find_spec(\"torch\") is not None\n",
    "    if _torch_available:\n",
    "        try:\n",
    "            _torch_version = importlib_metadata.version(\"torch\")\n",
    "            logger.info(f\"PyTorch version {_torch_version} available.\")\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            _torch_available = False\n",
    "else:\n",
    "    logger.info(\"Disabling PyTorch because USE_TF is set\")\n",
    "    _torch_available = False\n",
    "\n",
    "\n",
    "if USE_TF in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TORCH not in ENV_VARS_TRUE_VALUES:\n",
    "    _tf_available = importlib.util.find_spec(\"tensorflow\") is not None\n",
    "    if _tf_available:\n",
    "        # For the metadata, we have to look for both tensorflow and tensorflow-cpu\n",
    "        try:\n",
    "            _tf_version = importlib_metadata.version(\"tensorflow\")\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            try:\n",
    "                _tf_version = importlib_metadata.version(\"tensorflow-cpu\")\n",
    "            except importlib_metadata.PackageNotFoundError:\n",
    "                try:\n",
    "                    _tf_version = importlib_metadata.version(\"tensorflow-gpu\")\n",
    "                except importlib_metadata.PackageNotFoundError:\n",
    "                    try:\n",
    "                        _tf_version = importlib_metadata.version(\"tf-nightly\")\n",
    "                    except importlib_metadata.PackageNotFoundError:\n",
    "                        try:\n",
    "                            _tf_version = importlib_metadata.version(\"tf-nightly-cpu\")\n",
    "                        except importlib_metadata.PackageNotFoundError:\n",
    "                            try:\n",
    "                                _tf_version = importlib_metadata.version(\"tf-nightly-gpu\")\n",
    "                            except importlib_metadata.PackageNotFoundError:\n",
    "                                _tf_version = None\n",
    "                                _tf_available = False\n",
    "    if _tf_available:\n",
    "        if version.parse(_tf_version) < version.parse(\"2\"):\n",
    "            logger.info(f\"TensorFlow found but with version {_tf_version}. Transformers requires version 2 minimum.\")\n",
    "            _tf_available = False\n",
    "        else:\n",
    "            logger.info(f\"TensorFlow version {_tf_version} available.\")\n",
    "else:\n",
    "    logger.info(\"Disabling Tensorflow because USE_TORCH is set\")\n",
    "    _tf_available = False\n",
    "\n",
    "\n",
    "if USE_JAX in ENV_VARS_TRUE_AND_AUTO_VALUES:\n",
    "    _flax_available = importlib.util.find_spec(\"jax\") is not None and importlib.util.find_spec(\"flax\") is not None\n",
    "    if _flax_available:\n",
    "        try:\n",
    "            _jax_version = importlib_metadata.version(\"jax\")\n",
    "            _flax_version = importlib_metadata.version(\"flax\")\n",
    "            logger.info(f\"JAX version {_jax_version}, Flax version {_flax_version} available.\")\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            _flax_available = False\n",
    "else:\n",
    "    _flax_available = False\n",
    "\n",
    "\n",
    "_datasets_available = importlib.util.find_spec(\"datasets\") is not None\n",
    "try:\n",
    "    # Check we're not importing a \"datasets\" directory somewhere but the actual library by trying to grab the version\n",
    "    # AND checking it has an author field in the metadata that is HuggingFace.\n",
    "    _ = importlib_metadata.version(\"datasets\")\n",
    "    _datasets_metadata = importlib_metadata.metadata(\"datasets\")\n",
    "    if _datasets_metadata.get(\"author\", \"\") != \"HuggingFace Inc.\":\n",
    "        _datasets_available = False\n",
    "except importlib_metadata.PackageNotFoundError:\n",
    "    _datasets_available = False\n",
    "\n",
    "\n",
    "_faiss_available = importlib.util.find_spec(\"faiss\") is not None\n",
    "try:\n",
    "    _faiss_version = importlib_metadata.version(\"faiss\")\n",
    "    logger.debug(f\"Successfully imported faiss version {_faiss_version}\")\n",
    "except importlib_metadata.PackageNotFoundError:\n",
    "    try:\n",
    "        _faiss_version = importlib_metadata.version(\"faiss-cpu\")\n",
    "        logger.debug(f\"Successfully imported faiss version {_faiss_version}\")\n",
    "    except importlib_metadata.PackageNotFoundError:\n",
    "        _faiss_available = False\n",
    "\n",
    "\n",
    "_scatter_available = importlib.util.find_spec(\"torch_scatter\") is not None\n",
    "try:\n",
    "    _scatter_version = importlib_metadata.version(\"torch_scatter\")\n",
    "    logger.debug(f\"Successfully imported torch-scatter version {_scatter_version}\")\n",
    "except importlib_metadata.PackageNotFoundError:\n",
    "    _scatter_available = False\n",
    "\n",
    "\n",
    "_soundfile_available = importlib.util.find_spec(\"soundfile\") is not None\n",
    "try:\n",
    "    _soundfile_version = importlib_metadata.version(\"soundfile\")\n",
    "    logger.debug(f\"Successfully imported soundfile version {_soundfile_version}\")\n",
    "except importlib_metadata.PackageNotFoundError:\n",
    "    _soundfile_available = False\n",
    "\n",
    "\n",
    "torch_cache_home = os.getenv(\"TORCH_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"torch\"))\n",
    "old_default_cache_path = os.path.join(torch_cache_home, \"transformers\")\n",
    "# New default cache, shared with the Datasets library\n",
    "hf_cache_home = os.path.expanduser(\n",
    "    os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\"))\n",
    ")\n",
    "default_cache_path = os.path.join(hf_cache_home, \"transformers\")\n",
    "\n",
    "# Onetime move from the old location to the new one if no ENV variable has been set.\n",
    "if (\n",
    "    os.path.isdir(old_default_cache_path)\n",
    "    and not os.path.isdir(default_cache_path)\n",
    "    and \"PYTORCH_PRETRAINED_BERT_CACHE\" not in os.environ\n",
    "    and \"PYTORCH_TRANSFORMERS_CACHE\" not in os.environ\n",
    "    and \"TRANSFORMERS_CACHE\" not in os.environ\n",
    "):\n",
    "    logger.warn(\n",
    "        \"In Transformers v4.0.0, the default path to cache downloaded models changed from \"\n",
    "        \"'~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden \"\n",
    "        \"and '~/.cache/torch/transformers' is a directory that exists, we're moving it to \"\n",
    "        \"'~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should \"\n",
    "        \"only see this message once.\"\n",
    "    )\n",
    "    shutil.move(old_default_cache_path, default_cache_path)\n",
    "\n",
    "PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path)\n",
    "PYTORCH_TRANSFORMERS_CACHE = os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", PYTORCH_PRETRAINED_BERT_CACHE)\n",
    "TRANSFORMERS_CACHE = os.getenv(\"TRANSFORMERS_CACHE\", PYTORCH_TRANSFORMERS_CACHE)\n",
    "\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "TF2_WEIGHTS_NAME = \"tf_model.h5\"\n",
    "TF_WEIGHTS_NAME = \"model.ckpt\"\n",
    "FLAX_WEIGHTS_NAME = \"flax_model.msgpack\"\n",
    "CONFIG_NAME = \"config.json\"\n",
    "MODEL_CARD_NAME = \"modelcard.json\"\n",
    "\n",
    "SENTENCEPIECE_UNDERLINE = \"▁\"\n",
    "SPIECE_UNDERLINE = SENTENCEPIECE_UNDERLINE  # Kept for backward compatibility\n",
    "\n",
    "MULTIPLE_CHOICE_DUMMY_INPUTS = [\n",
    "    [[0, 1, 0, 1], [1, 0, 0, 1]]\n",
    "] * 2  # Needs to have 0s and 1s only since XLM uses it for langs too.\n",
    "DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]\n",
    "DUMMY_MASK = [[1, 1, 1, 1, 1], [1, 1, 1, 0, 0], [0, 0, 0, 1, 1]]\n",
    "\n",
    "S3_BUCKET_PREFIX = \"https://s3.amazonaws.com/models.huggingface.co/bert\"\n",
    "CLOUDFRONT_DISTRIB_PREFIX = \"https://cdn.huggingface.co\"\n",
    "HUGGINGFACE_CO_PREFIX = \"https://huggingface.co/{model_id}/resolve/{revision}/{filename}\"\n",
    "\n",
    "PRESET_MIRROR_DICT = {\n",
    "    \"tuna\": \"https://mirrors.tuna.tsinghua.edu.cn/hugging-face-models\",\n",
    "    \"bfsu\": \"https://mirrors.bfsu.edu.cn/hugging-face-models\",\n",
    "}\n",
    "\n",
    "\n",
    "def is_torch_available():\n",
    "    return _torch_available\n",
    "\n",
    "\n",
    "def is_tf_available():\n",
    "    return _tf_available\n",
    "\n",
    "\n",
    "def is_flax_available():\n",
    "    return _flax_available\n",
    "\n",
    "\n",
    "def is_torch_tpu_available():\n",
    "    if not _torch_available:\n",
    "        return False\n",
    "    # This test is probably enough, but just in case, we unpack a bit.\n",
    "    if importlib.util.find_spec(\"torch_xla\") is None:\n",
    "        return False\n",
    "    if importlib.util.find_spec(\"torch_xla.core\") is None:\n",
    "        return False\n",
    "    return importlib.util.find_spec(\"torch_xla.core.xla_model\") is not None\n",
    "\n",
    "\n",
    "def is_datasets_available():\n",
    "    return _datasets_available\n",
    "\n",
    "\n",
    "def is_psutil_available():\n",
    "    return importlib.util.find_spec(\"psutil\") is not None\n",
    "\n",
    "\n",
    "def is_py3nvml_available():\n",
    "    return importlib.util.find_spec(\"py3nvml\") is not None\n",
    "\n",
    "\n",
    "def is_apex_available():\n",
    "    return importlib.util.find_spec(\"apex\") is not None\n",
    "\n",
    "\n",
    "def is_faiss_available():\n",
    "    return _faiss_available\n",
    "\n",
    "\n",
    "def is_sklearn_available():\n",
    "    if importlib.util.find_spec(\"sklearn\") is None:\n",
    "        return False\n",
    "    if importlib.util.find_spec(\"scipy\") is None:\n",
    "        return False\n",
    "    return importlib.util.find_spec(\"sklearn.metrics\") and importlib.util.find_spec(\"scipy.stats\")\n",
    "\n",
    "\n",
    "def is_sentencepiece_available():\n",
    "    return importlib.util.find_spec(\"sentencepiece\") is not None\n",
    "\n",
    "\n",
    "def is_protobuf_available():\n",
    "    if importlib.util.find_spec(\"google\") is None:\n",
    "        return False\n",
    "    return importlib.util.find_spec(\"google.protobuf\") is not None\n",
    "\n",
    "\n",
    "def is_tokenizers_available():\n",
    "    return importlib.util.find_spec(\"tokenizers\") is not None\n",
    "\n",
    "\n",
    "def is_in_notebook():\n",
    "    try:\n",
    "        # Test adapted from tqdm.autonotebook: https://github.com/tqdm/tqdm/blob/master/tqdm/autonotebook.py\n",
    "        get_ipython = sys.modules[\"IPython\"].get_ipython\n",
    "        if \"IPKernelApp\" not in get_ipython().config:\n",
    "            raise ImportError(\"console\")\n",
    "        if \"VSCODE_PID\" in os.environ:\n",
    "            raise ImportError(\"vscode\")\n",
    "\n",
    "        return importlib.util.find_spec(\"IPython\") is not None\n",
    "    except (AttributeError, ImportError, KeyError):\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_scatter_available():\n",
    "    return _scatter_available\n",
    "\n",
    "\n",
    "def is_pandas_available():\n",
    "    return importlib.util.find_spec(\"pandas\") is not None\n",
    "\n",
    "\n",
    "def is_sagemaker_distributed_available():\n",
    "    # Get the sagemaker specific env variable.\n",
    "    sagemaker_params = os.getenv(\"SM_FRAMEWORK_PARAMS\", \"{}\")\n",
    "    try:\n",
    "        # Parse it and check the field \"sagemaker_distributed_dataparallel_enabled\".\n",
    "        sagemaker_params = json.loads(sagemaker_params)\n",
    "        if not sagemaker_params.get(\"sagemaker_distributed_dataparallel_enabled\", False):\n",
    "            return False\n",
    "    except json.JSONDecodeError:\n",
    "        return False\n",
    "    # Lastly, check if the `smdistributed` module is present.\n",
    "    return importlib.util.find_spec(\"smdistributed\") is not None\n",
    "\n",
    "\n",
    "def is_soundfile_availble():\n",
    "    return _soundfile_available\n",
    "\n",
    "\n",
    "def torch_only_method(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if not _torch_available:\n",
    "            raise ImportError(\n",
    "                \"You need to install pytorch to use this method or class, \"\n",
    "                \"or activate it with environment variables USE_TORCH=1 and USE_TF=0.\"\n",
    "            )\n",
    "        else:\n",
    "            return fn(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "DATASETS_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the 🤗 Datasets library but it was not found in your environment. You can install it with:\n",
    "```\n",
    "pip install datasets\n",
    "```\n",
    "In a notebook or a colab, you can install it by executing a cell with\n",
    "```\n",
    "!pip install datasets\n",
    "```\n",
    "then restarting your kernel.\n",
    "\n",
    "Note that if you have a local folder named `datasets` or a local python file named `datasets.py` in your current\n",
    "working directory, python may try to import this instead of the 🤗 Datasets library. You should rename this folder or\n",
    "that python file if that's the case.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "TOKENIZERS_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the 🤗 Tokenizers library but it was not found in your environment. You can install it with:\n",
    "```\n",
    "pip install tokenizers\n",
    "```\n",
    "In a notebook or a colab, you can install it by executing a cell with\n",
    "```\n",
    "!pip install tokenizers\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "SENTENCEPIECE_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
    "that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "PROTOBUF_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the protobuf library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\n",
    "that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "FAISS_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the faiss library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\n",
    "that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "PYTORCH_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "SKLEARN_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the scikit-learn library but it was not found in your environment. You can install it with:\n",
    "```\n",
    "pip install -U scikit-learn\n",
    "```\n",
    "In a notebook or a colab, you can install it by executing a cell with\n",
    "```\n",
    "!pip install -U scikit-learn\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "TENSORFLOW_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "FLAX_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page: https://github.com/google/flax and follow the ones that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "SCATTER_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the torch-scatter library but it was not found in your environment. You can install it with pip as\n",
    "explained here: https://github.com/rusty1s/pytorch_scatter.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "PANDAS_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the pandas library but it was not found in your environment. You can install it with pip as\n",
    "explained here: https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def requires_datasets(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_datasets_available():\n",
    "        raise ImportError(DATASETS_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_faiss(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_faiss_available():\n",
    "        raise ImportError(FAISS_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_pytorch(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_torch_available():\n",
    "        raise ImportError(PYTORCH_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_sklearn(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_sklearn_available():\n",
    "        raise ImportError(SKLEARN_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_tf(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_tf_available():\n",
    "        raise ImportError(TENSORFLOW_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_flax(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_flax_available():\n",
    "        raise ImportError(FLAX_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_tokenizers(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_tokenizers_available():\n",
    "        raise ImportError(TOKENIZERS_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_sentencepiece(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_sentencepiece_available():\n",
    "        raise ImportError(SENTENCEPIECE_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_protobuf(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_protobuf_available():\n",
    "        raise ImportError(PROTOBUF_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_pandas(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_pandas_available():\n",
    "        raise ImportError(PANDAS_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_scatter(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_scatter_available():\n",
    "        raise ImportError(SCATTER_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def add_start_docstrings(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        fn.__doc__ = \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def add_start_docstrings_to_model_forward(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        class_name = \":class:`~transformers.{}`\".format(fn.__qualname__.split(\".\")[0])\n",
    "        intro = \"   The {} forward method, overrides the :func:`__call__` special method.\".format(class_name)\n",
    "        note = r\"\"\"\n",
    "\n",
    "    .. note::\n",
    "        Although the recipe for forward pass needs to be defined within this function, one should call the\n",
    "        :class:`Module` instance afterwards instead of this since the former takes care of running the pre and post\n",
    "        processing steps while the latter silently ignores them.\n",
    "        \"\"\"\n",
    "        fn.__doc__ = intro + note + \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def add_end_docstrings(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        fn.__doc__ = fn.__doc__ + \"\".join(docstr)\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "PT_RETURN_INTRODUCTION = r\"\"\"\n",
    "    Returns:\n",
    "        :class:`~{full_output_type}` or :obj:`tuple(torch.FloatTensor)`: A :class:`~{full_output_type}` (if\n",
    "        ``return_dict=True`` is passed or when ``config.return_dict=True``) or a tuple of :obj:`torch.FloatTensor`\n",
    "        comprising various elements depending on the configuration (:class:`~transformers.{config_class}`) and inputs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "TF_RETURN_INTRODUCTION = r\"\"\"\n",
    "    Returns:\n",
    "        :class:`~{full_output_type}` or :obj:`tuple(tf.Tensor)`: A :class:`~{full_output_type}` (if\n",
    "        ``return_dict=True`` is passed or when ``config.return_dict=True``) or a tuple of :obj:`tf.Tensor` comprising\n",
    "        various elements depending on the configuration (:class:`~transformers.{config_class}`) and inputs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _get_indent(t):\n",
    "    \"\"\"Returns the indentation in the first line of t\"\"\"\n",
    "    search = re.search(r\"^(\\s*)\\S\", t)\n",
    "    return \"\" if search is None else search.groups()[0]\n",
    "\n",
    "\n",
    "def _convert_output_args_doc(output_args_doc):\n",
    "    \"\"\"Convert output_args_doc to display properly.\"\"\"\n",
    "    # Split output_arg_doc in blocks argument/description\n",
    "    indent = _get_indent(output_args_doc)\n",
    "    blocks = []\n",
    "    current_block = \"\"\n",
    "    for line in output_args_doc.split(\"\\n\"):\n",
    "        # If the indent is the same as the beginning, the line is the name of new arg.\n",
    "        if _get_indent(line) == indent:\n",
    "            if len(current_block) > 0:\n",
    "                blocks.append(current_block[:-1])\n",
    "            current_block = f\"{line}\\n\"\n",
    "        else:\n",
    "            # Otherwise it's part of the description of the current arg.\n",
    "            # We need to remove 2 spaces to the indentation.\n",
    "            current_block += f\"{line[2:]}\\n\"\n",
    "    blocks.append(current_block[:-1])\n",
    "\n",
    "    # Format each block for proper rendering\n",
    "    for i in range(len(blocks)):\n",
    "        blocks[i] = re.sub(r\"^(\\s+)(\\S+)(\\s+)\", r\"\\1- **\\2**\\3\", blocks[i])\n",
    "        blocks[i] = re.sub(r\":\\s*\\n\\s*(\\S)\", r\" -- \\1\", blocks[i])\n",
    "\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "\n",
    "def _prepare_output_docstrings(output_type, config_class):\n",
    "    \"\"\"\n",
    "    Prepares the return part of the docstring using `output_type`.\n",
    "    \"\"\"\n",
    "    docstrings = output_type.__doc__\n",
    "\n",
    "    # Remove the head of the docstring to keep the list of args only\n",
    "    lines = docstrings.split(\"\\n\")\n",
    "    i = 0\n",
    "    while i < len(lines) and re.search(r\"^\\s*(Args|Parameters):\\s*$\", lines[i]) is None:\n",
    "        i += 1\n",
    "    if i < len(lines):\n",
    "        docstrings = \"\\n\".join(lines[(i + 1) :])\n",
    "        docstrings = _convert_output_args_doc(docstrings)\n",
    "\n",
    "    # Add the return introduction\n",
    "    full_output_type = f\"{output_type.__module__}.{output_type.__name__}\"\n",
    "    intro = TF_RETURN_INTRODUCTION if output_type.__name__.startswith(\"TF\") else PT_RETURN_INTRODUCTION\n",
    "    intro = intro.format(full_output_type=full_output_type, config_class=config_class)\n",
    "    return intro + docstrings\n",
    "\n",
    "\n",
    "PT_TOKEN_CLASSIFICATION_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "        >>> outputs = model(**inputs, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "PT_QUESTION_ANSWERING_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "        >>> inputs = tokenizer(question, text, return_tensors='pt')\n",
    "        >>> start_positions = torch.tensor([1])\n",
    "        >>> end_positions = torch.tensor([3])\n",
    "\n",
    "        >>> outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> start_scores = outputs.start_logits\n",
    "        >>> end_scores = outputs.end_logits\n",
    "\"\"\"\n",
    "\n",
    "PT_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        >>> outputs = model(**inputs, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "PT_MASKED_LM_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"The capital of France is {mask}.\", return_tensors=\"pt\")\n",
    "        >>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "        >>> outputs = model(**inputs, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "PT_BASE_MODEL_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "\n",
    "        >>> last_hidden_states = outputs.last_hidden_state\n",
    "\"\"\"\n",
    "\n",
    "PT_MULTIPLE_CHOICE_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "        >>> choice0 = \"It is eaten with a fork and a knife.\"\n",
    "        >>> choice1 = \"It is eaten while held in the hand.\"\n",
    "        >>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "\n",
    "        >>> encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='pt', padding=True)\n",
    "        >>> outputs = model(**{{k: v.unsqueeze(0) for k,v in encoding.items()}}, labels=labels)  # batch size is 1\n",
    "\n",
    "        >>> # the linear classifier still needs to be trained\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "PT_CAUSAL_LM_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> import torch\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_TOKEN_CLASSIFICATION_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "        >>> input_ids = inputs[\"input_ids\"]\n",
    "        >>> inputs[\"labels\"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1\n",
    "\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_QUESTION_ANSWERING_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "        >>> input_dict = tokenizer(question, text, return_tensors='tf')\n",
    "        >>> outputs = model(input_dict)\n",
    "        >>> start_logits = outputs.start_logits\n",
    "        >>> end_logits = outputs.end_logits\n",
    "\n",
    "        >>> all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "        >>> answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])\n",
    "\"\"\"\n",
    "\n",
    "TF_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "        >>> inputs[\"labels\"] = tf.reshape(tf.constant(1), (-1, 1)) # Batch size 1\n",
    "\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_MASKED_LM_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"The capital of France is {mask}.\", return_tensors=\"tf\")\n",
    "        >>> inputs[\"labels\"] = tokenizer(\"The capital of France is Paris.\", return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_BASE_MODEL_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "        >>> outputs = model(inputs)\n",
    "\n",
    "        >>> last_hidden_states = outputs.last_hidden_state\n",
    "\"\"\"\n",
    "\n",
    "TF_MULTIPLE_CHOICE_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "        >>> choice0 = \"It is eaten with a fork and a knife.\"\n",
    "        >>> choice1 = \"It is eaten while held in the hand.\"\n",
    "\n",
    "        >>> encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='tf', padding=True)\n",
    "        >>> inputs = {{k: tf.expand_dims(v, 0) for k, v in encoding.items()}}\n",
    "        >>> outputs = model(inputs)  # batch size is 1\n",
    "\n",
    "        >>> # the linear classifier still needs to be trained\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_CAUSAL_LM_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_code_sample_docstrings(\n",
    "    *docstr, tokenizer_class=None, checkpoint=None, output_type=None, config_class=None, mask=None\n",
    "):\n",
    "    def docstring_decorator(fn):\n",
    "        model_class = fn.__qualname__.split(\".\")[0]\n",
    "        is_tf_class = model_class[:2] == \"TF\"\n",
    "        doc_kwargs = dict(model_class=model_class, tokenizer_class=tokenizer_class, checkpoint=checkpoint)\n",
    "\n",
    "        if \"SequenceClassification\" in model_class:\n",
    "            code_sample = TF_SEQUENCE_CLASSIFICATION_SAMPLE if is_tf_class else PT_SEQUENCE_CLASSIFICATION_SAMPLE\n",
    "        elif \"QuestionAnswering\" in model_class:\n",
    "            code_sample = TF_QUESTION_ANSWERING_SAMPLE if is_tf_class else PT_QUESTION_ANSWERING_SAMPLE\n",
    "        elif \"TokenClassification\" in model_class:\n",
    "            code_sample = TF_TOKEN_CLASSIFICATION_SAMPLE if is_tf_class else PT_TOKEN_CLASSIFICATION_SAMPLE\n",
    "        elif \"MultipleChoice\" in model_class:\n",
    "            code_sample = TF_MULTIPLE_CHOICE_SAMPLE if is_tf_class else PT_MULTIPLE_CHOICE_SAMPLE\n",
    "        elif \"MaskedLM\" in model_class or model_class in [\"FlaubertWithLMHeadModel\", \"XLMWithLMHeadModel\"]:\n",
    "            doc_kwargs[\"mask\"] = \"[MASK]\" if mask is None else mask\n",
    "            code_sample = TF_MASKED_LM_SAMPLE if is_tf_class else PT_MASKED_LM_SAMPLE\n",
    "        elif \"LMHead\" in model_class or \"CausalLM\" in model_class:\n",
    "            code_sample = TF_CAUSAL_LM_SAMPLE if is_tf_class else PT_CAUSAL_LM_SAMPLE\n",
    "        elif \"Model\" in model_class or \"Encoder\" in model_class:\n",
    "            code_sample = TF_BASE_MODEL_SAMPLE if is_tf_class else PT_BASE_MODEL_SAMPLE\n",
    "        else:\n",
    "            raise ValueError(f\"Docstring can't be built for model {model_class}\")\n",
    "\n",
    "        output_doc = _prepare_output_docstrings(output_type, config_class) if output_type is not None else \"\"\n",
    "        built_doc = code_sample.format(**doc_kwargs)\n",
    "        fn.__doc__ = (fn.__doc__ or \"\") + \"\".join(docstr) + output_doc + built_doc\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def replace_return_docstrings(output_type=None, config_class=None):\n",
    "    def docstring_decorator(fn):\n",
    "        docstrings = fn.__doc__\n",
    "        lines = docstrings.split(\"\\n\")\n",
    "        i = 0\n",
    "        while i < len(lines) and re.search(r\"^\\s*Returns?:\\s*$\", lines[i]) is None:\n",
    "            i += 1\n",
    "        if i < len(lines):\n",
    "            lines[i] = _prepare_output_docstrings(output_type, config_class)\n",
    "            docstrings = \"\\n\".join(lines)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"The function {fn} should have an empty 'Return:' or 'Returns:' in its docstring as placeholder, current docstring is:\\n{docstrings}\"\n",
    "            )\n",
    "        fn.__doc__ = docstrings\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def is_remote_url(url_or_filename):\n",
    "    parsed = urlparse(url_or_filename)\n",
    "    return parsed.scheme in (\"http\", \"https\")\n",
    "\n",
    "\n",
    "def hf_bucket_url(\n",
    "    model_id: str, filename: str, subfolder: Optional[str] = None, revision: Optional[str] = None, mirror=None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Resolve a model identifier, a file name, and an optional revision id, to a huggingface.co-hosted url, redirecting\n",
    "    to Cloudfront (a Content Delivery Network, or CDN) for large files.\n",
    "\n",
    "    Cloudfront is replicated over the globe so downloads are way faster for the end user (and it also lowers our\n",
    "    bandwidth costs).\n",
    "\n",
    "    Cloudfront aggressively caches files by default (default TTL is 24 hours), however this is not an issue here\n",
    "    because we migrated to a git-based versioning system on huggingface.co, so we now store the files on S3/Cloudfront\n",
    "    in a content-addressable way (i.e., the file name is its hash). Using content-addressable filenames means cache\n",
    "    can't ever be stale.\n",
    "\n",
    "    In terms of client-side caching from this library, we base our caching on the objects' ETag. An object' ETag is:\n",
    "    its sha1 if stored in git, or its sha256 if stored in git-lfs. Files cached locally from transformers before v3.5.0\n",
    "    are not shared with those new files, because the cached file's name contains a hash of the url (which changed).\n",
    "    \"\"\"\n",
    "    if subfolder is not None:\n",
    "        filename = f\"{subfolder}/{filename}\"\n",
    "\n",
    "    if mirror:\n",
    "        endpoint = PRESET_MIRROR_DICT.get(mirror, mirror)\n",
    "        legacy_format = \"/\" not in model_id\n",
    "        if legacy_format:\n",
    "            return f\"{endpoint}/{model_id}-{filename}\"\n",
    "        else:\n",
    "            return f\"{endpoint}/{model_id}/{filename}\"\n",
    "\n",
    "    if revision is None:\n",
    "        revision = \"main\"\n",
    "    return HUGGINGFACE_CO_PREFIX.format(model_id=model_id, revision=revision, filename=filename)\n",
    "\n",
    "\n",
    "def url_to_filename(url: str, etag: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Convert `url` into a hashed filename in a repeatable way. If `etag` is specified, append its hash to the url's,\n",
    "    delimited by a period. If the url ends with .h5 (Keras HDF5 weights) adds '.h5' to the name so that TF 2.0 can\n",
    "    identify it as a HDF5 file (see\n",
    "    https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n",
    "    \"\"\"\n",
    "    url_bytes = url.encode(\"utf-8\")\n",
    "    filename = sha256(url_bytes).hexdigest()\n",
    "\n",
    "    if etag:\n",
    "        etag_bytes = etag.encode(\"utf-8\")\n",
    "        filename += \".\" + sha256(etag_bytes).hexdigest()\n",
    "\n",
    "    if url.endswith(\".h5\"):\n",
    "        filename += \".h5\"\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def filename_to_url(filename, cache_dir=None):\n",
    "    \"\"\"\n",
    "    Return the url and etag (which may be ``None``) stored for `filename`. Raise ``EnvironmentError`` if `filename` or\n",
    "    its stored metadata do not exist.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if not os.path.exists(cache_path):\n",
    "        raise EnvironmentError(\"file {} not found\".format(cache_path))\n",
    "\n",
    "    meta_path = cache_path + \".json\"\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise EnvironmentError(\"file {} not found\".format(meta_path))\n",
    "\n",
    "    with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
    "        metadata = json.load(meta_file)\n",
    "    url = metadata[\"url\"]\n",
    "    etag = metadata[\"etag\"]\n",
    "\n",
    "    return url, etag\n",
    "\n",
    "\n",
    "def get_cached_models(cache_dir: Union[str, Path] = None) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Returns a list of tuples representing model binaries that are cached locally. Each tuple has shape\n",
    "    :obj:`(model_url, etag, size_MB)`. Filenames in :obj:`cache_dir` are use to get the metadata for each model, only\n",
    "    urls ending with `.bin` are added.\n",
    "\n",
    "    Args:\n",
    "        cache_dir (:obj:`Union[str, Path]`, `optional`):\n",
    "            The cache directory to search for models within. Will default to the transformers cache if unset.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple]: List of tuples each with shape :obj:`(model_url, etag, size_MB)`\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    elif isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    cached_models = []\n",
    "    for file in os.listdir(cache_dir):\n",
    "        if file.endswith(\".json\"):\n",
    "            meta_path = os.path.join(cache_dir, file)\n",
    "            with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
    "                metadata = json.load(meta_file)\n",
    "                url = metadata[\"url\"]\n",
    "                etag = metadata[\"etag\"]\n",
    "                if url.endswith(\".bin\"):\n",
    "                    size_MB = os.path.getsize(meta_path.strip(\".json\")) / 1e6\n",
    "                    cached_models.append((url, etag, size_MB))\n",
    "\n",
    "    return cached_models\n",
    "\n",
    "\n",
    "def cached_path(\n",
    "    url_or_filename,\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    "    proxies=None,\n",
    "    resume_download=False,\n",
    "    user_agent: Union[Dict, str, None] = None,\n",
    "    extract_compressed_file=False,\n",
    "    force_extract=False,\n",
    "    use_auth_token: Union[bool, str, None] = None,\n",
    "    local_files_only=False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given something that might be a URL (or might be a local path), determine which. If it's a URL, download the file\n",
    "    and cache it, and return the path to the cached file. If it's already a local path, make sure the file exists and\n",
    "    then return the path\n",
    "\n",
    "    Args:\n",
    "        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n",
    "        force_download: if True, re-download the file even if it's already cached in the cache dir.\n",
    "        resume_download: if True, resume the download if incompletely received file is found.\n",
    "        user_agent: Optional string or dict that will be appended to the user-agent on remote requests.\n",
    "        use_auth_token: Optional string or boolean to use as Bearer token for remote files. If True,\n",
    "            will get token from ~/.huggingface.\n",
    "        extract_compressed_file: if True and the path point to a zip or tar file, extract the compressed\n",
    "            file in a folder along the archive.\n",
    "        force_extract: if True when extract_compressed_file is True and the archive was already extracted,\n",
    "            re-extract the archive and override the folder where it was extracted.\n",
    "\n",
    "    Return:\n",
    "        Local path (string) of file or if networking is off, last version of file cached on disk.\n",
    "\n",
    "    Raises:\n",
    "        In case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(url_or_filename, Path):\n",
    "        url_or_filename = str(url_or_filename)\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    if is_remote_url(url_or_filename):\n",
    "        # URL, so get it from the cache (downloading if necessary)\n",
    "        output_path = get_from_cache(\n",
    "            url_or_filename,\n",
    "            cache_dir=cache_dir,\n",
    "            force_download=force_download,\n",
    "            proxies=proxies,\n",
    "            resume_download=resume_download,\n",
    "            user_agent=user_agent,\n",
    "            use_auth_token=use_auth_token,\n",
    "            local_files_only=local_files_only,\n",
    "        )\n",
    "    elif os.path.exists(url_or_filename):\n",
    "        # File, and it exists.\n",
    "        output_path = url_or_filename\n",
    "    elif urlparse(url_or_filename).scheme == \"\":\n",
    "        # File, but it doesn't exist.\n",
    "        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
    "    else:\n",
    "        # Something unknown\n",
    "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n",
    "\n",
    "    if extract_compressed_file:\n",
    "        if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path):\n",
    "            return output_path\n",
    "\n",
    "        # Path where we extract compressed archives\n",
    "        # We avoid '.' in dir name and add \"-extracted\" at the end: \"./model.zip\" => \"./model-zip-extracted/\"\n",
    "        output_dir, output_file = os.path.split(output_path)\n",
    "        output_extract_dir_name = output_file.replace(\".\", \"-\") + \"-extracted\"\n",
    "        output_path_extracted = os.path.join(output_dir, output_extract_dir_name)\n",
    "\n",
    "        if os.path.isdir(output_path_extracted) and os.listdir(output_path_extracted) and not force_extract:\n",
    "            return output_path_extracted\n",
    "\n",
    "        # Prevent parallel extractions\n",
    "        lock_path = output_path + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            shutil.rmtree(output_path_extracted, ignore_errors=True)\n",
    "            os.makedirs(output_path_extracted)\n",
    "            if is_zipfile(output_path):\n",
    "                with ZipFile(output_path, \"r\") as zip_file:\n",
    "                    zip_file.extractall(output_path_extracted)\n",
    "                    zip_file.close()\n",
    "            elif tarfile.is_tarfile(output_path):\n",
    "                tar_file = tarfile.open(output_path)\n",
    "                tar_file.extractall(output_path_extracted)\n",
    "                tar_file.close()\n",
    "            else:\n",
    "                raise EnvironmentError(\"Archive format of {} could not be identified\".format(output_path))\n",
    "\n",
    "        return output_path_extracted\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def http_user_agent(user_agent: Union[Dict, str, None] = None) -> str:\n",
    "    \"\"\"\n",
    "    Formats a user-agent string with basic info about a request.\n",
    "    \"\"\"\n",
    "    ua = \"transformers/{}; python/{}\".format(__version__, sys.version.split()[0])\n",
    "    if is_torch_available():\n",
    "        ua += f\"; torch/{_torch_version}\"\n",
    "    if is_tf_available():\n",
    "        ua += f\"; tensorflow/{_tf_version}\"\n",
    "    if isinstance(user_agent, dict):\n",
    "        ua += \"; \" + \"; \".join(\"{}/{}\".format(k, v) for k, v in user_agent.items())\n",
    "    elif isinstance(user_agent, str):\n",
    "        ua += \"; \" + user_agent\n",
    "    return ua\n",
    "\n",
    "\n",
    "def http_get(url: str, temp_file: BinaryIO, proxies=None, resume_size=0, headers: Optional[Dict[str, str]] = None):\n",
    "    \"\"\"\n",
    "    Donwload remote file. Do not gobble up errors.\n",
    "    \"\"\"\n",
    "    headers = copy.deepcopy(headers)\n",
    "    if resume_size > 0:\n",
    "        headers[\"Range\"] = \"bytes=%d-\" % (resume_size,)\n",
    "    r = requests.get(url, stream=True, proxies=proxies, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    content_length = r.headers.get(\"Content-Length\")\n",
    "    total = resume_size + int(content_length) if content_length is not None else None\n",
    "    progress = tqdm(\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        total=total,\n",
    "        initial=resume_size,\n",
    "        desc=\"Downloading\",\n",
    "        disable=bool(logging.get_verbosity() == logging.NOTSET),\n",
    "    )\n",
    "    for chunk in r.iter_content(chunk_size=1024):\n",
    "        if chunk:  # filter out keep-alive new chunks\n",
    "            progress.update(len(chunk))\n",
    "            temp_file.write(chunk)\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "def get_from_cache(\n",
    "    url: str,\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    "    proxies=None,\n",
    "    etag_timeout=10,\n",
    "    resume_download=False,\n",
    "    user_agent: Union[Dict, str, None] = None,\n",
    "    use_auth_token: Union[bool, str, None] = None,\n",
    "    local_files_only=False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given a URL, look for the corresponding file in the local cache. If it's not there, download it. Then return the\n",
    "    path to the cached file.\n",
    "\n",
    "    Return:\n",
    "        Local path (string) of file or if networking is off, last version of file cached on disk.\n",
    "\n",
    "    Raises:\n",
    "        In case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    headers = {\"user-agent\": http_user_agent(user_agent)}\n",
    "    if isinstance(use_auth_token, str):\n",
    "        headers[\"authorization\"] = \"Bearer {}\".format(use_auth_token)\n",
    "    elif use_auth_token:\n",
    "        token = HfFolder.get_token()\n",
    "        if token is None:\n",
    "            raise EnvironmentError(\"You specified use_auth_token=True, but a huggingface token was not found.\")\n",
    "        headers[\"authorization\"] = \"Bearer {}\".format(token)\n",
    "\n",
    "    url_to_download = url\n",
    "    etag = None\n",
    "    if not local_files_only:\n",
    "        try:\n",
    "            r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)\n",
    "            r.raise_for_status()\n",
    "            etag = r.headers.get(\"X-Linked-Etag\") or r.headers.get(\"ETag\")\n",
    "            # We favor a custom header indicating the etag of the linked resource, and\n",
    "            # we fallback to the regular etag header.\n",
    "            # If we don't have any of those, raise an error.\n",
    "            if etag is None:\n",
    "                raise OSError(\n",
    "                    \"Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility.\"\n",
    "                )\n",
    "            # In case of a redirect,\n",
    "            # save an extra redirect on the request.get call,\n",
    "            # and ensure we download the exact atomic version even if it changed\n",
    "            # between the HEAD and the GET (unlikely, but hey).\n",
    "            if 300 <= r.status_code <= 399:\n",
    "                url_to_download = r.headers[\"Location\"]\n",
    "        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):\n",
    "            # etag is already None\n",
    "            pass\n",
    "\n",
    "    filename = url_to_filename(url, etag)\n",
    "\n",
    "    # get cache path to put the file\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "\n",
    "    # etag is None == we don't have a connection or we passed local_files_only.\n",
    "    # try to get the last downloaded one\n",
    "    if etag is None:\n",
    "        if os.path.exists(cache_path):\n",
    "            return cache_path\n",
    "        else:\n",
    "            matching_files = [\n",
    "                file\n",
    "                for file in fnmatch.filter(os.listdir(cache_dir), filename.split(\".\")[0] + \".*\")\n",
    "                if not file.endswith(\".json\") and not file.endswith(\".lock\")\n",
    "            ]\n",
    "            if len(matching_files) > 0:\n",
    "                return os.path.join(cache_dir, matching_files[-1])\n",
    "            else:\n",
    "                # If files cannot be found and local_files_only=True,\n",
    "                # the models might've been found if local_files_only=False\n",
    "                # Notify the user about that\n",
    "                if local_files_only:\n",
    "                    raise FileNotFoundError(\n",
    "                        \"Cannot find the requested files in the cached path and outgoing traffic has been\"\n",
    "                        \" disabled. To enable model look-ups and downloads online, set 'local_files_only'\"\n",
    "                        \" to False.\"\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Connection error, and we cannot find the requested files in the cached path.\"\n",
    "                        \" Please try again or make sure your Internet connection is on.\"\n",
    "                    )\n",
    "\n",
    "    # From now on, etag is not None.\n",
    "    if os.path.exists(cache_path) and not force_download:\n",
    "        return cache_path\n",
    "\n",
    "    # Prevent parallel downloads of the same file with a lock.\n",
    "    lock_path = cache_path + \".lock\"\n",
    "    with FileLock(lock_path):\n",
    "\n",
    "        # If the download just completed while the lock was activated.\n",
    "        if os.path.exists(cache_path) and not force_download:\n",
    "            # Even if returning early like here, the lock will be released.\n",
    "            return cache_path\n",
    "\n",
    "        if resume_download:\n",
    "            incomplete_path = cache_path + \".incomplete\"\n",
    "\n",
    "            @contextmanager\n",
    "            def _resumable_file_manager() -> \"io.BufferedWriter\":\n",
    "                with open(incomplete_path, \"ab\") as f:\n",
    "                    yield f\n",
    "\n",
    "            temp_file_manager = _resumable_file_manager\n",
    "            if os.path.exists(incomplete_path):\n",
    "                resume_size = os.stat(incomplete_path).st_size\n",
    "            else:\n",
    "                resume_size = 0\n",
    "        else:\n",
    "            temp_file_manager = partial(tempfile.NamedTemporaryFile, mode=\"wb\", dir=cache_dir, delete=False)\n",
    "            resume_size = 0\n",
    "\n",
    "        # Download to temporary file, then copy to cache dir once finished.\n",
    "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
    "        with temp_file_manager() as temp_file:\n",
    "            logger.info(\"%s not found in cache or force_download set to True, downloading to %s\", url, temp_file.name)\n",
    "\n",
    "            http_get(url_to_download, temp_file, proxies=proxies, resume_size=resume_size, headers=headers)\n",
    "\n",
    "        logger.info(\"storing %s in cache at %s\", url, cache_path)\n",
    "        os.replace(temp_file.name, cache_path)\n",
    "\n",
    "        logger.info(\"creating metadata file for %s\", cache_path)\n",
    "        meta = {\"url\": url, \"etag\": etag}\n",
    "        meta_path = cache_path + \".json\"\n",
    "        with open(meta_path, \"w\") as meta_file:\n",
    "            json.dump(meta, meta_file)\n",
    "\n",
    "    return cache_path\n",
    "\n",
    "\n",
    "class cached_property(property):\n",
    "    \"\"\"\n",
    "    Descriptor that mimics @property but caches output in member variable.\n",
    "\n",
    "    From tensorflow_datasets\n",
    "\n",
    "    Built-in in functools from Python 3.8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __get__(self, obj, objtype=None):\n",
    "        # See docs.python.org/3/howto/descriptor.html#properties\n",
    "        if obj is None:\n",
    "            return self\n",
    "        if self.fget is None:\n",
    "            raise AttributeError(\"unreadable attribute\")\n",
    "        attr = \"__cached_\" + self.fget.__name__\n",
    "        cached = getattr(obj, attr, None)\n",
    "        if cached is None:\n",
    "            cached = self.fget(obj)\n",
    "            setattr(obj, attr, cached)\n",
    "        return cached\n",
    "\n",
    "\n",
    "def torch_required(func):\n",
    "    # Chose a different decorator name than in tests so it's clear they are not the same.\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if is_torch_available():\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            raise ImportError(f\"Method `{func.__name__}` requires PyTorch.\")\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def tf_required(func):\n",
    "    # Chose a different decorator name than in tests so it's clear they are not the same.\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if is_tf_available():\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            raise ImportError(f\"Method `{func.__name__}` requires TF.\")\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def is_tensor(x):\n",
    "    \"\"\" Tests if ``x`` is a :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`. \"\"\"\n",
    "    if is_torch_available():\n",
    "        import torch\n",
    "\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return True\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "\n",
    "        if isinstance(x, tf.Tensor):\n",
    "            return True\n",
    "    return isinstance(x, np.ndarray)\n",
    "\n",
    "\n",
    "class ModelOutput(OrderedDict):\n",
    "    \"\"\"\n",
    "    Base class for all model outputs as dataclass. Has a ``__getitem__`` that allows indexing by integer or slice (like\n",
    "    a tuple) or strings (like a dictionary) that will ignore the ``None`` attributes. Otherwise behaves like a regular\n",
    "    python dictionary.\n",
    "\n",
    "    .. warning::\n",
    "        You can't unpack a :obj:`ModelOutput` directly. Use the :meth:`~transformers.file_utils.ModelOutput.to_tuple`\n",
    "        method to convert it to a tuple before.\n",
    "    \"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        class_fields = fields(self)\n",
    "\n",
    "        # Safety and consistency checks\n",
    "        assert len(class_fields), f\"{self.__class__.__name__} has no fields.\"\n",
    "        assert all(\n",
    "            field.default is None for field in class_fields[1:]\n",
    "        ), f\"{self.__class__.__name__} should not have more than one required field.\"\n",
    "\n",
    "        first_field = getattr(self, class_fields[0].name)\n",
    "        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n",
    "\n",
    "        if other_fields_are_none and not is_tensor(first_field):\n",
    "            try:\n",
    "                iterator = iter(first_field)\n",
    "                first_field_iterator = True\n",
    "            except TypeError:\n",
    "                first_field_iterator = False\n",
    "\n",
    "            # if we provided an iterator as first field and the iterator is a (key, value) iterator\n",
    "            # set the associated fields\n",
    "            if first_field_iterator:\n",
    "                for element in iterator:\n",
    "                    if (\n",
    "                        not isinstance(element, (list, tuple))\n",
    "                        or not len(element) == 2\n",
    "                        or not isinstance(element[0], str)\n",
    "                    ):\n",
    "                        break\n",
    "                    setattr(self, element[0], element[1])\n",
    "                    if element[1] is not None:\n",
    "                        self[element[0]] = element[1]\n",
    "            elif first_field is not None:\n",
    "                self[class_fields[0].name] = first_field\n",
    "        else:\n",
    "            for field in class_fields:\n",
    "                v = getattr(self, field.name)\n",
    "                if v is not None:\n",
    "                    self[field.name] = v\n",
    "\n",
    "    def __delitem__(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def setdefault(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def pop(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if isinstance(k, str):\n",
    "            inner_dict = {k: v for (k, v) in self.items()}\n",
    "            return inner_dict[k]\n",
    "        else:\n",
    "            return self.to_tuple()[k]\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if name in self.keys() and value is not None:\n",
    "            # Don't call self.__setitem__ to avoid recursion errors\n",
    "            super().__setitem__(name, value)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        # Will raise a KeyException if needed\n",
    "        super().__setitem__(key, value)\n",
    "        # Don't call self.__setattr__ to avoid recursion errors\n",
    "        super().__setattr__(key, value)\n",
    "\n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        \"\"\"\n",
    "        Convert self to a tuple containing all the attributes/keys that are not ``None``.\n",
    "        \"\"\"\n",
    "        return tuple(self[k] for k in self.keys())\n",
    "\n",
    "\n",
    "class _BaseLazyModule(ModuleType):\n",
    "    \"\"\"\n",
    "    Module class that surfaces all objects but only performs associated imports when the objects are requested.\n",
    "    \"\"\"\n",
    "\n",
    "    # Very heavily inspired by optuna.integration._IntegrationModule\n",
    "    # https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py\n",
    "    def __init__(self, name, import_structure):\n",
    "        super().__init__(name)\n",
    "        self._modules = set(import_structure.keys())\n",
    "        self._class_to_module = {}\n",
    "        for key, values in import_structure.items():\n",
    "            for value in values:\n",
    "                self._class_to_module[value] = key\n",
    "        # Needed for autocompletion in an IDE\n",
    "        self.__all__ = list(import_structure.keys()) + sum(import_structure.values(), [])\n",
    "\n",
    "    # Needed for autocompletion in an IDE\n",
    "    def __dir__(self):\n",
    "        return super().__dir__() + self.__all__\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        if name in self._modules:\n",
    "            value = self._get_module(name)\n",
    "        elif name in self._class_to_module.keys():\n",
    "            module = self._get_module(self._class_to_module[name])\n",
    "            value = getattr(module, name)\n",
    "        else:\n",
    "            raise AttributeError(f\"module {self.__name__} has no attribute {name}\")\n",
    "\n",
    "        setattr(self, name, value)\n",
    "        return value\n",
    "\n",
    "    def _get_module(self, module_name: str) -> ModuleType:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configure util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:35.469984Z",
     "start_time": "2021-02-10T14:49:35.427319Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Configuration base class and utilities.\"\"\"\n",
    "\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "# from . import __version__\n",
    "# from .file_utils import CONFIG_NAME, cached_path, hf_bucket_url, is_remote_url\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class PretrainedConfig(object):\n",
    "    r\"\"\"\n",
    "    Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as\n",
    "    methods for loading/downloading/saving configurations.\n",
    "\n",
    "    Note: A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to\n",
    "    initialize a model does **not** load the model weights. It only affects the model's configuration.\n",
    "\n",
    "    Class attributes (overridden by derived classes)\n",
    "\n",
    "        - **model_type** (:obj:`str`): An identifier for the model type, serialized into the JSON file, and used to\n",
    "          recreate the correct object in :class:`~transformers.AutoConfig`.\n",
    "        - **is_composition** (:obj:`bool`): Whether the config class is composed of multiple sub-configs. In this case\n",
    "          the config has to be initialized from two or more configs of type :class:`~transformers.PretrainedConfig`\n",
    "          like: :class:`~transformers.EncoderDecoderConfig` or :class:`~RagConfig`.\n",
    "        - **keys_to_ignore_at_inference** (:obj:`List[str]`): A list of keys to ignore by default when looking at\n",
    "          dictionary outputs of the model during inference.\n",
    "\n",
    "    Args:\n",
    "        name_or_path (:obj:`str`, `optional`, defaults to :obj:`\"\"`):\n",
    "            Store the string that was passed to :func:`~transformers.PreTrainedModel.from_pretrained` or\n",
    "            :func:`~transformers.TFPreTrainedModel.from_pretrained` as ``pretrained_model_name_or_path`` if the\n",
    "            configuration was created with such a method.\n",
    "        output_hidden_states (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether or not the model should return all hidden-states.\n",
    "        output_attentions (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether or not the model should returns all attentions.\n",
    "        return_dict (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not the model should return a :class:`~transformers.file_utils.ModelOutput` instead of a plain\n",
    "            tuple.\n",
    "        is_encoder_decoder (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether the model is used as an encoder/decoder or not.\n",
    "        is_decoder (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether the model is used as decoder or not (in which case it's used as an encoder).\n",
    "        add_cross_attention (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether cross-attention layers should be added to the model. Note, this option is only relevant for models\n",
    "            that can be used as decoder models within the `:class:~transformers.EncoderDecoderModel` class, which\n",
    "            consists of all models in ``AUTO_MODELS_FOR_CAUSAL_LM``.\n",
    "        tie_encoder_decoder (:obj:`bool`, `optional`, defaults to :obj:`False`)\n",
    "            Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n",
    "            and decoder model to have the exact same parameter names.\n",
    "        prune_heads (:obj:`Dict[int, List[int]]`, `optional`, defaults to :obj:`{}`):\n",
    "            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n",
    "            heads to prune in said layer.\n",
    "\n",
    "            For instance ``{1: [0, 2], 2: [2, 3]}`` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n",
    "        xla_device (:obj:`bool`, `optional`):\n",
    "            A flag to indicate if TPU are available or not.\n",
    "        chunk_size_feed_forward (:obj:`int`, `optional`, defaults to :obj:`0`):\n",
    "            The chunk size of all feed forward layers in the residual attention blocks. A chunk size of :obj:`0` means\n",
    "            that the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes\n",
    "            :obj:`n` < sequence_length embeddings at a time. For more information on feed forward chunking, see `How\n",
    "            does Feed Forward Chunking work? <../glossary.html#feed-forward-chunking>`__ .\n",
    "\n",
    "    Parameters for sequence generation\n",
    "\n",
    "        - **max_length** (:obj:`int`, `optional`, defaults to 20) -- Maximum length that will be used by default in the\n",
    "          :obj:`generate` method of the model.\n",
    "        - **min_length** (:obj:`int`, `optional`, defaults to 10) -- Minimum length that will be used by default in the\n",
    "          :obj:`generate` method of the model.\n",
    "        - **do_sample** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Flag that will be used by default in the\n",
    "          :obj:`generate` method of the model. Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "        - **early_stopping** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Flag that will be used by default\n",
    "          in the :obj:`generate` method of the model. Whether to stop the beam search when at least ``num_beams``\n",
    "          sentences are finished per batch or not.\n",
    "        - **num_beams** (:obj:`int`, `optional`, defaults to 1) -- Number of beams for beam search that will be used by\n",
    "          default in the :obj:`generate` method of the model. 1 means no beam search.\n",
    "        - **num_beam_groups** (:obj:`int`, `optional`, defaults to 1) -- Number of groups to divide :obj:`num_beams`\n",
    "          into in order to ensure diversity among different groups of beams that will be used by default in the\n",
    "          :obj:`generate` method of the model. 1 means no group beam search.\n",
    "        - **diversity_penalty** (:obj:`float`, `optional`, defaults to 0.0) -- Value to control diversity for group\n",
    "          beam search. that will be used by default in the :obj:`generate` method of the model. 0 means no diversity\n",
    "          penalty. The higher the penalty, the more diverse are the outputs.\n",
    "        - **temperature** (:obj:`float`, `optional`, defaults to 1) -- The value used to module the next token\n",
    "          probabilities that will be used by default in the :obj:`generate` method of the model. Must be strictly\n",
    "          positive.\n",
    "        - **top_k** (:obj:`int`, `optional`, defaults to 50) -- Number of highest probability vocabulary tokens to keep\n",
    "          for top-k-filtering that will be used by default in the :obj:`generate` method of the model.\n",
    "        - **top_p** (:obj:`float`, `optional`, defaults to 1) -- Value that will be used by default in the\n",
    "          :obj:`generate` method of the model for ``top_p``. If set to float < 1, only the most probable tokens with\n",
    "          probabilities that add up to ``top_p`` or higher are kept for generation.\n",
    "        - **repetition_penalty** (:obj:`float`, `optional`, defaults to 1) -- Parameter for repetition penalty that\n",
    "          will be used by default in the :obj:`generate` method of the model. 1.0 means no penalty.\n",
    "        - **length_penalty** (:obj:`float`, `optional`, defaults to 1) -- Exponential penalty to the length that will\n",
    "          be used by default in the :obj:`generate` method of the model.\n",
    "        - **no_repeat_ngram_size** (:obj:`int`, `optional`, defaults to 0) -- Value that will be used by default in the\n",
    "          :obj:`generate` method of the model for ``no_repeat_ngram_size``. If set to int > 0, all ngrams of that size\n",
    "          can only occur once.\n",
    "        - **encoder_no_repeat_ngram_size** (:obj:`int`, `optional`, defaults to 0) -- Value that will be used by\n",
    "          default in the :obj:`generate` method of the model for ``encoder_no_repeat_ngram_size``. If set to int > 0,\n",
    "          all ngrams of that size that occur in the ``encoder_input_ids`` cannot occur in the ``decoder_input_ids``.\n",
    "        - **bad_words_ids** (:obj:`List[int]`, `optional`) -- List of token ids that are not allowed to be generated\n",
    "          that will be used by default in the :obj:`generate` method of the model. In order to get the tokens of the\n",
    "          words that should not appear in the generated text, use :obj:`tokenizer.encode(bad_word,\n",
    "          add_prefix_space=True)`.\n",
    "        - **num_return_sequences** (:obj:`int`, `optional`, defaults to 1) -- Number of independently computed returned\n",
    "          sequences for each element in the batch that will be used by default in the :obj:`generate` method of the\n",
    "          model.\n",
    "        - **output_scores** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Whether the model should return the\n",
    "          logits when used for generation\n",
    "        - **return_dict_in_generate** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Whether the model should\n",
    "          return a :class:`~transformers.file_utils.ModelOutput` instead of a :obj:`torch.LongTensor`\n",
    "\n",
    "\n",
    "    Parameters for fine-tuning tasks\n",
    "\n",
    "        - **architectures** (:obj:`List[str]`, `optional`) -- Model architectures that can be used with the model\n",
    "          pretrained weights.\n",
    "        - **finetuning_task** (:obj:`str`, `optional`) -- Name of the task used to fine-tune the model. This can be\n",
    "          used when converting from an original (TensorFlow or PyTorch) checkpoint.\n",
    "        - **id2label** (:obj:`Dict[int, str]`, `optional`) -- A map from index (for instance prediction index, or\n",
    "          target index) to label.\n",
    "        - **label2id** (:obj:`Dict[str, int]`, `optional`) -- A map from label to index for the model.\n",
    "        - **num_labels** (:obj:`int`, `optional`) -- Number of labels to use in the last layer added to the model,\n",
    "          typically for a classification task.\n",
    "        - **task_specific_params** (:obj:`Dict[str, Any]`, `optional`) -- Additional keyword arguments to store for the\n",
    "          current task.\n",
    "\n",
    "    Parameters linked to the tokenizer\n",
    "\n",
    "        - **tokenizer_class** (:obj:`str`, `optional`) -- The name of the associated tokenizer class to use (if none is\n",
    "          set, will use the tokenizer associated to the model by default).\n",
    "        - **prefix** (:obj:`str`, `optional`) -- A specific prompt that should be added at the beginning of each text\n",
    "          before calling the model.\n",
    "        - **bos_token_id** (:obj:`int`, `optional`)) -- The id of the `beginning-of-stream` token.\n",
    "        - **pad_token_id** (:obj:`int`, `optional`)) -- The id of the `padding` token.\n",
    "        - **eos_token_id** (:obj:`int`, `optional`)) -- The id of the `end-of-stream` token.\n",
    "        - **decoder_start_token_id** (:obj:`int`, `optional`)) -- If an encoder-decoder model starts decoding with a\n",
    "          different token than `bos`, the id of that token.\n",
    "        - **sep_token_id** (:obj:`int`, `optional`)) -- The id of the `separation` token.\n",
    "\n",
    "    PyTorch specific parameters\n",
    "\n",
    "        - **torchscript** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Whether or not the model should be\n",
    "          used with Torchscript.\n",
    "        - **tie_word_embeddings** (:obj:`bool`, `optional`, defaults to :obj:`True`) -- Whether the model's input and\n",
    "          output word embeddings should be tied. Note that this is only relevant if the model has a output word\n",
    "          embedding layer.\n",
    "\n",
    "    TensorFlow specific parameters\n",
    "\n",
    "        - **use_bfloat16** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Whether or not the model should use\n",
    "          BFloat16 scalars (only used by some TensorFlow models).\n",
    "    \"\"\"\n",
    "    model_type: str = \"\"\n",
    "    is_composition: bool = False\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Attributes with defaults\n",
    "        self.return_dict = kwargs.pop(\"return_dict\", True)\n",
    "        self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n",
    "        self.output_attentions = kwargs.pop(\"output_attentions\", False)\n",
    "        self.torchscript = kwargs.pop(\"torchscript\", False)  # Only used by PyTorch models\n",
    "        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n",
    "        self.pruned_heads = kwargs.pop(\"pruned_heads\", {})\n",
    "        self.tie_word_embeddings = kwargs.pop(\n",
    "            \"tie_word_embeddings\", True\n",
    "        )  # Whether input and output word embeddings should be tied for all MLM, LM and Seq2Seq models.\n",
    "\n",
    "        # Is decoder is used in encoder-decoder models to differentiate encoder from decoder\n",
    "        self.is_encoder_decoder = kwargs.pop(\"is_encoder_decoder\", False)\n",
    "        self.is_decoder = kwargs.pop(\"is_decoder\", False)\n",
    "        self.add_cross_attention = kwargs.pop(\"add_cross_attention\", False)\n",
    "        self.tie_encoder_decoder = kwargs.pop(\"tie_encoder_decoder\", False)\n",
    "\n",
    "        # Parameters for sequence generation\n",
    "        self.max_length = kwargs.pop(\"max_length\", 20)\n",
    "        self.min_length = kwargs.pop(\"min_length\", 0)\n",
    "        self.do_sample = kwargs.pop(\"do_sample\", False)\n",
    "        self.early_stopping = kwargs.pop(\"early_stopping\", False)\n",
    "        self.num_beams = kwargs.pop(\"num_beams\", 1)\n",
    "        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n",
    "        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n",
    "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
    "        self.top_k = kwargs.pop(\"top_k\", 50)\n",
    "        self.top_p = kwargs.pop(\"top_p\", 1.0)\n",
    "        self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n",
    "        self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n",
    "        self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", 0)\n",
    "        self.encoder_no_repeat_ngram_size = kwargs.pop(\"encoder_no_repeat_ngram_size\", 0)\n",
    "        self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n",
    "        self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n",
    "        self.chunk_size_feed_forward = kwargs.pop(\"chunk_size_feed_forward\", 0)\n",
    "        self.output_scores = kwargs.pop(\"output_scores\", False)\n",
    "        self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", False)\n",
    "\n",
    "        # Fine-tuning task arguments\n",
    "        self.architectures = kwargs.pop(\"architectures\", None)\n",
    "        self.finetuning_task = kwargs.pop(\"finetuning_task\", None)\n",
    "        self.id2label = kwargs.pop(\"id2label\", None)\n",
    "        self.label2id = kwargs.pop(\"label2id\", None)\n",
    "        if self.id2label is not None:\n",
    "            kwargs.pop(\"num_labels\", None)\n",
    "            self.id2label = dict((int(key), value) for key, value in self.id2label.items())\n",
    "            # Keys are always strings in JSON so convert ids to int here.\n",
    "        else:\n",
    "            self.num_labels = kwargs.pop(\"num_labels\", 2)\n",
    "\n",
    "        # Tokenizer arguments TODO: eventually tokenizer and models should share the same config\n",
    "        self.tokenizer_class = kwargs.pop(\"tokenizer_class\", None)\n",
    "        self.prefix = kwargs.pop(\"prefix\", None)\n",
    "        self.bos_token_id = kwargs.pop(\"bos_token_id\", None)\n",
    "        self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n",
    "        self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n",
    "        self.sep_token_id = kwargs.pop(\"sep_token_id\", None)\n",
    "\n",
    "        self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n",
    "\n",
    "        # task specific arguments\n",
    "        self.task_specific_params = kwargs.pop(\"task_specific_params\", None)\n",
    "\n",
    "        # TPU arguments\n",
    "        self.xla_device = kwargs.pop(\"xla_device\", None)\n",
    "\n",
    "        # Name or path to the pretrained checkpoint\n",
    "        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n",
    "\n",
    "        # Drop the transformers version info\n",
    "        kwargs.pop(\"transformers_version\", None)\n",
    "\n",
    "        # Additional attributes without default values\n",
    "        for key, value in kwargs.items():\n",
    "            try:\n",
    "                setattr(self, key, value)\n",
    "            except AttributeError as err:\n",
    "                logger.error(\"Can't set {} with value {} for {}\".format(key, value, self))\n",
    "                raise err\n",
    "\n",
    "    @property\n",
    "    def name_or_path(self) -> str:\n",
    "        return self._name_or_path\n",
    "\n",
    "    @name_or_path.setter\n",
    "    def name_or_path(self, value):\n",
    "        self._name_or_path = str(value)  # Make sure that name_or_path is a string (for JSON encoding)\n",
    "\n",
    "    @property\n",
    "    def use_return_dict(self) -> bool:\n",
    "        \"\"\"\n",
    "        :obj:`bool`: Whether or not return :class:`~transformers.file_utils.ModelOutput` instead of tuples.\n",
    "        \"\"\"\n",
    "        # If torchscript is set, force `return_dict=False` to avoid jit errors\n",
    "        return self.return_dict and not self.torchscript\n",
    "\n",
    "    @property\n",
    "    def num_labels(self) -> int:\n",
    "        \"\"\"\n",
    "        :obj:`int`: The number of labels for classification models.\n",
    "        \"\"\"\n",
    "        return len(self.id2label)\n",
    "\n",
    "    @num_labels.setter\n",
    "    def num_labels(self, num_labels: int):\n",
    "        self.id2label = {i: \"LABEL_{}\".format(i) for i in range(num_labels)}\n",
    "        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike]):\n",
    "        \"\"\"\n",
    "        Save a configuration object to the directory ``save_directory``, so that it can be re-loaded using the\n",
    "        :func:`~transformers.PretrainedConfig.from_pretrained` class method.\n",
    "\n",
    "        Args:\n",
    "            save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_directory):\n",
    "            raise AssertionError(\"Provided path ({}) should be a directory, not a file\".format(save_directory))\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n",
    "\n",
    "        self.to_json_file(output_config_file, use_diff=True)\n",
    "        logger.info(\"Configuration saved in {}\".format(output_config_file))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n",
    "        r\"\"\"\n",
    "        Instantiate a :class:`~transformers.PretrainedConfig` (or a derived class) from a pretrained model\n",
    "        configuration.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
    "                This can be either:\n",
    "\n",
    "                - a string, the `model id` of a pretrained model configuration hosted inside a model repo on\n",
    "                  huggingface.co. Valid model ids can be located at the root-level, like ``bert-base-uncased``, or\n",
    "                  namespaced under a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                - a path to a `directory` containing a configuration file saved using the\n",
    "                  :func:`~transformers.PretrainedConfig.save_pretrained` method, e.g., ``./my_model_directory/``.\n",
    "                - a path or url to a saved configuration JSON `file`, e.g.,\n",
    "                  ``./my_model_directory/configuration.json``.\n",
    "            cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
    "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
    "                standard cache should not be used.\n",
    "            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to force to (re-)download the configuration files and override the cached versions if\n",
    "                they exist.\n",
    "            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to delete incompletely received file. Attempts to resume the download if such a file\n",
    "                exists.\n",
    "            proxies (:obj:`Dict[str, str]`, `optional`):\n",
    "                A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
    "                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n",
    "            use_auth_token (:obj:`str` or `bool`, `optional`):\n",
    "                The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
    "                generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
    "            revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
    "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
    "                git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
    "                identifier allowed by git.\n",
    "            return_unused_kwargs (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                If :obj:`False`, then this function returns just the final configuration object.\n",
    "\n",
    "                If :obj:`True`, then this functions returns a :obj:`Tuple(config, unused_kwargs)` where `unused_kwargs`\n",
    "                is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e.,\n",
    "                the part of ``kwargs`` which has not been used to update ``config`` and is otherwise ignored.\n",
    "            kwargs (:obj:`Dict[str, Any]`, `optional`):\n",
    "                The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n",
    "                values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n",
    "                by the ``return_unused_kwargs`` keyword parameter.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            :class:`PretrainedConfig`: The configuration object instantiated from this pretrained model.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a\n",
    "            # derived class: BertConfig\n",
    "            config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from huggingface.co and cache.\n",
    "            config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n",
    "            config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n",
    "            config = BertConfig.from_pretrained('bert-base-uncased', output_attentions=True, foo=False)\n",
    "            assert config.output_attentions == True\n",
    "            config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attentions=True,\n",
    "                                                               foo=False, return_unused_kwargs=True)\n",
    "            assert config.output_attentions == True\n",
    "            assert unused_kwargs == {'foo': False}\n",
    "\n",
    "        \"\"\"\n",
    "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
    "        return cls.from_dict(config_dict, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_config_dict(\n",
    "        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n",
    "    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        From a ``pretrained_model_name_or_path``, resolve to a dictionary of parameters, to be used for instantiating a\n",
    "        :class:`~transformers.PretrainedConfig` using ``from_dict``.\n",
    "\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
    "                The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.\n",
    "\n",
    "        \"\"\"\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "        revision = kwargs.pop(\"revision\", None)\n",
    "\n",
    "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
    "        if os.path.isdir(pretrained_model_name_or_path):\n",
    "            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n",
    "        elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "            config_file = pretrained_model_name_or_path\n",
    "        else:\n",
    "            config_file = hf_bucket_url(\n",
    "                pretrained_model_name_or_path, filename=CONFIG_NAME, revision=revision, mirror=None\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Load from URL or cache if already cached\n",
    "            resolved_config_file = cached_path(\n",
    "                config_file,\n",
    "                cache_dir=cache_dir,\n",
    "                force_download=force_download,\n",
    "                proxies=proxies,\n",
    "                resume_download=resume_download,\n",
    "                local_files_only=local_files_only,\n",
    "                use_auth_token=use_auth_token,\n",
    "            )\n",
    "            # Load config dict\n",
    "            config_dict = cls._dict_from_json_file(resolved_config_file)\n",
    "\n",
    "        except EnvironmentError as err:\n",
    "            logger.error(err)\n",
    "            msg = (\n",
    "                f\"Can't load config for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
    "                f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\n",
    "            )\n",
    "            raise EnvironmentError(msg)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            msg = (\n",
    "                \"Couldn't reach server at '{}' to download configuration file or \"\n",
    "                \"configuration file is not a valid JSON file. \"\n",
    "                \"Please check network or file content here: {}.\".format(config_file, resolved_config_file)\n",
    "            )\n",
    "            raise EnvironmentError(msg)\n",
    "\n",
    "        if resolved_config_file == config_file:\n",
    "            logger.info(\"loading configuration file {}\".format(config_file))\n",
    "        else:\n",
    "            logger.info(\"loading configuration file {} from cache at {}\".format(config_file, resolved_config_file))\n",
    "\n",
    "        return config_dict, kwargs\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n",
    "        \"\"\"\n",
    "        Instantiates a :class:`~transformers.PretrainedConfig` from a Python dictionary of parameters.\n",
    "\n",
    "        Args:\n",
    "            config_dict (:obj:`Dict[str, Any]`):\n",
    "                Dictionary that will be used to instantiate the configuration object. Such a dictionary can be\n",
    "                retrieved from a pretrained checkpoint by leveraging the\n",
    "                :func:`~transformers.PretrainedConfig.get_config_dict` method.\n",
    "            kwargs (:obj:`Dict[str, Any]`):\n",
    "                Additional parameters from which to initialize the configuration object.\n",
    "\n",
    "        Returns:\n",
    "            :class:`PretrainedConfig`: The configuration object instantiated from those parameters.\n",
    "        \"\"\"\n",
    "        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n",
    "\n",
    "        config = cls(**config_dict)\n",
    "\n",
    "        if hasattr(config, \"pruned_heads\"):\n",
    "            config.pruned_heads = dict((int(key), value) for key, value in config.pruned_heads.items())\n",
    "\n",
    "        # Update config with kwargs if needed\n",
    "        to_remove = []\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(config, key):\n",
    "                setattr(config, key, value)\n",
    "                to_remove.append(key)\n",
    "        for key in to_remove:\n",
    "            kwargs.pop(key, None)\n",
    "\n",
    "        logger.info(\"Model config %s\", str(config))\n",
    "        if return_unused_kwargs:\n",
    "            return config, kwargs\n",
    "        else:\n",
    "            return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file: Union[str, os.PathLike]) -> \"PretrainedConfig\":\n",
    "        \"\"\"\n",
    "        Instantiates a :class:`~transformers.PretrainedConfig` from the path to a JSON file of parameters.\n",
    "\n",
    "        Args:\n",
    "            json_file (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Path to the JSON file containing the parameters.\n",
    "\n",
    "        Returns:\n",
    "            :class:`PretrainedConfig`: The configuration object instantiated from that JSON file.\n",
    "\n",
    "        \"\"\"\n",
    "        config_dict = cls._dict_from_json_file(json_file)\n",
    "        return cls(**config_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "            text = reader.read()\n",
    "        return json.loads(text)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.__dict__ == other.__dict__\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{} {}\".format(self.__class__.__name__, self.to_json_string())\n",
    "\n",
    "    def to_diff_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Removes all attributes from config which correspond to the default config attributes for better readability and\n",
    "        serializes to a Python dictionary.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n",
    "        \"\"\"\n",
    "        config_dict = self.to_dict()\n",
    "\n",
    "        # get the default config dict\n",
    "        default_config_dict = PretrainedConfig().to_dict()\n",
    "\n",
    "        # get class specific config dict\n",
    "        class_config_dict = self.__class__().to_dict() if not self.is_composition else {}\n",
    "\n",
    "        serializable_config_dict = {}\n",
    "\n",
    "        # only serialize values that differ from the default config\n",
    "        for key, value in config_dict.items():\n",
    "            if (\n",
    "                key not in default_config_dict\n",
    "                or key == \"transformers_version\"\n",
    "                or value != default_config_dict[key]\n",
    "                or (key in class_config_dict and value != class_config_dict[key])\n",
    "            ):\n",
    "                serializable_config_dict[key] = value\n",
    "\n",
    "        return serializable_config_dict\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Serializes this instance to a Python dictionary.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n",
    "        \"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        if hasattr(self.__class__, \"model_type\"):\n",
    "            output[\"model_type\"] = self.__class__.model_type\n",
    "\n",
    "        # Transformers version when serializing the model\n",
    "        output[\"transformers_version\"] = __version__\n",
    "\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self, use_diff: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Serializes this instance to a JSON string.\n",
    "\n",
    "        Args:\n",
    "            use_diff (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                If set to ``True``, only the difference between the config instance and the default\n",
    "                ``PretrainedConfig()`` is serialized to JSON string.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`str`: String containing all the attributes that make up this configuration instance in JSON format.\n",
    "        \"\"\"\n",
    "        if use_diff is True:\n",
    "            config_dict = self.to_diff_dict()\n",
    "        else:\n",
    "            config_dict = self.to_dict()\n",
    "        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "    def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True):\n",
    "        \"\"\"\n",
    "        Save this instance to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            json_file_path (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Path to the JSON file in which this configuration instance's parameters will be saved.\n",
    "            use_diff (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                If set to ``True``, only the difference between the config instance and the default\n",
    "                ``PretrainedConfig()`` is serialized to JSON file.\n",
    "        \"\"\"\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(self.to_json_string(use_diff=use_diff))\n",
    "\n",
    "    def update(self, config_dict: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Updates attributes of this class with attributes from ``config_dict``.\n",
    "\n",
    "        Args:\n",
    "            config_dict (:obj:`Dict[str, Any]`): Dictionary of attributes that shall be updated for this class.\n",
    "        \"\"\"\n",
    "        for key, value in config_dict.items():\n",
    "            setattr(self, key, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer util base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:39.824605Z",
     "start_time": "2021-02-10T14:49:39.582298Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Base classes common to both the slow and the fast tokenization classes: PreTrainedTokenizerBase (host all the user\n",
    "fronting encoding methods) Special token mixing (host the special tokens logic) and BatchEncoding (wrap the dictionary\n",
    "of output with special method for the Fast tokenizers)\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from collections import OrderedDict, UserDict\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, NamedTuple, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "# from .file_utils import (\n",
    "#     add_end_docstrings,\n",
    "#     cached_path,\n",
    "#     hf_bucket_url,\n",
    "#     is_flax_available,\n",
    "#     is_remote_url,\n",
    "#     is_tf_available,\n",
    "#     is_tokenizers_available,\n",
    "#     is_torch_available,\n",
    "#     torch_required,\n",
    "# )\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    if is_torch_available():\n",
    "        import torch\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "    if is_flax_available():\n",
    "        import jax.numpy as jnp  # noqa: F401\n",
    "\n",
    "\n",
    "def _is_numpy(x):\n",
    "    return isinstance(x, np.ndarray)\n",
    "\n",
    "\n",
    "def _is_torch(x):\n",
    "    import torch\n",
    "\n",
    "    return isinstance(x, torch.Tensor)\n",
    "\n",
    "\n",
    "def _is_torch_device(x):\n",
    "    import torch\n",
    "\n",
    "    return isinstance(x, torch.device)\n",
    "\n",
    "\n",
    "def _is_tensorflow(x):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    return isinstance(x, tf.Tensor)\n",
    "\n",
    "\n",
    "def _is_jax(x):\n",
    "    import jax.numpy as jnp  # noqa: F811\n",
    "\n",
    "    return isinstance(x, jnp.ndarray)\n",
    "\n",
    "\n",
    "if is_tokenizers_available():\n",
    "    from tokenizers import AddedToken\n",
    "    from tokenizers import Encoding as EncodingFast\n",
    "else:\n",
    "\n",
    "    @dataclass(frozen=True, eq=True)\n",
    "    class AddedToken:\n",
    "        \"\"\"\n",
    "        AddedToken represents a token to be added to a Tokenizer An AddedToken can have special options defining the\n",
    "        way it should behave.\n",
    "        \"\"\"\n",
    "\n",
    "        content: str = field(default_factory=str)\n",
    "        single_word: bool = False\n",
    "        lstrip: bool = False\n",
    "        rstrip: bool = False\n",
    "        normalized: bool = True\n",
    "\n",
    "        def __getstate__(self):\n",
    "            return self.__dict__\n",
    "\n",
    "    @dataclass\n",
    "    class EncodingFast:\n",
    "        \"\"\" This is dummy class because without the `tokenizers` library we don't have these objects anyway \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "VERY_LARGE_INTEGER = int(1e30)  # This is used to set the max input length for a model with infinite size input\n",
    "LARGE_INTEGER = int(1e20)  # This is used when we need something big but slightly smaller than VERY_LARGE_INTEGER\n",
    "\n",
    "# Define type aliases and NamedTuples\n",
    "TextInput = str\n",
    "PreTokenizedInput = List[str]\n",
    "EncodedInput = List[int]\n",
    "TextInputPair = Tuple[str, str]\n",
    "PreTokenizedInputPair = Tuple[List[str], List[str]]\n",
    "EncodedInputPair = Tuple[List[int], List[int]]\n",
    "\n",
    "\n",
    "# Slow tokenizers used to be saved in three separated files\n",
    "SPECIAL_TOKENS_MAP_FILE = \"special_tokens_map.json\"\n",
    "ADDED_TOKENS_FILE = \"added_tokens.json\"\n",
    "TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n",
    "\n",
    "# Fast tokenizers (provided by HuggingFace tokenizer's library) can be saved in a single file\n",
    "FULL_TOKENIZER_FILE = \"tokenizer.json\"\n",
    "\n",
    "\n",
    "class ExplicitEnum(Enum):\n",
    "    \"\"\"\n",
    "    Enum with more explicit error message for missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _missing_(cls, value):\n",
    "        raise ValueError(\n",
    "            \"%r is not a valid %s, please select one of %s\"\n",
    "            % (value, cls.__name__, str(list(cls._value2member_map_.keys())))\n",
    "        )\n",
    "\n",
    "\n",
    "class TruncationStrategy(ExplicitEnum):\n",
    "    \"\"\"\n",
    "    Possible values for the ``truncation`` argument in :meth:`PreTrainedTokenizerBase.__call__`. Useful for\n",
    "    tab-completion in an IDE.\n",
    "    \"\"\"\n",
    "\n",
    "    ONLY_FIRST = \"only_first\"\n",
    "    ONLY_SECOND = \"only_second\"\n",
    "    LONGEST_FIRST = \"longest_first\"\n",
    "    DO_NOT_TRUNCATE = \"do_not_truncate\"\n",
    "\n",
    "\n",
    "class PaddingStrategy(ExplicitEnum):\n",
    "    \"\"\"\n",
    "    Possible values for the ``padding`` argument in :meth:`PreTrainedTokenizerBase.__call__`. Useful for tab-completion\n",
    "    in an IDE.\n",
    "    \"\"\"\n",
    "\n",
    "    LONGEST = \"longest\"\n",
    "    MAX_LENGTH = \"max_length\"\n",
    "    DO_NOT_PAD = \"do_not_pad\"\n",
    "\n",
    "\n",
    "class TensorType(ExplicitEnum):\n",
    "    \"\"\"\n",
    "    Possible values for the ``return_tensors`` argument in :meth:`PreTrainedTokenizerBase.__call__`. Useful for\n",
    "    tab-completion in an IDE.\n",
    "    \"\"\"\n",
    "\n",
    "    PYTORCH = \"pt\"\n",
    "    TENSORFLOW = \"tf\"\n",
    "    NUMPY = \"np\"\n",
    "    JAX = \"jax\"\n",
    "\n",
    "\n",
    "class CharSpan(NamedTuple):\n",
    "    \"\"\"\n",
    "    Character span in the original string.\n",
    "\n",
    "    Args:\n",
    "        start (:obj:`int`): Index of the first character in the original string.\n",
    "        end (:obj:`int`): Index of the character following the last character in the original string.\n",
    "    \"\"\"\n",
    "\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "\n",
    "class TokenSpan(NamedTuple):\n",
    "    \"\"\"\n",
    "    Token span in an encoded string (list of tokens).\n",
    "\n",
    "    Args:\n",
    "        start (:obj:`int`): Index of the first token in the span.\n",
    "        end (:obj:`int`): Index of the token following the last token in the span.\n",
    "    \"\"\"\n",
    "\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "\n",
    "def to_py_obj(obj):\n",
    "    \"\"\"\n",
    "    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (dict, BatchEncoding)):\n",
    "        return {k: to_py_obj(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [to_py_obj(o) for o in obj]\n",
    "    elif is_tf_available() and _is_tensorflow(obj):\n",
    "        return obj.numpy().tolist()\n",
    "    elif is_torch_available() and _is_torch(obj):\n",
    "        return obj.detach().cpu().tolist()\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "class BatchEncoding(UserDict):\n",
    "    \"\"\"\n",
    "    Holds the output of the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus` and\n",
    "    :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode` methods (tokens,\n",
    "    attention_masks, etc).\n",
    "\n",
    "    This class is derived from a python dictionary and can be used as a dictionary. In addition, this class exposes\n",
    "    utility methods to map from word/character space to token space.\n",
    "\n",
    "    Args:\n",
    "        data (:obj:`dict`):\n",
    "            Dictionary of lists/arrays/tensors returned by the encode/batch_encode methods ('input_ids',\n",
    "            'attention_mask', etc.).\n",
    "        encoding (:obj:`tokenizers.Encoding` or :obj:`Sequence[tokenizers.Encoding]`, `optional`):\n",
    "            If the tokenizer is a fast tokenizer which outputs additional information like mapping from word/character\n",
    "            space to token space the :obj:`tokenizers.Encoding` instance or list of instance (for batches) hold this\n",
    "            information.\n",
    "        tensor_type (:obj:`Union[None, str, TensorType]`, `optional`):\n",
    "            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n",
    "            initialization.\n",
    "        prepend_batch_axis (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether or not to add a batch axis when converting to tensors (see :obj:`tensor_type` above).\n",
    "        n_sequences (:obj:`Optional[int]`, `optional`):\n",
    "            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n",
    "            initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Optional[Dict[str, Any]] = None,\n",
    "        encoding: Optional[Union[EncodingFast, Sequence[EncodingFast]]] = None,\n",
    "        tensor_type: Union[None, str, TensorType] = None,\n",
    "        prepend_batch_axis: bool = False,\n",
    "        n_sequences: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__(data)\n",
    "\n",
    "        if isinstance(encoding, EncodingFast):\n",
    "            encoding = [encoding]\n",
    "\n",
    "        self._encodings = encoding\n",
    "\n",
    "        if n_sequences is None and encoding is not None and len(encoding):\n",
    "            n_sequences = encoding[0].n_sequences\n",
    "\n",
    "        self._n_sequences = n_sequences\n",
    "\n",
    "        self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
    "\n",
    "    @property\n",
    "    def n_sequences(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[int]`: The number of sequences used to generate each sample from the batch encoded in this\n",
    "        :class:`~transformers.BatchEncoding`. Currently can be one of :obj:`None` (unknown), :obj:`1` (a single\n",
    "        sentence) or :obj:`2` (a pair of sentences)\n",
    "        \"\"\"\n",
    "        return self._n_sequences\n",
    "\n",
    "    @property\n",
    "    def is_fast(self) -> bool:\n",
    "        \"\"\"\n",
    "        :obj:`bool`: Indicate whether this :class:`~transformers.BatchEncoding` was generated from the result of a\n",
    "        :class:`~transformers.PreTrainedTokenizerFast` or not.\n",
    "        \"\"\"\n",
    "        return self._encodings is not None\n",
    "\n",
    "    def __getitem__(self, item: Union[int, str]) -> Union[Any, EncodingFast]:\n",
    "        \"\"\"\n",
    "        If the key is a string, returns the value of the dict associated to :obj:`key` ('input_ids', 'attention_mask',\n",
    "        etc.).\n",
    "\n",
    "        If the key is an integer, get the :obj:`tokenizers.Encoding` for batch item with index :obj:`key`.\n",
    "        \"\"\"\n",
    "        if isinstance(item, str):\n",
    "            return self.data[item]\n",
    "        elif self._encodings is not None:\n",
    "            return self._encodings[item]\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"Indexing with integers (to access backend Encoding for a given batch index) \"\n",
    "                \"is not available when using Python based tokenizers\"\n",
    "            )\n",
    "\n",
    "    def __getattr__(self, item: str):\n",
    "        try:\n",
    "            return self.data[item]\n",
    "        except KeyError:\n",
    "            raise AttributeError\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\"data\": self.data, \"encodings\": self._encodings}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if \"data\" in state:\n",
    "            self.data = state[\"data\"]\n",
    "\n",
    "        if \"encodings\" in state:\n",
    "            self._encodings = state[\"encodings\"]\n",
    "\n",
    "    def keys(self):\n",
    "        return self.data.keys()\n",
    "\n",
    "    def values(self):\n",
    "        return self.data.values()\n",
    "\n",
    "    def items(self):\n",
    "        return self.data.items()\n",
    "\n",
    "    # After this point:\n",
    "    # Extended properties and methods only available for fast (Rust-based) tokenizers\n",
    "    # provided by HuggingFace tokenizers library.\n",
    "\n",
    "    @property\n",
    "    def encodings(self) -> Optional[List[EncodingFast]]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[List[tokenizers.Encoding]]`: The list all encodings from the tokenization process. Returns\n",
    "        :obj:`None` if the input was tokenized through Python (i.e., not a fast) tokenizer.\n",
    "        \"\"\"\n",
    "        return self._encodings\n",
    "\n",
    "    def tokens(self, batch_index: int = 0) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to\n",
    "        integer indices) at a given batch index (only works for the output of a fast tokenizer).\n",
    "\n",
    "        Args:\n",
    "            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[str]`: The list of tokens at that index.\n",
    "        \"\"\"\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"tokens() is not available when using Python-based tokenizers\")\n",
    "        return self._encodings[batch_index].tokens\n",
    "\n",
    "    def sequence_ids(self, batch_index: int = 0) -> List[Optional[int]]:\n",
    "        \"\"\"\n",
    "        Return a list mapping the tokens to the id of their original sentences:\n",
    "\n",
    "            - :obj:`None` for special tokens added around or between sequences,\n",
    "            - :obj:`0` for tokens corresponding to words in the first sequence,\n",
    "            - :obj:`1` for tokens corresponding to words in the second sequence when a pair of sequences was jointly\n",
    "              encoded.\n",
    "\n",
    "        Args:\n",
    "            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[Optional[int]]`: A list indicating the sequence id corresponding to each token. Special tokens\n",
    "            added by the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their\n",
    "            corresponding sequence.\n",
    "        \"\"\"\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"sequence_ids() is not available when using Python-based tokenizers\")\n",
    "        return self._encodings[batch_index].sequence_ids\n",
    "\n",
    "    def words(self, batch_index: int = 0) -> List[Optional[int]]:\n",
    "        \"\"\"\n",
    "        Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\n",
    "\n",
    "        Args:\n",
    "            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[Optional[int]]`: A list indicating the word corresponding to each token. Special tokens added by\n",
    "            the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their corresponding\n",
    "            word (several tokens will be mapped to the same word index if they are parts of that word).\n",
    "        \"\"\"\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"words() is not available when using Python-based tokenizers\")\n",
    "        warnings.warn(\n",
    "            \"`BatchEncoding.words()` property is deprecated and should be replaced with the identical, \"\n",
    "            \"but more self-explanatory `BatchEncoding.word_ids()` property.\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "        return self.word_ids(batch_index)\n",
    "\n",
    "    def word_ids(self, batch_index: int = 0) -> List[Optional[int]]:\n",
    "        \"\"\"\n",
    "        Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\n",
    "\n",
    "        Args:\n",
    "            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[Optional[int]]`: A list indicating the word corresponding to each token. Special tokens added by\n",
    "            the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their corresponding\n",
    "            word (several tokens will be mapped to the same word index if they are parts of that word).\n",
    "        \"\"\"\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"word_ids() is not available when using Python-based tokenizers\")\n",
    "        return self._encodings[batch_index].word_ids\n",
    "\n",
    "    def token_to_sequence(self, batch_or_token_index: int, token_index: Optional[int] = None) -> int:\n",
    "        \"\"\"\n",
    "        Get the index of the sequence represented by the given token. In the general use case, this method returns\n",
    "        :obj:`0` for a single sequence or the first sequence of a pair, and :obj:`1` for the second sequence of a pair\n",
    "\n",
    "        Can be called as:\n",
    "\n",
    "        - ``self.token_to_sequence(token_index)`` if batch size is 1\n",
    "        - ``self.token_to_sequence(batch_index, token_index)`` if batch size is greater than 1\n",
    "\n",
    "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,\n",
    "        words are defined by the user). In this case it allows to easily associate encoded tokens with provided\n",
    "        tokenized words.\n",
    "\n",
    "        Args:\n",
    "            batch_or_token_index (:obj:`int`):\n",
    "                Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of\n",
    "                the token in the sequence.\n",
    "            token_index (:obj:`int`, `optional`):\n",
    "                If a batch index is provided in `batch_or_token_index`, this can be the index of the token in the\n",
    "                sequence.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: Index of the word in the input sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"token_to_sequence() is not available when using Python based tokenizers\")\n",
    "        if token_index is not None:\n",
    "            batch_index = batch_or_token_index\n",
    "        else:\n",
    "            batch_index = 0\n",
    "            token_index = batch_or_token_index\n",
    "        if batch_index < 0:\n",
    "            batch_index = self._batch_size + batch_index\n",
    "        if token_index < 0:\n",
    "            token_index = self._seq_len + token_index\n",
    "        return self._encodings[batch_index].token_to_sequence(token_index)\n",
    "\n",
    "    def token_to_word(self, batch_or_token_index: int, token_index: Optional[int] = None) -> int:\n",
    "        \"\"\"\n",
    "        Get the index of the word corresponding (i.e. comprising) to an encoded token in a sequence of the batch.\n",
    "\n",
    "        Can be called as:\n",
    "\n",
    "        - ``self.token_to_word(token_index)`` if batch size is 1\n",
    "        - ``self.token_to_word(batch_index, token_index)`` if batch size is greater than 1\n",
    "\n",
    "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,\n",
    "        words are defined by the user). In this case it allows to easily associate encoded tokens with provided\n",
    "        tokenized words.\n",
    "\n",
    "        Args:\n",
    "            batch_or_token_index (:obj:`int`):\n",
    "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
    "                the token in the sequence.\n",
    "            token_index (:obj:`int`, `optional`):\n",
    "                If a batch index is provided in `batch_or_token_index`, this can be the index of the token in the\n",
    "                sequence.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: Index of the word in the input sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"token_to_word() is not available when using Python based tokenizers\")\n",
    "        if token_index is not None:\n",
    "            batch_index = batch_or_token_index\n",
    "        else:\n",
    "            batch_index = 0\n",
    "            token_index = batch_or_token_index\n",
    "        if batch_index < 0:\n",
    "            batch_index = self._batch_size + batch_index\n",
    "        if token_index < 0:\n",
    "            token_index = self._seq_len + token_index\n",
    "        return self._encodings[batch_index].token_to_word(token_index)\n",
    "\n",
    "    def word_to_tokens(\n",
    "        self, batch_or_word_index: int, word_index: Optional[int] = None, sequence_index: int = 0\n",
    "    ) -> Optional[TokenSpan]:\n",
    "        \"\"\"\n",
    "        Get the encoded token span corresponding to a word in a sequence of the batch.\n",
    "\n",
    "        Token spans are returned as a :class:`~transformers.tokenization_utils_base.TokenSpan` with:\n",
    "\n",
    "        - **start** -- Index of the first token.\n",
    "        - **end** -- Index of the token following the last token.\n",
    "\n",
    "        Can be called as:\n",
    "\n",
    "        - ``self.word_to_tokens(word_index, sequence_index: int = 0)`` if batch size is 1\n",
    "        - ``self.word_to_tokens(batch_index, word_index, sequence_index: int = 0)`` if batch size is greater or equal\n",
    "          to 1\n",
    "\n",
    "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words\n",
    "        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized\n",
    "        words.\n",
    "\n",
    "        Args:\n",
    "            batch_or_word_index (:obj:`int`):\n",
    "                Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of\n",
    "                the word in the sequence.\n",
    "            word_index (:obj:`int`, `optional`):\n",
    "                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the\n",
    "                sequence.\n",
    "            sequence_index (:obj:`int`, `optional`, defaults to 0):\n",
    "                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0\n",
    "                or 1) the provided word index belongs to.\n",
    "\n",
    "        Returns:\n",
    "            Optional :class:`~transformers.tokenization_utils_base.TokenSpan` Span of tokens in the encoded sequence.\n",
    "            Returns :obj:`None` if no tokens correspond to the word.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"word_to_tokens() is not available when using Python based tokenizers\")\n",
    "        if word_index is not None:\n",
    "            batch_index = batch_or_word_index\n",
    "        else:\n",
    "            batch_index = 0\n",
    "            word_index = batch_or_word_index\n",
    "        if batch_index < 0:\n",
    "            batch_index = self._batch_size + batch_index\n",
    "        if word_index < 0:\n",
    "            word_index = self._seq_len + word_index\n",
    "        span = self._encodings[batch_index].word_to_tokens(word_index, sequence_index)\n",
    "        return TokenSpan(*span) if span is not None else None\n",
    "\n",
    "    def token_to_chars(self, batch_or_token_index: int, token_index: Optional[int] = None) -> CharSpan:\n",
    "        \"\"\"\n",
    "        Get the character span corresponding to an encoded token in a sequence of the batch.\n",
    "\n",
    "        Character spans are returned as a :class:`~transformers.tokenization_utils_base.CharSpan` with:\n",
    "\n",
    "        - **start** -- Index of the first character in the original string associated to the token.\n",
    "        - **end** -- Index of the character following the last character in the original string associated to the\n",
    "          token.\n",
    "\n",
    "        Can be called as:\n",
    "\n",
    "        - ``self.token_to_chars(token_index)`` if batch size is 1\n",
    "        - ``self.token_to_chars(batch_index, token_index)`` if batch size is greater or equal to 1\n",
    "\n",
    "        Args:\n",
    "            batch_or_token_index (:obj:`int`):\n",
    "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
    "                the token in the sequence.\n",
    "            token_index (:obj:`int`, `optional`):\n",
    "                If a batch index is provided in `batch_or_token_index`, this can be the index of the token or tokens in\n",
    "                the sequence.\n",
    "\n",
    "        Returns:\n",
    "            :class:`~transformers.tokenization_utils_base.CharSpan`: Span of characters in the original string.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"token_to_chars() is not available when using Python based tokenizers\")\n",
    "        if token_index is not None:\n",
    "            batch_index = batch_or_token_index\n",
    "        else:\n",
    "            batch_index = 0\n",
    "            token_index = batch_or_token_index\n",
    "        return CharSpan(*(self._encodings[batch_index].token_to_chars(token_index)))\n",
    "\n",
    "    def char_to_token(\n",
    "        self, batch_or_char_index: int, char_index: Optional[int] = None, sequence_index: int = 0\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Get the index of the token in the encoded output comprising a character in the original string for a sequence\n",
    "        of the batch.\n",
    "\n",
    "        Can be called as:\n",
    "\n",
    "        - ``self.char_to_token(char_index)`` if batch size is 1\n",
    "        - ``self.char_to_token(batch_index, char_index)`` if batch size is greater or equal to 1\n",
    "\n",
    "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words\n",
    "        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized\n",
    "        words.\n",
    "\n",
    "        Args:\n",
    "            batch_or_char_index (:obj:`int`):\n",
    "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
    "                the word in the sequence\n",
    "            char_index (:obj:`int`, `optional`):\n",
    "                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the\n",
    "                sequence.\n",
    "            sequence_index (:obj:`int`, `optional`, defaults to 0):\n",
    "                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0\n",
    "                or 1) the provided character index belongs to.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: Index of the token.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"char_to_token() is not available when using Python based tokenizers\")\n",
    "        if char_index is not None:\n",
    "            batch_index = batch_or_char_index\n",
    "        else:\n",
    "            batch_index = 0\n",
    "            char_index = batch_or_char_index\n",
    "        return self._encodings[batch_index].char_to_token(char_index, sequence_index)\n",
    "\n",
    "    def word_to_chars(\n",
    "        self, batch_or_word_index: int, word_index: Optional[int] = None, sequence_index: int = 0\n",
    "    ) -> CharSpan:\n",
    "        \"\"\"\n",
    "        Get the character span in the original string corresponding to given word in a sequence of the batch.\n",
    "\n",
    "        Character spans are returned as a CharSpan NamedTuple with:\n",
    "\n",
    "        - start: index of the first character in the original string\n",
    "        - end: index of the character following the last character in the original string\n",
    "\n",
    "        Can be called as:\n",
    "\n",
    "        - ``self.word_to_chars(word_index)`` if batch size is 1\n",
    "        - ``self.word_to_chars(batch_index, word_index)`` if batch size is greater or equal to 1\n",
    "\n",
    "        Args:\n",
    "            batch_or_word_index (:obj:`int`):\n",
    "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
    "                the word in the sequence\n",
    "            word_index (:obj:`int`, `optional`):\n",
    "                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the\n",
    "                sequence.\n",
    "            sequence_index (:obj:`int`, `optional`, defaults to 0):\n",
    "                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0\n",
    "                or 1) the provided word index belongs to.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`CharSpan` or :obj:`List[CharSpan]`: Span(s) of the associated character or characters in the string.\n",
    "            CharSpan are NamedTuple with:\n",
    "\n",
    "                - start: index of the first character associated to the token in the original string\n",
    "                - end: index of the character following the last character associated to the token in the original\n",
    "                  string\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"word_to_chars() is not available when using Python based tokenizers\")\n",
    "        if word_index is not None:\n",
    "            batch_index = batch_or_word_index\n",
    "        else:\n",
    "            batch_index = 0\n",
    "            word_index = batch_or_word_index\n",
    "        return CharSpan(*(self._encodings[batch_index].word_to_chars(word_index, sequence_index)))\n",
    "\n",
    "    def char_to_word(self, batch_or_char_index: int, char_index: Optional[int] = None, sequence_index: int = 0) -> int:\n",
    "        \"\"\"\n",
    "        Get the word in the original string corresponding to a character in the original string of a sequence of the\n",
    "        batch.\n",
    "\n",
    "        Can be called as:\n",
    "\n",
    "        - ``self.char_to_word(char_index)`` if batch size is 1\n",
    "        - ``self.char_to_word(batch_index, char_index)`` if batch size is greater than 1\n",
    "\n",
    "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words\n",
    "        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized\n",
    "        words.\n",
    "\n",
    "        Args:\n",
    "            batch_or_char_index (:obj:`int`):\n",
    "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
    "                the character in the original string.\n",
    "            char_index (:obj:`int`, `optional`):\n",
    "                If a batch index is provided in `batch_or_token_index`, this can be the index of the character in the\n",
    "                original string.\n",
    "            sequence_index (:obj:`int`, `optional`, defaults to 0):\n",
    "                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0\n",
    "                or 1) the provided character index belongs to.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int` or :obj:`List[int]`: Index or indices of the associated encoded token(s).\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._encodings:\n",
    "            raise ValueError(\"char_to_word() is not available when using Python based tokenizers\")\n",
    "        if char_index is not None:\n",
    "            batch_index = batch_or_char_index\n",
    "        else:\n",
    "            batch_index = 0\n",
    "            char_index = batch_or_char_index\n",
    "        return self._encodings[batch_index].char_to_word(char_index, sequence_index)\n",
    "\n",
    "    def convert_to_tensors(\n",
    "        self, tensor_type: Optional[Union[str, TensorType]] = None, prepend_batch_axis: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Convert the inner content to tensors.\n",
    "\n",
    "        Args:\n",
    "            tensor_type (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
    "                The type of tensors to use. If :obj:`str`, should be one of the values of the enum\n",
    "                :class:`~transformers.tokenization_utils_base.TensorType`. If :obj:`None`, no modification is done.\n",
    "            prepend_batch_axis (:obj:`int`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to add the batch dimension during the conversion.\n",
    "        \"\"\"\n",
    "        if tensor_type is None:\n",
    "            return self\n",
    "\n",
    "        # Convert to TensorType\n",
    "        if not isinstance(tensor_type, TensorType):\n",
    "            tensor_type = TensorType(tensor_type)\n",
    "\n",
    "        # Get a function reference for the correct framework\n",
    "        if tensor_type == TensorType.TENSORFLOW:\n",
    "            if not is_tf_available():\n",
    "                raise ImportError(\n",
    "                    \"Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.\"\n",
    "                )\n",
    "            import tensorflow as tf\n",
    "\n",
    "            as_tensor = tf.constant\n",
    "            is_tensor = tf.is_tensor\n",
    "        elif tensor_type == TensorType.PYTORCH:\n",
    "            if not is_torch_available():\n",
    "                raise ImportError(\"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\")\n",
    "            import torch\n",
    "\n",
    "            as_tensor = torch.tensor\n",
    "            is_tensor = torch.is_tensor\n",
    "        elif tensor_type == TensorType.JAX:\n",
    "            if not is_flax_available():\n",
    "                raise ImportError(\"Unable to convert output to JAX tensors format, JAX is not installed.\")\n",
    "            import jax.numpy as jnp  # noqa: F811\n",
    "\n",
    "            as_tensor = jnp.array\n",
    "            is_tensor = _is_jax\n",
    "        else:\n",
    "            as_tensor = np.asarray\n",
    "            is_tensor = _is_numpy\n",
    "        # (mfuntowicz: This code is unreachable)\n",
    "        # else:\n",
    "        #     raise ImportError(\n",
    "        #         \"Unable to convert output to tensors format {}\".format(tensor_type)\n",
    "        #     )\n",
    "\n",
    "        # Do the tensor conversion in batch\n",
    "        for key, value in self.items():\n",
    "            try:\n",
    "                if prepend_batch_axis:\n",
    "                    value = [value]\n",
    "\n",
    "                if not is_tensor(value):\n",
    "                    tensor = as_tensor(value)\n",
    "\n",
    "                    # Removing this for now in favor of controlling the shape with `prepend_batch_axis`\n",
    "                    # # at-least2d\n",
    "                    # if tensor.ndim > 2:\n",
    "                    #     tensor = tensor.squeeze(0)\n",
    "                    # elif tensor.ndim < 2:\n",
    "                    #     tensor = tensor[None, :]\n",
    "\n",
    "                    self[key] = tensor\n",
    "            except:  # noqa E722\n",
    "                if key == \"overflowing_tokens\":\n",
    "                    raise ValueError(\n",
    "                        \"Unable to create tensor returning overflowing tokens of different lengths. \"\n",
    "                        \"Please see if a fast version of this tokenizer is available to have this feature available.\"\n",
    "                    )\n",
    "                raise ValueError(\n",
    "                    \"Unable to create tensor, you should probably activate truncation and/or padding \"\n",
    "                    \"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\n",
    "                )\n",
    "\n",
    "        return self\n",
    "\n",
    "    @torch_required\n",
    "    def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":\n",
    "        \"\"\"\n",
    "        Send all values to device by calling :obj:`v.to(device)` (PyTorch only).\n",
    "\n",
    "        Args:\n",
    "            device (:obj:`str` or :obj:`torch.device`): The device to put the tensors on.\n",
    "\n",
    "        Returns:\n",
    "            :class:`~transformers.BatchEncoding`: The same instance of :class:`~transformers.BatchEncoding` after\n",
    "            modification.\n",
    "        \"\"\"\n",
    "\n",
    "        # This check catches things like APEX blindly calling \"to\" on all inputs to a module\n",
    "        # Otherwise it passes the casts down and casts the LongTensor containing the token idxs\n",
    "        # into a HalfTensor\n",
    "        if isinstance(device, str) or _is_torch_device(device) or isinstance(device, int):\n",
    "            self.data = {k: v.to(device=device) for k, v in self.data.items()}\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Attempting to cast a BatchEncoding to another type, {str(device)}. This is not supported.\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "\n",
    "class SpecialTokensMixin:\n",
    "    \"\"\"\n",
    "    A mixin derived by :class:`~transformers.PreTrainedTokenizer` and :class:`~transformers.PreTrainedTokenizerFast` to\n",
    "    handle specific behaviors related to special tokens. In particular, this class hold the attributes which can be\n",
    "    used to directly access these special tokens in a model-independent manner and allow to set and update the special\n",
    "    tokens.\n",
    "\n",
    "    Args:\n",
    "        bos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing the beginning of a sentence.\n",
    "        eos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing the end of a sentence.\n",
    "        unk_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing an out-of-vocabulary token.\n",
    "        sep_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token separating two different sentences in the same input (used by BERT for instance).\n",
    "        pad_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n",
    "            attention mechanisms or loss computation.\n",
    "        cls_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing the class of the input (used by BERT for instance).\n",
    "        mask_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing a masked token (used by masked-language modeling pretraining objectives, like\n",
    "            BERT).\n",
    "        additional_special_tokens (tuple or list of :obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A tuple or a list of additional special tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    SPECIAL_TOKENS_ATTRIBUTES = [\n",
    "        \"bos_token\",\n",
    "        \"eos_token\",\n",
    "        \"unk_token\",\n",
    "        \"sep_token\",\n",
    "        \"pad_token\",\n",
    "        \"cls_token\",\n",
    "        \"mask_token\",\n",
    "        \"additional_special_tokens\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, verbose=True, **kwargs):\n",
    "        self._bos_token = None\n",
    "        self._eos_token = None\n",
    "        self._unk_token = None\n",
    "        self._sep_token = None\n",
    "        self._pad_token = None\n",
    "        self._cls_token = None\n",
    "        self._mask_token = None\n",
    "        self._pad_token_type_id = 0\n",
    "        self._additional_special_tokens = []\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # We directly set the hidden value to allow initialization with special tokens\n",
    "        # which are not yet in the vocabulary. Necessary for serialization/de-serialization\n",
    "        # TODO clean this up at some point (probably by switching to fast tokenizers)\n",
    "        for key, value in kwargs.items():\n",
    "            if value is None:\n",
    "                continue\n",
    "            if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
    "                if key == \"additional_special_tokens\":\n",
    "                    assert isinstance(value, (list, tuple)), f\"Value {value} is not a list or tuple\"\n",
    "                    assert all(isinstance(t, str) for t in value), \"One of the tokens is not a string\"\n",
    "                    setattr(self, key, value)\n",
    "                elif isinstance(value, (str, AddedToken)):\n",
    "                    setattr(self, key, value)\n",
    "                else:\n",
    "                    raise TypeError(\n",
    "                        \"special token {} has to be either str or AddedToken but got: {}\".format(key, type(value))\n",
    "                    )\n",
    "\n",
    "    def sanitize_special_tokens(self) -> int:\n",
    "        \"\"\"\n",
    "        Make sure that all the special tokens attributes of the tokenizer (:obj:`tokenizer.mask_token`,\n",
    "        :obj:`tokenizer.cls_token`, etc.) are in the vocabulary.\n",
    "\n",
    "        Add the missing ones to the vocabulary if needed.\n",
    "\n",
    "        Return:\n",
    "            :obj:`int`: The number of tokens added in the vocabulary during the operation.\n",
    "        \"\"\"\n",
    "        return self.add_tokens(self.all_special_tokens_extended, special_tokens=True)\n",
    "\n",
    "    def add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, AddedToken]]) -> int:\n",
    "        \"\"\"\n",
    "        Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
    "        special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
    "        current vocabulary).\n",
    "\n",
    "        Using : obj:`add_special_tokens` will ensure your special tokens can be used in several ways:\n",
    "\n",
    "        - Special tokens are carefully handled by the tokenizer (they are never split).\n",
    "        - You can easily refer to special tokens using tokenizer class attributes like :obj:`tokenizer.cls_token`. This\n",
    "          makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
    "\n",
    "        When possible, special tokens are already registered for provided pretrained models (for instance\n",
    "        :class:`~transformers.BertTokenizer` :obj:`cls_token` is already registered to be :obj`'[CLS]'` and XLM's one\n",
    "        is also registered to be :obj:`'</s>'`).\n",
    "\n",
    "        Args:\n",
    "            special_tokens_dict (dictionary `str` to `str` or :obj:`tokenizers.AddedToken`):\n",
    "                Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``,\n",
    "                ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
    "                ``additional_special_tokens``].\n",
    "\n",
    "                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
    "                assign the index of the ``unk_token`` to them).\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: Number of tokens added to the vocabulary.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # Let's see how to add a new classification token to GPT-2\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "            special_tokens_dict = {'cls_token': '<CLS>'}\n",
    "\n",
    "            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "            print('We have added', num_added_toks, 'tokens')\n",
    "            # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "            assert tokenizer.cls_token == '<CLS>'\n",
    "        \"\"\"\n",
    "        if not special_tokens_dict:\n",
    "            return 0\n",
    "\n",
    "        added_tokens = 0\n",
    "        for key, value in special_tokens_dict.items():\n",
    "            assert key in self.SPECIAL_TOKENS_ATTRIBUTES, f\"Key {key} is not a special token\"\n",
    "\n",
    "            if self.verbose:\n",
    "                logger.info(\"Assigning %s to the %s key of the tokenizer\", value, key)\n",
    "            setattr(self, key, value)\n",
    "\n",
    "            if key == \"additional_special_tokens\":\n",
    "                assert isinstance(value, (list, tuple)) and all(\n",
    "                    isinstance(t, (str, AddedToken)) for t in value\n",
    "                ), f\"Tokens {value} for key {key} should all be str or AddedToken instances\"\n",
    "                added_tokens += self.add_tokens(value, special_tokens=True)\n",
    "            else:\n",
    "                assert isinstance(\n",
    "                    value, (str, AddedToken)\n",
    "                ), f\"Token {value} for key {key} should be a str or an AddedToken instance\"\n",
    "                added_tokens += self.add_tokens([value], special_tokens=True)\n",
    "\n",
    "        return added_tokens\n",
    "\n",
    "    def add_tokens(\n",
    "        self, new_tokens: Union[str, AddedToken, List[Union[str, AddedToken]]], special_tokens: bool = False\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
    "        it with indices starting from length of the current vocabulary.\n",
    "\n",
    "        Args:\n",
    "            new_tokens (:obj:`str`, :obj:`tokenizers.AddedToken` or a list of `str` or :obj:`tokenizers.AddedToken`):\n",
    "                Tokens are only added if they are not already in the vocabulary. :obj:`tokenizers.AddedToken` wraps a\n",
    "                string token to let you personalize its behavior: whether this token should only match against a single\n",
    "                word, whether this token should strip all potential whitespaces on the left side, whether this token\n",
    "                should strip all potential whitespaces on the right side, etc.\n",
    "            special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
    "                (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
    "\n",
    "                See details for :obj:`tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: Number of tokens added to the vocabulary.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "            model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
    "            print('We have added', num_added_toks, 'tokens')\n",
    "             # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        \"\"\"\n",
    "        if not new_tokens:\n",
    "            return 0\n",
    "\n",
    "        if not isinstance(new_tokens, (list, tuple)):\n",
    "            new_tokens = [new_tokens]\n",
    "\n",
    "        return self._add_tokens(new_tokens, special_tokens=special_tokens)\n",
    "\n",
    "    def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def bos_token(self) -> str:\n",
    "        \"\"\"\n",
    "        :obj:`str`: Beginning of sentence token. Log an error if used while not having been set.\n",
    "        \"\"\"\n",
    "        if self._bos_token is None and self.verbose:\n",
    "            logger.error(\"Using bos_token, but it is not set yet.\")\n",
    "            return None\n",
    "        return str(self._bos_token)\n",
    "\n",
    "    @property\n",
    "    def eos_token(self) -> str:\n",
    "        \"\"\"\n",
    "        :obj:`str`: End of sentence token. Log an error if used while not having been set.\n",
    "        \"\"\"\n",
    "        if self._eos_token is None and self.verbose:\n",
    "            logger.error(\"Using eos_token, but it is not set yet.\")\n",
    "            return None\n",
    "        return str(self._eos_token)\n",
    "\n",
    "    @property\n",
    "    def unk_token(self) -> str:\n",
    "        \"\"\"\n",
    "        :obj:`str`: Unknown token. Log an error if used while not having been set.\n",
    "        \"\"\"\n",
    "        if self._unk_token is None and self.verbose:\n",
    "            logger.error(\"Using unk_token, but it is not set yet.\")\n",
    "            return None\n",
    "        return str(self._unk_token)\n",
    "\n",
    "    @property\n",
    "    def sep_token(self) -> str:\n",
    "        \"\"\"\n",
    "        :obj:`str`: Separation token, to separate context and query in an input sequence. Log an error if used while\n",
    "        not having been set.\n",
    "        \"\"\"\n",
    "        if self._sep_token is None and self.verbose:\n",
    "            logger.error(\"Using sep_token, but it is not set yet.\")\n",
    "            return None\n",
    "        return str(self._sep_token)\n",
    "\n",
    "    @property\n",
    "    def pad_token(self) -> str:\n",
    "        \"\"\"\n",
    "        :obj:`str`: Padding token. Log an error if used while not having been set.\n",
    "        \"\"\"\n",
    "        if self._pad_token is None and self.verbose:\n",
    "            logger.error(\"Using pad_token, but it is not set yet.\")\n",
    "            return None\n",
    "        return str(self._pad_token)\n",
    "\n",
    "    @property\n",
    "    def cls_token(self) -> str:\n",
    "        \"\"\"\n",
    "        :obj:`str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the\n",
    "        full depth of the model. Log an error if used while not having been set.\n",
    "        \"\"\"\n",
    "        if self._cls_token is None and self.verbose:\n",
    "            logger.error(\"Using cls_token, but it is not set yet.\")\n",
    "            return None\n",
    "        return str(self._cls_token)\n",
    "\n",
    "    @property\n",
    "    def mask_token(self) -> str:\n",
    "        \"\"\"\n",
    "        :obj:`str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while\n",
    "        not having been set.\n",
    "        \"\"\"\n",
    "        if self._mask_token is None and self.verbose:\n",
    "            logger.error(\"Using mask_token, but it is not set yet.\")\n",
    "            return None\n",
    "        return str(self._mask_token)\n",
    "\n",
    "    @property\n",
    "    def additional_special_tokens(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        :obj:`List[str]`: All the additional special tokens you may want to use. Log an error if used while not having\n",
    "        been set.\n",
    "        \"\"\"\n",
    "        if self._additional_special_tokens is None and self.verbose:\n",
    "            logger.error(\"Using additional_special_tokens, but it is not set yet.\")\n",
    "            return None\n",
    "        return [str(tok) for tok in self._additional_special_tokens]\n",
    "\n",
    "    @bos_token.setter\n",
    "    def bos_token(self, value):\n",
    "        self._bos_token = value\n",
    "\n",
    "    @eos_token.setter\n",
    "    def eos_token(self, value):\n",
    "        self._eos_token = value\n",
    "\n",
    "    @unk_token.setter\n",
    "    def unk_token(self, value):\n",
    "        self._unk_token = value\n",
    "\n",
    "    @sep_token.setter\n",
    "    def sep_token(self, value):\n",
    "        self._sep_token = value\n",
    "\n",
    "    @pad_token.setter\n",
    "    def pad_token(self, value):\n",
    "        self._pad_token = value\n",
    "\n",
    "    @cls_token.setter\n",
    "    def cls_token(self, value):\n",
    "        self._cls_token = value\n",
    "\n",
    "    @mask_token.setter\n",
    "    def mask_token(self, value):\n",
    "        self._mask_token = value\n",
    "\n",
    "    @additional_special_tokens.setter\n",
    "    def additional_special_tokens(self, value):\n",
    "        self._additional_special_tokens = value\n",
    "\n",
    "    @property\n",
    "    def bos_token_id(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns :obj:`None` if the token\n",
    "        has not been set.\n",
    "        \"\"\"\n",
    "        if self._bos_token is None:\n",
    "            return None\n",
    "        return self.convert_tokens_to_ids(self.bos_token)\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[int]`: Id of the end of sentence token in the vocabulary. Returns :obj:`None` if the token has\n",
    "        not been set.\n",
    "        \"\"\"\n",
    "        if self._eos_token is None:\n",
    "            return None\n",
    "        return self.convert_tokens_to_ids(self.eos_token)\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[int]`: Id of the unknown token in the vocabulary. Returns :obj:`None` if the token has not been\n",
    "        set.\n",
    "        \"\"\"\n",
    "        if self._unk_token is None:\n",
    "            return None\n",
    "        return self.convert_tokens_to_ids(self.unk_token)\n",
    "\n",
    "    @property\n",
    "    def sep_token_id(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
    "        sequence. Returns :obj:`None` if the token has not been set.\n",
    "        \"\"\"\n",
    "        if self._sep_token is None:\n",
    "            return None\n",
    "        return self.convert_tokens_to_ids(self.sep_token)\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[int]`: Id of the padding token in the vocabulary. Returns :obj:`None` if the token has not been\n",
    "        set.\n",
    "        \"\"\"\n",
    "        if self._pad_token is None:\n",
    "            return None\n",
    "        return self.convert_tokens_to_ids(self.pad_token)\n",
    "\n",
    "    @property\n",
    "    def pad_token_type_id(self) -> int:\n",
    "        \"\"\"\n",
    "        :obj:`int`: Id of the padding token type in the vocabulary.\n",
    "        \"\"\"\n",
    "        return self._pad_token_type_id\n",
    "\n",
    "    @property\n",
    "    def cls_token_id(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input\n",
    "        sequence leveraging self-attention along the full depth of the model.\n",
    "\n",
    "        Returns :obj:`None` if the token has not been set.\n",
    "        \"\"\"\n",
    "        if self._cls_token is None:\n",
    "            return None\n",
    "        return self.convert_tokens_to_ids(self.cls_token)\n",
    "\n",
    "    @property\n",
    "    def mask_token_id(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        :obj:`Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
    "        modeling. Returns :obj:`None` if the token has not been set.\n",
    "        \"\"\"\n",
    "        if self._mask_token is None:\n",
    "            return None\n",
    "        return self.convert_tokens_to_ids(self.mask_token)\n",
    "\n",
    "    @property\n",
    "    def additional_special_tokens_ids(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        :obj:`List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not\n",
    "        having been set.\n",
    "        \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.additional_special_tokens)\n",
    "\n",
    "    @bos_token_id.setter\n",
    "    def bos_token_id(self, value):\n",
    "        self._bos_token = self.convert_tokens_to_ids(value)\n",
    "\n",
    "    @eos_token_id.setter\n",
    "    def eos_token_id(self, value):\n",
    "        self._eos_token = self.convert_tokens_to_ids(value)\n",
    "\n",
    "    @unk_token_id.setter\n",
    "    def unk_token_id(self, value):\n",
    "        self._unk_token = self.convert_tokens_to_ids(value)\n",
    "\n",
    "    @sep_token_id.setter\n",
    "    def sep_token_id(self, value):\n",
    "        self._sep_token = self.convert_tokens_to_ids(value)\n",
    "\n",
    "    @pad_token_id.setter\n",
    "    def pad_token_id(self, value):\n",
    "        self._pad_token = self.convert_tokens_to_ids(value)\n",
    "\n",
    "    @cls_token_id.setter\n",
    "    def cls_token_id(self, value):\n",
    "        self._cls_token = self.convert_tokens_to_ids(value)\n",
    "\n",
    "    @mask_token_id.setter\n",
    "    def mask_token_id(self, value):\n",
    "        self._mask_token = self.convert_tokens_to_ids(value)\n",
    "\n",
    "    @additional_special_tokens_ids.setter\n",
    "    def additional_special_tokens_ids(self, values):\n",
    "        self._additional_special_tokens = [self.convert_tokens_to_ids(value) for value in values]\n",
    "\n",
    "    @property\n",
    "    def special_tokens_map(self) -> Dict[str, Union[str, List[str]]]:\n",
    "        \"\"\"\n",
    "        :obj:`Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (:obj:`cls_token`,\n",
    "        :obj:`unk_token`, etc.) to their values (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
    "\n",
    "        Convert potential tokens of :obj:`tokenizers.AddedToken` type to string.\n",
    "        \"\"\"\n",
    "        set_attr = {}\n",
    "        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
    "            attr_value = getattr(self, \"_\" + attr)\n",
    "            if attr_value:\n",
    "                set_attr[attr] = str(attr_value)\n",
    "        return set_attr\n",
    "\n",
    "    @property\n",
    "    def special_tokens_map_extended(self) -> Dict[str, Union[str, AddedToken, List[Union[str, AddedToken]]]]:\n",
    "        \"\"\"\n",
    "        :obj:`Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary\n",
    "        mapping special token class attributes (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values\n",
    "        (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
    "\n",
    "        Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
    "        how special tokens are tokenized.\n",
    "        \"\"\"\n",
    "        set_attr = {}\n",
    "        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
    "            attr_value = getattr(self, \"_\" + attr)\n",
    "            if attr_value:\n",
    "                set_attr[attr] = attr_value\n",
    "        return set_attr\n",
    "\n",
    "    @property\n",
    "    def all_special_tokens(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        :obj:`List[str]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class attributes.\n",
    "\n",
    "        Convert tokens of :obj:`tokenizers.AddedToken` type to string.\n",
    "        \"\"\"\n",
    "        all_toks = [str(s) for s in self.all_special_tokens_extended]\n",
    "        return all_toks\n",
    "\n",
    "    @property\n",
    "    def all_special_tokens_extended(self) -> List[Union[str, AddedToken]]:\n",
    "        \"\"\"\n",
    "        :obj:`List[Union[str, tokenizers.AddedToken]]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.)\n",
    "        mapped to class attributes.\n",
    "\n",
    "        Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
    "        how special tokens are tokenized.\n",
    "        \"\"\"\n",
    "        all_toks = []\n",
    "        set_attr = self.special_tokens_map_extended\n",
    "        for attr_value in set_attr.values():\n",
    "            all_toks = all_toks + (list(attr_value) if isinstance(attr_value, (list, tuple)) else [attr_value])\n",
    "        all_toks = list(OrderedDict.fromkeys(all_toks))\n",
    "        return all_toks\n",
    "\n",
    "    @property\n",
    "    def all_special_ids(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        :obj:`List[int]`: List the ids of the special tokens(:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class\n",
    "        attributes.\n",
    "        \"\"\"\n",
    "        all_toks = self.all_special_tokens\n",
    "        all_ids = self.convert_tokens_to_ids(all_toks)\n",
    "        return all_ids\n",
    "\n",
    "\n",
    "ENCODE_KWARGS_DOCSTRING = r\"\"\"\n",
    "            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to encode the sequences with the special tokens relative to their model.\n",
    "            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
    "                Activates and controls padding. Accepts the following values:\n",
    "\n",
    "                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
    "                  single sequence if provided).\n",
    "                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "                  maximum acceptable input length for the model if that argument is not provided.\n",
    "                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "                  different lengths).\n",
    "            truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
    "                Activates and controls truncation. Accepts the following values:\n",
    "\n",
    "                * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
    "                  :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
    "                  provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
    "                  if a pair of sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
    "                  the maximum acceptable input length for the model if that argument is not provided. This will only\n",
    "                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
    "                  to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
    "                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
    "                  sequence lengths greater than the model maximum admissible input size).\n",
    "            max_length (:obj:`int`, `optional`):\n",
    "                Controls the maximum length to use by one of the truncation/padding parameters.\n",
    "\n",
    "                If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
    "                length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
    "                input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
    "            stride (:obj:`int`, `optional`, defaults to 0):\n",
    "                If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
    "                :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
    "                returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
    "                argument defines the number of overlapping tokens.\n",
    "            is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
    "                will skip the pre-tokenization step. This is useful for NER or token classification.\n",
    "            pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "                If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
    "                the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "            return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
    "                If set, will return tensors instead of list of python integers. Acceptable values are:\n",
    "\n",
    "                * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
    "                * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
    "                * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
    "\"\"\"\n",
    "\n",
    "ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING = r\"\"\"\n",
    "            return_token_type_ids (:obj:`bool`, `optional`):\n",
    "                Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
    "                the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
    "\n",
    "                `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
    "            return_attention_mask (:obj:`bool`, `optional`):\n",
    "                Whether to return the attention mask. If left to the default, will return the attention mask according\n",
    "                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
    "\n",
    "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "            return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return overflowing token sequences.\n",
    "            return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return special tokens mask information.\n",
    "            return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
    "\n",
    "                This is only available on fast tokenizers inheriting from\n",
    "                :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
    "                :obj:`NotImplementedError`.\n",
    "            return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return the lengths of the encoded inputs.\n",
    "            verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to print more information and warnings.\n",
    "            **kwargs: passed to the :obj:`self.tokenize()` method\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
    "\n",
    "            - **input_ids** -- List of token ids to be fed to a model.\n",
    "\n",
    "              `What are input IDs? <../glossary.html#input-ids>`__\n",
    "\n",
    "            - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
    "              or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
    "\n",
    "              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
    "\n",
    "            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
    "              :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
    "\n",
    "              `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "\n",
    "            - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
    "              :obj:`return_overflowing_tokens=True`).\n",
    "            - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
    "              :obj:`return_overflowing_tokens=True`).\n",
    "            - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
    "              regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
    "            - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
    "\"\"\"\n",
    "\n",
    "INIT_TOKENIZER_DOCSTRING = r\"\"\"\n",
    "    Class attributes (overridden by derived classes)\n",
    "\n",
    "        - **vocab_files_names** (:obj:`Dict[str, str]`) -- A dictionary with, as keys, the ``__init__`` keyword name of\n",
    "          each vocabulary file required by the model, and as associated values, the filename for saving the associated\n",
    "          file (string).\n",
    "        - **pretrained_vocab_files_map** (:obj:`Dict[str, Dict[str, str]]`) -- A dictionary of dictionaries, with the\n",
    "          high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the\n",
    "          low-level being the :obj:`short-cut-names` of the pretrained models with, as associated values, the\n",
    "          :obj:`url` to the associated pretrained vocabulary file.\n",
    "        - **max_model_input_sizes** (:obj:`Dict[str, Optinal[int]]`) -- A dictionary with, as keys, the\n",
    "          :obj:`short-cut-names` of the pretrained models, and as associated values, the maximum length of the sequence\n",
    "          inputs of this model, or :obj:`None` if the model has no maximum input size.\n",
    "        - **pretrained_init_configuration** (:obj:`Dict[str, Dict[str, Any]]`) -- A dictionary with, as keys, the\n",
    "          :obj:`short-cut-names` of the pretrained models, and as associated values, a dictionary of specific arguments\n",
    "          to pass to the ``__init__`` method of the tokenizer class for this pretrained model when loading the\n",
    "          tokenizer with the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`\n",
    "          method.\n",
    "        - **model_input_names** (:obj:`List[str]`) -- A list of inputs expected in the forward pass of the model.\n",
    "        - **padding_side** (:obj:`str`) -- The default value for the side on which the model should have padding\n",
    "          applied. Should be :obj:`'right'` or :obj:`'left'`.\n",
    "\n",
    "    Args:\n",
    "        model_max_length (:obj:`int`, `optional`):\n",
    "            The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is\n",
    "            loaded with :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`, this\n",
    "            will be set to the value stored for the associated model in ``max_model_input_sizes`` (see above). If no\n",
    "            value is provided, will default to VERY_LARGE_INTEGER (:obj:`int(1e30)`).\n",
    "        padding_side: (:obj:`str`, `optional`):\n",
    "            The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
    "            Default value is picked from the class attribute of the same name.\n",
    "        model_input_names (:obj:`List[string]`, `optional`):\n",
    "            The list of inputs accepted by the forward pass of the model (like :obj:`\"token_type_ids\"` or\n",
    "            :obj:`\"attention_mask\"`). Default value is picked from the class attribute of the same name.\n",
    "        bos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing the beginning of a sentence. Will be associated to ``self.bos_token`` and\n",
    "            ``self.bos_token_id``.\n",
    "        eos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing the end of a sentence. Will be associated to ``self.eos_token`` and\n",
    "            ``self.eos_token_id``.\n",
    "        unk_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing an out-of-vocabulary token. Will be associated to ``self.unk_token`` and\n",
    "            ``self.unk_token_id``.\n",
    "        sep_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token separating two different sentences in the same input (used by BERT for instance). Will be\n",
    "            associated to ``self.sep_token`` and ``self.sep_token_id``.\n",
    "        pad_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n",
    "            attention mechanisms or loss computation. Will be associated to ``self.pad_token`` and\n",
    "            ``self.pad_token_id``.\n",
    "        cls_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing the class of the input (used by BERT for instance). Will be associated to\n",
    "            ``self.cls_token`` and ``self.cls_token_id``.\n",
    "        mask_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A special token representing a masked token (used by masked-language modeling pretraining objectives, like\n",
    "            BERT). Will be associated to ``self.mask_token`` and ``self.mask_token_id``.\n",
    "        additional_special_tokens (tuple or list of :obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
    "            A tuple or a list of additional special tokens. Add them here to ensure they won't be split by the\n",
    "            tokenization process. Will be associated to ``self.additional_special_tokens`` and\n",
    "            ``self.additional_special_tokens_ids``.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)\n",
    "class PreTrainedTokenizerBase(SpecialTokensMixin):\n",
    "    \"\"\"\n",
    "    Base class for :class:`~transformers.PreTrainedTokenizer` and :class:`~transformers.PreTrainedTokenizerFast`.\n",
    "\n",
    "    Handles shared (mostly boiler plate) methods for those two classes.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names: Dict[str, str] = {}\n",
    "    pretrained_vocab_files_map: Dict[str, Dict[str, str]] = {}\n",
    "    pretrained_init_configuration: Dict[str, Dict[str, Any]] = {}\n",
    "    max_model_input_sizes: Dict[str, Optional[int]] = {}\n",
    "\n",
    "    # first name has to correspond to main model input name\n",
    "    # to make sure `tokenizer.pad(...)` works correctly\n",
    "    model_input_names: List[str] = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "    padding_side: str = \"right\"\n",
    "    slow_tokenizer_class = None\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)\n",
    "        self.init_inputs = ()\n",
    "        self.init_kwargs = copy.deepcopy(kwargs)\n",
    "        self.name_or_path = kwargs.pop(\"name_or_path\", \"\")\n",
    "\n",
    "        # For backward compatibility we fallback to set model_max_length from max_len if provided\n",
    "        model_max_length = kwargs.pop(\"model_max_length\", kwargs.pop(\"max_len\", None))\n",
    "        self.model_max_length = model_max_length if model_max_length is not None else VERY_LARGE_INTEGER\n",
    "\n",
    "        # Padding side is right by default and overridden in subclasses. If specified in the kwargs, it is changed.\n",
    "        self.padding_side = kwargs.pop(\"padding_side\", self.padding_side)\n",
    "        assert self.padding_side in [\n",
    "            \"right\",\n",
    "            \"left\",\n",
    "        ], f\"Padding side should be selected between 'right' and 'left', current value: {self.padding_side}\"\n",
    "        self.model_input_names = kwargs.pop(\"model_input_names\", self.model_input_names)\n",
    "\n",
    "        self.deprecation_warnings = (\n",
    "            {}\n",
    "        )  # Use to store when we have already noticed a deprecation warning (avoid overlogging).\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def max_len_single_sentence(self) -> int:\n",
    "        \"\"\"\n",
    "        :obj:`int`: The maximum length of a sentence that can be fed to the model.\n",
    "        \"\"\"\n",
    "        return self.model_max_length - self.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "    @property\n",
    "    def max_len_sentences_pair(self) -> int:\n",
    "        \"\"\"\n",
    "        :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
    "        \"\"\"\n",
    "        return self.model_max_length - self.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "    @max_len_single_sentence.setter\n",
    "    def max_len_single_sentence(self, value) -> int:\n",
    "        # For backward compatibility, allow to try to setup 'max_len_single_sentence'.\n",
    "        if value == self.model_max_length - self.num_special_tokens_to_add(pair=False) and self.verbose:\n",
    "            if not self.deprecation_warnings.get(\"max_len_single_sentence\", False):\n",
    "                logger.warning(\n",
    "                    \"Setting 'max_len_single_sentence' is now deprecated. \" \"This value is automatically set up.\"\n",
    "                )\n",
    "            self.deprecation_warnings[\"max_len_single_sentence\"] = True\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Setting 'max_len_single_sentence' is now deprecated. \" \"This value is automatically set up.\"\n",
    "            )\n",
    "\n",
    "    @max_len_sentences_pair.setter\n",
    "    def max_len_sentences_pair(self, value) -> int:\n",
    "        # For backward compatibility, allow to try to setup 'max_len_sentences_pair'.\n",
    "        if value == self.model_max_length - self.num_special_tokens_to_add(pair=True) and self.verbose:\n",
    "            if not self.deprecation_warnings.get(\"max_len_sentences_pair\", False):\n",
    "                logger.warning(\n",
    "                    \"Setting 'max_len_sentences_pair' is now deprecated. \" \"This value is automatically set up.\"\n",
    "                )\n",
    "            self.deprecation_warnings[\"max_len_sentences_pair\"] = True\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Setting 'max_len_sentences_pair' is now deprecated. \" \"This value is automatically set up.\"\n",
    "            )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{'PreTrainedTokenizerFast' if self.is_fast else 'PreTrainedTokenizer'}(name_or_path='{self.name_or_path}', \"\n",
    "            f\"vocab_size={self.vocab_size}, model_max_len={self.model_max_length}, is_fast={self.is_fast}, \"\n",
    "            f\"padding_side='{self.padding_side}', special_tokens={self.special_tokens_map_extended})\"\n",
    "        )\n",
    "\n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Returns the vocabulary as a dictionary of token to index.\n",
    "\n",
    "        :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when\n",
    "        :obj:`token` is in the vocab.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Dict[str, int]`: The vocabulary.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, **kwargs):\n",
    "        r\"\"\"\n",
    "        Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
    "        a predefined tokenizer.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Can be either:\n",
    "\n",
    "                - A string, the `model id` of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
    "                  Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under a\n",
    "                  user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
    "                  using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
    "                  method, e.g., ``./my_model_directory/``.\n",
    "                - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
    "                  file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
    "                  ``./my_model_directory/vocab.txt``.\n",
    "            cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
    "                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
    "                standard cache should not be used.\n",
    "            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
    "                exist.\n",
    "            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
    "                exists.\n",
    "            proxies (:obj:`Dict[str, str], `optional`):\n",
    "                A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
    "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
    "            use_auth_token (:obj:`str` or `bool`, `optional`):\n",
    "                The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
    "                generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
    "            revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
    "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
    "                git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
    "                identifier allowed by git.\n",
    "            subfolder (:obj:`str`, `optional`):\n",
    "                In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
    "                facebook/rag-token-base), specify it here.\n",
    "            inputs (additional positional arguments, `optional`):\n",
    "                Will be passed along to the Tokenizer ``__init__`` method.\n",
    "            kwargs (additional keyword arguments, `optional`):\n",
    "                Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
    "                ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
    "                ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
    "            # Download vocabulary from huggingface.co and cache.\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
    "            tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "\n",
    "            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
    "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
    "\n",
    "            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
    "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
    "\n",
    "            # You can link tokens to special vocabulary when instantiating\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
    "            # You should be sure '<unk>' is in the vocabulary when doing that.\n",
    "            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
    "            assert tokenizer.unk_token == '<unk>'\n",
    "\n",
    "        \"\"\"\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
    "        revision = kwargs.pop(\"revision\", None)\n",
    "        subfolder = kwargs.pop(\"subfolder\", None)\n",
    "\n",
    "        s3_models = list(cls.max_model_input_sizes.keys())\n",
    "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
    "        vocab_files = {}\n",
    "        init_configuration = {}\n",
    "        if pretrained_model_name_or_path in s3_models:\n",
    "            # Get the vocabulary from AWS S3 bucket\n",
    "            for file_id, map_list in cls.pretrained_vocab_files_map.items():\n",
    "                vocab_files[file_id] = map_list[pretrained_model_name_or_path]\n",
    "            if (\n",
    "                cls.pretrained_init_configuration\n",
    "                and pretrained_model_name_or_path in cls.pretrained_init_configuration\n",
    "            ):\n",
    "                init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path].copy()\n",
    "        else:\n",
    "            # Get the vocabulary from local files\n",
    "            logger.info(\n",
    "                \"Model name '{}' not found in model shortcut name list ({}). \"\n",
    "                \"Assuming '{}' is a path, a model identifier, or url to a directory containing tokenizer files.\".format(\n",
    "                    pretrained_model_name_or_path, \", \".join(s3_models), pretrained_model_name_or_path\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "                if len(cls.vocab_files_names) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"Calling {}.from_pretrained() with the path to a single file or url is not supported.\"\n",
    "                        \"Use a model identifier or the path to a directory instead.\".format(cls.__name__)\n",
    "                    )\n",
    "                logger.warning(\n",
    "                    \"Calling {}.from_pretrained() with the path to a single file or url is deprecated\".format(\n",
    "                        cls.__name__\n",
    "                    )\n",
    "                )\n",
    "                file_id = list(cls.vocab_files_names.keys())[0]\n",
    "                vocab_files[file_id] = pretrained_model_name_or_path\n",
    "            else:\n",
    "                # At this point pretrained_model_name_or_path is either a directory or a model identifier name\n",
    "                additional_files_names = {\n",
    "                    \"added_tokens_file\": ADDED_TOKENS_FILE,\n",
    "                    \"special_tokens_map_file\": SPECIAL_TOKENS_MAP_FILE,\n",
    "                    \"tokenizer_config_file\": TOKENIZER_CONFIG_FILE,\n",
    "                    \"tokenizer_file\": FULL_TOKENIZER_FILE,\n",
    "                }\n",
    "                # Look for the tokenizer files\n",
    "                for file_id, file_name in {**cls.vocab_files_names, **additional_files_names}.items():\n",
    "                    if os.path.isdir(pretrained_model_name_or_path):\n",
    "                        if subfolder is not None:\n",
    "                            full_file_name = os.path.join(pretrained_model_name_or_path, subfolder, file_name)\n",
    "                        else:\n",
    "                            full_file_name = os.path.join(pretrained_model_name_or_path, file_name)\n",
    "                        if not os.path.exists(full_file_name):\n",
    "                            logger.info(\"Didn't find file {}. We won't load it.\".format(full_file_name))\n",
    "                            full_file_name = None\n",
    "                    else:\n",
    "                        full_file_name = hf_bucket_url(\n",
    "                            pretrained_model_name_or_path,\n",
    "                            filename=file_name,\n",
    "                            subfolder=subfolder,\n",
    "                            revision=revision,\n",
    "                            mirror=None,\n",
    "                        )\n",
    "\n",
    "                    vocab_files[file_id] = full_file_name\n",
    "\n",
    "        # Get files from url, cache, or disk depending on the case\n",
    "        resolved_vocab_files = {}\n",
    "        unresolved_files = []\n",
    "        for file_id, file_path in vocab_files.items():\n",
    "            if file_path is None:\n",
    "                resolved_vocab_files[file_id] = None\n",
    "            else:\n",
    "                try:\n",
    "                    try:\n",
    "                        resolved_vocab_files[file_id] = cached_path(\n",
    "                            file_path,\n",
    "                            cache_dir=cache_dir,\n",
    "                            force_download=force_download,\n",
    "                            proxies=proxies,\n",
    "                            resume_download=resume_download,\n",
    "                            local_files_only=local_files_only,\n",
    "                            use_auth_token=use_auth_token,\n",
    "                        )\n",
    "                    except FileNotFoundError as error:\n",
    "                        if local_files_only:\n",
    "                            unresolved_files.append(file_id)\n",
    "                        else:\n",
    "                            raise error\n",
    "\n",
    "                except requests.exceptions.HTTPError as err:\n",
    "                    if \"404 Client Error\" in str(err):\n",
    "                        logger.debug(err)\n",
    "                        resolved_vocab_files[file_id] = None\n",
    "                    else:\n",
    "                        raise err\n",
    "\n",
    "        if len(unresolved_files) > 0:\n",
    "            logger.info(\n",
    "                f\"Can't load following files from cache: {unresolved_files} and cannot check if these \"\n",
    "                \"files are necessary for the tokenizer to operate.\"\n",
    "            )\n",
    "\n",
    "        if all(full_file_name is None for full_file_name in resolved_vocab_files.values()):\n",
    "            msg = (\n",
    "                f\"Can't load tokenizer for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
    "                f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\n",
    "            )\n",
    "            raise EnvironmentError(msg)\n",
    "\n",
    "        for file_id, file_path in vocab_files.items():\n",
    "            if file_id not in resolved_vocab_files:\n",
    "                continue\n",
    "\n",
    "            if file_path == resolved_vocab_files[file_id]:\n",
    "                logger.info(\"loading file {}\".format(file_path))\n",
    "            else:\n",
    "                logger.info(\"loading file {} from cache at {}\".format(file_path, resolved_vocab_files[file_id]))\n",
    "\n",
    "        return cls._from_pretrained(\n",
    "            resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _from_pretrained(\n",
    "        cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs\n",
    "    ):\n",
    "        # We instantiate fast tokenizers based on a slow tokenizer if we don't have access to the tokenizer.json\n",
    "        # file or if `from_slow` is set to True.\n",
    "        from_slow = kwargs.get(\"from_slow\", False)\n",
    "        has_tokenizer_file = resolved_vocab_files.get(\"tokenizer_file\", None) is not None\n",
    "        if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class is not None:\n",
    "            slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n",
    "                copy.deepcopy(resolved_vocab_files),\n",
    "                pretrained_model_name_or_path,\n",
    "                copy.deepcopy(init_configuration),\n",
    "                *init_inputs,\n",
    "                **(copy.deepcopy(kwargs)),\n",
    "            )\n",
    "        else:\n",
    "            slow_tokenizer = None\n",
    "\n",
    "        # Prepare tokenizer initialization kwargs\n",
    "        # Did we saved some inputs and kwargs to reload ?\n",
    "        tokenizer_config_file = resolved_vocab_files.pop(\"tokenizer_config_file\", None)\n",
    "        if tokenizer_config_file is not None:\n",
    "            with open(tokenizer_config_file, encoding=\"utf-8\") as tokenizer_config_handle:\n",
    "                init_kwargs = json.load(tokenizer_config_handle)\n",
    "            saved_init_inputs = init_kwargs.pop(\"init_inputs\", ())\n",
    "            if not init_inputs:\n",
    "                init_inputs = saved_init_inputs\n",
    "        else:\n",
    "            init_kwargs = init_configuration\n",
    "\n",
    "        # Update with newly provided kwargs\n",
    "        init_kwargs.update(kwargs)\n",
    "\n",
    "        # Convert AddedTokens serialized as dict to class instances\n",
    "        def convert_added_tokens(obj: Union[AddedToken, Any]):\n",
    "            if isinstance(obj, dict) and \"__type\" in obj and obj[\"__type\"] == \"AddedToken\":\n",
    "                obj.pop(\"__type\")\n",
    "                return AddedToken(**obj)\n",
    "            elif isinstance(obj, (list, tuple)):\n",
    "                return list(convert_added_tokens(o) for o in obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_added_tokens(v) for k, v in obj.items()}\n",
    "            return obj\n",
    "\n",
    "        init_kwargs = convert_added_tokens(init_kwargs)\n",
    "\n",
    "        # Set max length if needed\n",
    "        if pretrained_model_name_or_path in cls.max_model_input_sizes:\n",
    "            # if we're using a pretrained model, ensure the tokenizer\n",
    "            # wont index sequences longer than the number of positional embeddings\n",
    "            model_max_length = cls.max_model_input_sizes[pretrained_model_name_or_path]\n",
    "            if model_max_length is not None and isinstance(model_max_length, (int, float)):\n",
    "                init_kwargs[\"model_max_length\"] = min(init_kwargs.get(\"model_max_length\", int(1e30)), model_max_length)\n",
    "\n",
    "        # Merge resolved_vocab_files arguments in init_kwargs.\n",
    "        added_tokens_file = resolved_vocab_files.pop(\"added_tokens_file\", None)\n",
    "        for args_name, file_path in resolved_vocab_files.items():\n",
    "            if args_name not in init_kwargs:\n",
    "                init_kwargs[args_name] = file_path\n",
    "\n",
    "        if slow_tokenizer is not None:\n",
    "            init_kwargs[\"__slow_tokenizer\"] = slow_tokenizer\n",
    "\n",
    "        init_kwargs[\"name_or_path\"] = pretrained_model_name_or_path\n",
    "\n",
    "        # Instantiate tokenizer.\n",
    "        try:\n",
    "            tokenizer = cls(*init_inputs, **init_kwargs)\n",
    "        except OSError:\n",
    "            raise OSError(\n",
    "                \"Unable to load vocabulary from file. \"\n",
    "                \"Please check that the provided vocabulary is accessible and not corrupted.\"\n",
    "            )\n",
    "\n",
    "        # Save inputs and kwargs for saving and re-loading with ``save_pretrained``\n",
    "        # Removed: Now done at the base class level\n",
    "        # tokenizer.init_inputs = init_inputs\n",
    "        # tokenizer.init_kwargs = init_kwargs\n",
    "\n",
    "        # If there is a complementary special token map, load it\n",
    "        special_tokens_map_file = resolved_vocab_files.pop(\"special_tokens_map_file\", None)\n",
    "        if special_tokens_map_file is not None:\n",
    "            with open(special_tokens_map_file, encoding=\"utf-8\") as special_tokens_map_handle:\n",
    "                special_tokens_map = json.load(special_tokens_map_handle)\n",
    "            for key, value in special_tokens_map.items():\n",
    "                if isinstance(value, dict):\n",
    "                    value = AddedToken(**value)\n",
    "                elif isinstance(value, list):\n",
    "                    value = [AddedToken(**token) if isinstance(token, dict) else token for token in value]\n",
    "                setattr(tokenizer, key, value)\n",
    "\n",
    "        # Add supplementary tokens.\n",
    "        special_tokens = tokenizer.all_special_tokens\n",
    "        if added_tokens_file is not None:\n",
    "            with open(added_tokens_file, encoding=\"utf-8\") as added_tokens_handle:\n",
    "                added_tok_encoder = json.load(added_tokens_handle)\n",
    "\n",
    "            # Sort added tokens by index\n",
    "            added_tok_encoder_sorted = list(sorted(added_tok_encoder.items(), key=lambda x: x[1]))\n",
    "\n",
    "            for token, index in added_tok_encoder_sorted:\n",
    "                assert index == len(tokenizer), (\n",
    "                    f\"Non-consecutive added token '{token}' found. \"\n",
    "                    f\"Should have index {len(tokenizer)} but has index {index} in saved vocabulary.\"\n",
    "                )\n",
    "                tokenizer.add_tokens(token, special_tokens=bool(token in special_tokens))\n",
    "\n",
    "        # Check all our special tokens are registered as \"no split\" token (we don't cut them) and are in the vocab\n",
    "        added_tokens = tokenizer.sanitize_special_tokens()\n",
    "        if added_tokens:\n",
    "            logger.warning(\n",
    "                \"Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\"\n",
    "            )\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def save_pretrained(\n",
    "        self,\n",
    "        save_directory: Union[str, os.PathLike],\n",
    "        legacy_format: bool = True,\n",
    "        filename_prefix: Optional[str] = None,\n",
    "    ) -> Tuple[str]:\n",
    "        \"\"\"\n",
    "        Save the full tokenizer state.\n",
    "\n",
    "\n",
    "        This method make sure the full tokenizer can then be re-loaded using the\n",
    "        :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained` class method.\n",
    "\n",
    "        .. Note::\n",
    "            A \"fast\" tokenizer (instance of :class:`transformers.PreTrainedTokenizerFast`) saved with this method will\n",
    "            not be possible to load back in a \"slow\" tokenizer, i.e. in a :class:`transformers.PreTrainedTokenizer`\n",
    "            instance. It can only be loaded in a \"fast\" tokenizer, i.e. in a\n",
    "            :class:`transformers.PreTrainedTokenizerFast` instance.\n",
    "\n",
    "        .. Warning::\n",
    "           This won't save modifications you may have applied to the tokenizer after the instantiation (for instance,\n",
    "           modifying :obj:`tokenizer.do_lower_case` after creation).\n",
    "\n",
    "        Args:\n",
    "            save_directory (:obj:`str` or :obj:`os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
    "            legacy_format (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether to save the tokenizer in legacy format (default), i.e. with tokenizer specific vocabulary and a\n",
    "                separate added_tokens files or in the unified JSON file format for the `tokenizers` library. It's only\n",
    "                possible to save a Fast tokenizer in the unified JSON format and this format is incompatible with\n",
    "                \"slow\" tokenizers (not powered by the `tokenizers` library).\n",
    "            filename_prefix: (:obj:`str`, `optional`):\n",
    "                A prefix to add to the names of the files saved by the tokenizer.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of :obj:`str`: The files saved.\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_directory):\n",
    "            logger.error(\"Provided path ({}) should be a directory, not a file\".format(save_directory))\n",
    "            return\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        special_tokens_map_file = os.path.join(\n",
    "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + SPECIAL_TOKENS_MAP_FILE\n",
    "        )\n",
    "        tokenizer_config_file = os.path.join(\n",
    "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + TOKENIZER_CONFIG_FILE\n",
    "        )\n",
    "\n",
    "        tokenizer_config = copy.deepcopy(self.init_kwargs)\n",
    "        if len(self.init_inputs) > 0:\n",
    "            tokenizer_config[\"init_inputs\"] = copy.deepcopy(self.init_inputs)\n",
    "        for file_id in self.vocab_files_names.keys():\n",
    "            tokenizer_config.pop(file_id, None)\n",
    "\n",
    "        # Sanitize AddedTokens\n",
    "        def convert_added_tokens(obj: Union[AddedToken, Any], add_type_field=True):\n",
    "            if isinstance(obj, AddedToken):\n",
    "                out = obj.__getstate__()\n",
    "                if add_type_field:\n",
    "                    out[\"__type\"] = \"AddedToken\"\n",
    "                return out\n",
    "            elif isinstance(obj, (list, tuple)):\n",
    "                return list(convert_added_tokens(o, add_type_field=add_type_field) for o in obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_added_tokens(v, add_type_field=add_type_field) for k, v in obj.items()}\n",
    "            return obj\n",
    "\n",
    "        # add_type_field=True to allow dicts in the kwargs / differentiate from AddedToken serialization\n",
    "        tokenizer_config = convert_added_tokens(tokenizer_config, add_type_field=True)\n",
    "        with open(tokenizer_config_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(tokenizer_config, ensure_ascii=False))\n",
    "\n",
    "        # Sanitize AddedTokens in special_tokens_map\n",
    "        write_dict = convert_added_tokens(self.special_tokens_map_extended, add_type_field=False)\n",
    "        with open(special_tokens_map_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(write_dict, ensure_ascii=False))\n",
    "\n",
    "        file_names = (tokenizer_config_file, special_tokens_map_file)\n",
    "\n",
    "        return self._save_pretrained(\n",
    "            save_directory=save_directory,\n",
    "            file_names=file_names,\n",
    "            legacy_format=legacy_format,\n",
    "            filename_prefix=filename_prefix,\n",
    "        )\n",
    "\n",
    "    def _save_pretrained(\n",
    "        self,\n",
    "        save_directory: Union[str, os.PathLike],\n",
    "        file_names: Tuple[str],\n",
    "        legacy_format: bool = True,\n",
    "        filename_prefix: Optional[str] = None,\n",
    "    ) -> Tuple[str]:\n",
    "        \"\"\"\n",
    "        Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens.\n",
    "\n",
    "        Fast tokenizers can also be saved in a unique JSON file containing {config + vocab + added-tokens} using the\n",
    "        specific :meth:`~transformers.tokenization_utils_fast.PreTrainedTokenizerFast._save_pretrained`\n",
    "        \"\"\"\n",
    "        if not legacy_format:\n",
    "            raise ValueError(\n",
    "                \"Only fast tokenizers (instances of PretrainedTokenizerFast) can be saved in non legacy format.\"\n",
    "            )\n",
    "\n",
    "        save_directory = str(save_directory)\n",
    "\n",
    "        added_tokens_file = os.path.join(\n",
    "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + ADDED_TOKENS_FILE\n",
    "        )\n",
    "        added_vocab = self.get_added_vocab()\n",
    "        if added_vocab:\n",
    "            with open(added_tokens_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                out_str = json.dumps(added_vocab, ensure_ascii=False)\n",
    "                f.write(out_str)\n",
    "\n",
    "        vocab_files = self.save_vocabulary(save_directory, filename_prefix=filename_prefix)\n",
    "\n",
    "        return file_names + vocab_files + (added_tokens_file,)\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
    "        \"\"\"\n",
    "        Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
    "\n",
    "        This method won't save the configuration and special token mappings of the tokenizer. Use\n",
    "        :meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save the whole state of the tokenizer.\n",
    "\n",
    "        Args:\n",
    "            save_directory (:obj:`str`):\n",
    "                The directory in which to save the vocabulary.\n",
    "            filename_prefix (:obj:`str`, `optional`):\n",
    "                An optional prefix to add to the named of the saved files.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Tuple(str)`: Paths to the files saved.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]:\n",
    "        \"\"\"\n",
    "        Converts a string in a sequence of tokens, replacing unknown tokens with the :obj:`unk_token`.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`):\n",
    "                The sequence to be encoded.\n",
    "            pair (:obj:`str`, `optional`):\n",
    "                A second sequence to be encoded with the first.\n",
    "            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to add the special tokens associated with the corresponding model.\n",
    "            kwargs (additional keyword arguments, `optional`):\n",
    "                Will be passed to the underlying model specific encode method. See details in\n",
    "                :meth:`~transformers.PreTrainedTokenizer.__call__`\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[str]`: The list of tokens.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @add_end_docstrings(\n",
    "        ENCODE_KWARGS_DOCSTRING,\n",
    "        \"\"\"\n",
    "            **kwargs: Passed along to the `.tokenize()` method.\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`: The tokenized ids of the\n",
    "            text.\n",
    "        \"\"\",\n",
    "    )\n",
    "    def encode(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        **kwargs\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
    "\n",
    "        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
    "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
    "                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
    "                method).\n",
    "            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
    "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
    "                the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
    "                ``convert_tokens_to_ids`` method).\n",
    "        \"\"\"\n",
    "        encoded_inputs = self.encode_plus(\n",
    "            text,\n",
    "            text_pair=text_pair,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            return_tensors=return_tensors,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return encoded_inputs[\"input_ids\"]\n",
    "\n",
    "    def num_special_tokens_to_add(self, pair: bool = False) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _get_padding_truncation_strategies(\n",
    "        self, padding=False, truncation=False, max_length=None, pad_to_multiple_of=None, verbose=True, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Find the correct padding/truncation strategy with backward compatibility for old arguments (truncation_strategy\n",
    "        and pad_to_max_length) and behaviors.\n",
    "        \"\"\"\n",
    "        old_truncation_strategy = kwargs.pop(\"truncation_strategy\", \"do_not_truncate\")\n",
    "        old_pad_to_max_length = kwargs.pop(\"pad_to_max_length\", False)\n",
    "\n",
    "        # Backward compatibility for previous behavior, maybe we should deprecate it:\n",
    "        # If you only set max_length, it activates truncation for max_length\n",
    "        if max_length is not None and padding is False and truncation is False:\n",
    "            if verbose:\n",
    "                if not self.deprecation_warnings.get(\"Truncation-not-explicitly-activated\", False):\n",
    "                    logger.warning(\n",
    "                        \"Truncation was not explicitly activated but `max_length` is provided a specific value, \"\n",
    "                        \"please use `truncation=True` to explicitly truncate examples to max length. \"\n",
    "                        \"Defaulting to 'longest_first' truncation strategy. \"\n",
    "                        \"If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy \"\n",
    "                        \"more precisely by providing a specific strategy to `truncation`.\"\n",
    "                    )\n",
    "                self.deprecation_warnings[\"Truncation-not-explicitly-activated\"] = True\n",
    "            truncation = \"longest_first\"\n",
    "\n",
    "        # Get padding strategy\n",
    "        if padding is False and old_pad_to_max_length:\n",
    "            if verbose:\n",
    "                warnings.warn(\n",
    "                    \"The `pad_to_max_length` argument is deprecated and will be removed in a future version, \"\n",
    "                    \"use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or \"\n",
    "                    \"use `padding='max_length'` to pad to a max length. In this case, you can give a specific \"\n",
    "                    \"length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the \"\n",
    "                    \"maximal input size of the model (e.g. 512 for Bert).\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "            if max_length is None:\n",
    "                padding_strategy = PaddingStrategy.LONGEST\n",
    "            else:\n",
    "                padding_strategy = PaddingStrategy.MAX_LENGTH\n",
    "        elif padding is not False:\n",
    "            if padding is True:\n",
    "                padding_strategy = PaddingStrategy.LONGEST  # Default to pad to the longest sequence in the batch\n",
    "            elif not isinstance(padding, PaddingStrategy):\n",
    "                padding_strategy = PaddingStrategy(padding)\n",
    "            elif isinstance(padding, PaddingStrategy):\n",
    "                padding_strategy = padding\n",
    "        else:\n",
    "            padding_strategy = PaddingStrategy.DO_NOT_PAD\n",
    "\n",
    "        # Get truncation strategy\n",
    "        if truncation is False and old_truncation_strategy != \"do_not_truncate\":\n",
    "            if verbose:\n",
    "                warnings.warn(\n",
    "                    \"The `truncation_strategy` argument is deprecated and will be removed in a future version, \"\n",
    "                    \"use `truncation=True` to truncate examples to a max length. You can give a specific \"\n",
    "                    \"length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the \"\n",
    "                    \"maximal input size of the model (e.g. 512 for Bert). \"\n",
    "                    \" If you have pairs of inputs, you can give a specific truncation strategy selected among \"\n",
    "                    \"`truncation='only_first'` (will only truncate the first sentence in the pairs) \"\n",
    "                    \"`truncation='only_second'` (will only truncate the second sentence in the pairs) \"\n",
    "                    \"or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "            truncation_strategy = TruncationStrategy(old_truncation_strategy)\n",
    "        elif truncation is not False:\n",
    "            if truncation is True:\n",
    "                truncation_strategy = (\n",
    "                    TruncationStrategy.LONGEST_FIRST\n",
    "                )  # Default to truncate the longest sequences in pairs of inputs\n",
    "            elif not isinstance(truncation, TruncationStrategy):\n",
    "                truncation_strategy = TruncationStrategy(truncation)\n",
    "            elif isinstance(truncation, TruncationStrategy):\n",
    "                truncation_strategy = truncation\n",
    "        else:\n",
    "            truncation_strategy = TruncationStrategy.DO_NOT_TRUNCATE\n",
    "\n",
    "        # Set max length if needed\n",
    "        if max_length is None:\n",
    "            if padding_strategy == PaddingStrategy.MAX_LENGTH:\n",
    "                if self.model_max_length > LARGE_INTEGER:\n",
    "                    if verbose:\n",
    "                        if not self.deprecation_warnings.get(\"Asking-to-pad-to-max_length\", False):\n",
    "                            logger.warning(\n",
    "                                \"Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. \"\n",
    "                                \"Default to no padding.\"\n",
    "                            )\n",
    "                        self.deprecation_warnings[\"Asking-to-pad-to-max_length\"] = True\n",
    "                    padding_strategy = PaddingStrategy.DO_NOT_PAD\n",
    "                else:\n",
    "                    max_length = self.model_max_length\n",
    "\n",
    "            if truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE:\n",
    "                if self.model_max_length > LARGE_INTEGER:\n",
    "                    if verbose:\n",
    "                        if not self.deprecation_warnings.get(\"Asking-to-truncate-to-max_length\", False):\n",
    "                            logger.warning(\n",
    "                                \"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. \"\n",
    "                                \"Default to no truncation.\"\n",
    "                            )\n",
    "                        self.deprecation_warnings[\"Asking-to-truncate-to-max_length\"] = True\n",
    "                    truncation_strategy = TruncationStrategy.DO_NOT_TRUNCATE\n",
    "                else:\n",
    "                    max_length = self.model_max_length\n",
    "\n",
    "        # Test if we have a padding token\n",
    "        if padding_strategy != PaddingStrategy.DO_NOT_PAD and (not self.pad_token or self.pad_token_id < 0):\n",
    "            raise ValueError(\n",
    "                \"Asking to pad but the tokenizer does not have a padding token. \"\n",
    "                \"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\n",
    "                \"or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\"\n",
    "            )\n",
    "\n",
    "        # Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\n",
    "        if (\n",
    "            truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE\n",
    "            and padding_strategy != PaddingStrategy.DO_NOT_PAD\n",
    "            and pad_to_multiple_of is not None\n",
    "            and max_length is not None\n",
    "            and (max_length % pad_to_multiple_of != 0)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Truncation and padding are both activated but \"\n",
    "                f\"truncation length ({max_length}) is not a multiple of pad_to_multiple_of ({pad_to_multiple_of}).\"\n",
    "            )\n",
    "\n",
    "        return padding_strategy, truncation_strategy, max_length, kwargs\n",
    "\n",
    "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
    "    def __call__(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
    "        sequences.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
    "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
    "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
    "                :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
    "            text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
    "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
    "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
    "                :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
    "        \"\"\"\n",
    "        # Input type checking for clearer error\n",
    "        assert isinstance(text, str) or (\n",
    "            isinstance(text, (list, tuple))\n",
    "            and (\n",
    "                len(text) == 0\n",
    "                or (\n",
    "                    isinstance(text[0], str)\n",
    "                    or (isinstance(text[0], (list, tuple)) and (len(text[0]) == 0 or isinstance(text[0][0], str)))\n",
    "                )\n",
    "            )\n",
    "        ), (\n",
    "            \"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\n",
    "            \"or `List[List[str]]` (batch of pretokenized examples).\"\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            text_pair is None\n",
    "            or isinstance(text_pair, str)\n",
    "            or (\n",
    "                isinstance(text_pair, (list, tuple))\n",
    "                and (\n",
    "                    len(text_pair) == 0\n",
    "                    or (\n",
    "                        isinstance(text_pair[0], str)\n",
    "                        or (\n",
    "                            isinstance(text_pair[0], (list, tuple))\n",
    "                            and (len(text_pair[0]) == 0 or isinstance(text_pair[0][0], str))\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ), (\n",
    "            \"text_pair input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\n",
    "            \"or `List[List[str]]` (batch of pretokenized examples).\"\n",
    "        )\n",
    "\n",
    "        is_batched = bool(\n",
    "            (not is_split_into_words and isinstance(text, (list, tuple)))\n",
    "            or (\n",
    "                is_split_into_words and isinstance(text, (list, tuple)) and text and isinstance(text[0], (list, tuple))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if is_batched:\n",
    "            batch_text_or_text_pairs = list(zip(text, text_pair)) if text_pair is not None else text\n",
    "            return self.batch_encode_plus(\n",
    "                batch_text_or_text_pairs=batch_text_or_text_pairs,\n",
    "                add_special_tokens=add_special_tokens,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                max_length=max_length,\n",
    "                stride=stride,\n",
    "                is_split_into_words=is_split_into_words,\n",
    "                pad_to_multiple_of=pad_to_multiple_of,\n",
    "                return_tensors=return_tensors,\n",
    "                return_token_type_ids=return_token_type_ids,\n",
    "                return_attention_mask=return_attention_mask,\n",
    "                return_overflowing_tokens=return_overflowing_tokens,\n",
    "                return_special_tokens_mask=return_special_tokens_mask,\n",
    "                return_offsets_mapping=return_offsets_mapping,\n",
    "                return_length=return_length,\n",
    "                verbose=verbose,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            return self.encode_plus(\n",
    "                text=text,\n",
    "                text_pair=text_pair,\n",
    "                add_special_tokens=add_special_tokens,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                max_length=max_length,\n",
    "                stride=stride,\n",
    "                is_split_into_words=is_split_into_words,\n",
    "                pad_to_multiple_of=pad_to_multiple_of,\n",
    "                return_tensors=return_tensors,\n",
    "                return_token_type_ids=return_token_type_ids,\n",
    "                return_attention_mask=return_attention_mask,\n",
    "                return_overflowing_tokens=return_overflowing_tokens,\n",
    "                return_special_tokens_mask=return_special_tokens_mask,\n",
    "                return_offsets_mapping=return_offsets_mapping,\n",
    "                return_length=return_length,\n",
    "                verbose=verbose,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
    "    def encode_plus(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        Tokenize and prepare for the model a sequence or a pair of sequences.\n",
    "\n",
    "        .. warning::\n",
    "            This method is deprecated, ``__call__`` should be used instead.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
    "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
    "                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
    "                method).\n",
    "            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
    "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
    "                the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
    "                ``convert_tokens_to_ids`` method).\n",
    "        \"\"\"\n",
    "\n",
    "        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\n",
    "        padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return self._encode_plus(\n",
    "            text=text,\n",
    "            text_pair=text_pair,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            padding_strategy=padding_strategy,\n",
    "            truncation_strategy=truncation_strategy,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            is_split_into_words=is_split_into_words,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=return_tensors,\n",
    "            return_token_type_ids=return_token_type_ids,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "            return_overflowing_tokens=return_overflowing_tokens,\n",
    "            return_special_tokens_mask=return_special_tokens_mask,\n",
    "            return_offsets_mapping=return_offsets_mapping,\n",
    "            return_length=return_length,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _encode_plus(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
    "        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
    "    def batch_encode_plus(\n",
    "        self,\n",
    "        batch_text_or_text_pairs: Union[\n",
    "            List[TextInput],\n",
    "            List[TextInputPair],\n",
    "            List[PreTokenizedInput],\n",
    "            List[PreTokenizedInputPair],\n",
    "            List[EncodedInput],\n",
    "            List[EncodedInputPair],\n",
    "        ],\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
    "\n",
    "        .. warning::\n",
    "            This method is deprecated, ``__call__`` should be used instead.\n",
    "\n",
    "        Args:\n",
    "            batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
    "                Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
    "                string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
    "                details in ``encode_plus``).\n",
    "        \"\"\"\n",
    "\n",
    "        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\n",
    "        padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return self._batch_encode_plus(\n",
    "            batch_text_or_text_pairs=batch_text_or_text_pairs,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            padding_strategy=padding_strategy,\n",
    "            truncation_strategy=truncation_strategy,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            is_split_into_words=is_split_into_words,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=return_tensors,\n",
    "            return_token_type_ids=return_token_type_ids,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "            return_overflowing_tokens=return_overflowing_tokens,\n",
    "            return_special_tokens_mask=return_special_tokens_mask,\n",
    "            return_offsets_mapping=return_offsets_mapping,\n",
    "            return_length=return_length,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _batch_encode_plus(\n",
    "        self,\n",
    "        batch_text_or_text_pairs: Union[\n",
    "            List[TextInput],\n",
    "            List[TextInputPair],\n",
    "            List[PreTokenizedInput],\n",
    "            List[PreTokenizedInputPair],\n",
    "            List[EncodedInput],\n",
    "            List[EncodedInputPair],\n",
    "        ],\n",
    "        add_special_tokens: bool = True,\n",
    "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
    "        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def pad(\n",
    "        self,\n",
    "        encoded_inputs: Union[\n",
    "            BatchEncoding,\n",
    "            List[BatchEncoding],\n",
    "            Dict[str, EncodedInput],\n",
    "            Dict[str, List[EncodedInput]],\n",
    "            List[Dict[str, EncodedInput]],\n",
    "        ],\n",
    "        padding: Union[bool, str, PaddingStrategy] = True,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        verbose: bool = True,\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
    "        in the batch.\n",
    "\n",
    "        Padding side (left/right) padding token ids are defined at the tokenizer level (with ``self.padding_side``,\n",
    "        ``self.pad_token_id`` and ``self.pad_token_type_id``)\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
    "            result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n",
    "            case of PyTorch tensors, you will lose the specific device of your tensors however.\n",
    "\n",
    "        Args:\n",
    "            encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):\n",
    "                Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or :obj:`Dict[str,\n",
    "                List[int]]`) or a batch of tokenized inputs (list of :class:`~transformers.BatchEncoding`, `Dict[str,\n",
    "                List[List[int]]]` or `List[Dict[str, List[int]]]`) so you can use this method during preprocessing as\n",
    "                well as in a PyTorch Dataloader collate function.\n",
    "\n",
    "                Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n",
    "                see the note above for the return type.\n",
    "            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
    "                 index) among:\n",
    "\n",
    "                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
    "                  single sequence if provided).\n",
    "                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "                  maximum acceptable input length for the model if that argument is not provided.\n",
    "                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "                  different lengths).\n",
    "            max_length (:obj:`int`, `optional`):\n",
    "                Maximum length of the returned list and optionally padding length (see above).\n",
    "            pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "                If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
    "                >= 7.5 (Volta).\n",
    "            return_attention_mask (:obj:`bool`, `optional`):\n",
    "                Whether to return the attention mask. If left to the default, will return the attention mask according\n",
    "                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
    "\n",
    "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "            return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
    "                If set, will return tensors instead of list of python integers. Acceptable values are:\n",
    "\n",
    "                * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
    "                * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
    "                * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
    "            verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to print more information and warnings.\n",
    "        \"\"\"\n",
    "        # If we have a list of dicts, let's convert it in a dict of lists\n",
    "        # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n",
    "        if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], (dict, BatchEncoding)):\n",
    "            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n",
    "\n",
    "        # The model's main input name, usually `input_ids`, has be passed for padding\n",
    "        if self.model_input_names[0] not in encoded_inputs:\n",
    "            raise ValueError(\n",
    "                \"You should supply an encoding or a list of encodings to this method\"\n",
    "                f\"that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}\"\n",
    "            )\n",
    "\n",
    "        required_input = encoded_inputs[self.model_input_names[0]]\n",
    "\n",
    "        if not required_input:\n",
    "            if return_attention_mask:\n",
    "                encoded_inputs[\"attention_mask\"] = []\n",
    "            return encoded_inputs\n",
    "\n",
    "        # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects\n",
    "        # and rebuild them afterwards if no return_tensors is specified\n",
    "        # Note that we lose the specific device the tensor may be on for PyTorch\n",
    "\n",
    "        first_element = required_input[0]\n",
    "        if isinstance(first_element, (list, tuple)):\n",
    "            # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.\n",
    "            index = 0\n",
    "            while len(required_input[index]) == 0:\n",
    "                index += 1\n",
    "            if index < len(required_input):\n",
    "                first_element = required_input[index][0]\n",
    "        # At this state, if `first_element` is still a list/tuple, it's an empty one so there is nothing to do.\n",
    "        if not isinstance(first_element, (int, list, tuple)):\n",
    "            if is_tf_available() and _is_tensorflow(first_element):\n",
    "                return_tensors = \"tf\" if return_tensors is None else return_tensors\n",
    "            elif is_torch_available() and _is_torch(first_element):\n",
    "                return_tensors = \"pt\" if return_tensors is None else return_tensors\n",
    "            elif isinstance(first_element, np.ndarray):\n",
    "                return_tensors = \"np\" if return_tensors is None else return_tensors\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"type of {first_element} unknown: {type(first_element)}. \"\n",
    "                    f\"Should be one of a python, numpy, pytorch or tensorflow object.\"\n",
    "                )\n",
    "\n",
    "            for key, value in encoded_inputs.items():\n",
    "                encoded_inputs[key] = to_py_obj(value)\n",
    "\n",
    "        # Convert padding_strategy in PaddingStrategy\n",
    "        padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(\n",
    "            padding=padding, max_length=max_length, verbose=verbose\n",
    "        )\n",
    "\n",
    "        required_input = encoded_inputs[self.model_input_names[0]]\n",
    "        if required_input and not isinstance(required_input[0], (list, tuple)):\n",
    "            encoded_inputs = self._pad(\n",
    "                encoded_inputs,\n",
    "                max_length=max_length,\n",
    "                padding_strategy=padding_strategy,\n",
    "                pad_to_multiple_of=pad_to_multiple_of,\n",
    "                return_attention_mask=return_attention_mask,\n",
    "            )\n",
    "            return BatchEncoding(encoded_inputs, tensor_type=return_tensors)\n",
    "\n",
    "        batch_size = len(required_input)\n",
    "        assert all(\n",
    "            len(v) == batch_size for v in encoded_inputs.values()\n",
    "        ), \"Some items in the output dictionary have a different batch size than others.\"\n",
    "\n",
    "        if padding_strategy == PaddingStrategy.LONGEST:\n",
    "            max_length = max(len(inputs) for inputs in required_input)\n",
    "            padding_strategy = PaddingStrategy.MAX_LENGTH\n",
    "\n",
    "        batch_outputs = {}\n",
    "        for i in range(batch_size):\n",
    "            inputs = dict((k, v[i]) for k, v in encoded_inputs.items())\n",
    "            outputs = self._pad(\n",
    "                inputs,\n",
    "                max_length=max_length,\n",
    "                padding_strategy=padding_strategy,\n",
    "                pad_to_multiple_of=pad_to_multiple_of,\n",
    "                return_attention_mask=return_attention_mask,\n",
    "            )\n",
    "\n",
    "            for key, value in outputs.items():\n",
    "                if key not in batch_outputs:\n",
    "                    batch_outputs[key] = []\n",
    "                batch_outputs[key].append(value)\n",
    "\n",
    "        return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Create the token type IDs corresponding to the sequences passed. `What are token type IDs?\n",
    "        <../glossary.html#token-type-ids>`__\n",
    "\n",
    "        Should be overridden in a subclass if the model has a special way of building those.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: The token type ids.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return len(token_ids_0) * [0]\n",
    "        return [0] * len(token_ids_0) + [1] * len(token_ids_1)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
    "        adding special tokens.\n",
    "\n",
    "        This implementation does not add special tokens and this method should be overridden in a subclass.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: The model input with special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return token_ids_0\n",
    "        return token_ids_0 + token_ids_1\n",
    "\n",
    "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
    "    def prepare_for_model(\n",
    "        self,\n",
    "        ids: List[int],\n",
    "        pair_ids: Optional[List[int]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        prepend_batch_axis: bool = False,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
    "        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
    "        manages a moving window (with user defined stride) for overflowing tokens\n",
    "\n",
    "        Args:\n",
    "            ids (:obj:`List[int]`):\n",
    "                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
    "                and ``convert_tokens_to_ids`` methods.\n",
    "            pair_ids (:obj:`List[int]`, `optional`):\n",
    "                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
    "                and ``convert_tokens_to_ids`` methods.\n",
    "        \"\"\"\n",
    "\n",
    "        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\n",
    "        padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        pair = bool(pair_ids is not None)\n",
    "        len_ids = len(ids)\n",
    "        len_pair_ids = len(pair_ids) if pair else 0\n",
    "\n",
    "        if return_token_type_ids and not add_special_tokens:\n",
    "            raise ValueError(\n",
    "                \"Asking to return token_type_ids while setting add_special_tokens to False \"\n",
    "                \"results in an undefined behavior. Please set add_special_tokens to True or \"\n",
    "                \"set return_token_type_ids to None.\"\n",
    "            )\n",
    "\n",
    "        # Load from model defaults\n",
    "        if return_token_type_ids is None:\n",
    "            return_token_type_ids = \"token_type_ids\" in self.model_input_names\n",
    "        if return_attention_mask is None:\n",
    "            return_attention_mask = \"attention_mask\" in self.model_input_names\n",
    "\n",
    "        encoded_inputs = {}\n",
    "\n",
    "        # Compute the total size of the returned encodings\n",
    "        total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)\n",
    "\n",
    "        # Truncation: Handle max sequence length\n",
    "        overflowing_tokens = []\n",
    "        if truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE and max_length and total_len > max_length:\n",
    "            ids, pair_ids, overflowing_tokens = self.truncate_sequences(\n",
    "                ids,\n",
    "                pair_ids=pair_ids,\n",
    "                num_tokens_to_remove=total_len - max_length,\n",
    "                truncation_strategy=truncation_strategy,\n",
    "                stride=stride,\n",
    "            )\n",
    "\n",
    "        if return_overflowing_tokens:\n",
    "            encoded_inputs[\"overflowing_tokens\"] = overflowing_tokens\n",
    "            encoded_inputs[\"num_truncated_tokens\"] = total_len - max_length\n",
    "\n",
    "        # Add special tokens\n",
    "        if add_special_tokens:\n",
    "            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)\n",
    "            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)\n",
    "        else:\n",
    "            sequence = ids + pair_ids if pair else ids\n",
    "            token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])\n",
    "\n",
    "        # Build output dictionary\n",
    "        encoded_inputs[\"input_ids\"] = sequence\n",
    "        if return_token_type_ids:\n",
    "            encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "        if return_special_tokens_mask:\n",
    "            if add_special_tokens:\n",
    "                encoded_inputs[\"special_tokens_mask\"] = self.get_special_tokens_mask(ids, pair_ids)\n",
    "            else:\n",
    "                encoded_inputs[\"special_tokens_mask\"] = [0] * len(sequence)\n",
    "\n",
    "        # Check lengths\n",
    "        self._eventual_warn_about_too_long_sequence(encoded_inputs[\"input_ids\"], max_length, verbose)\n",
    "\n",
    "        # Padding\n",
    "        if padding_strategy != PaddingStrategy.DO_NOT_PAD or return_attention_mask:\n",
    "            encoded_inputs = self.pad(\n",
    "                encoded_inputs,\n",
    "                max_length=max_length,\n",
    "                padding=padding_strategy.value,\n",
    "                pad_to_multiple_of=pad_to_multiple_of,\n",
    "                return_attention_mask=return_attention_mask,\n",
    "            )\n",
    "\n",
    "        if return_length:\n",
    "            encoded_inputs[\"length\"] = len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "        batch_outputs = BatchEncoding(\n",
    "            encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis\n",
    "        )\n",
    "\n",
    "        return batch_outputs\n",
    "\n",
    "    def truncate_sequences(\n",
    "        self,\n",
    "        ids: List[int],\n",
    "        pair_ids: Optional[List[int]] = None,\n",
    "        num_tokens_to_remove: int = 0,\n",
    "        truncation_strategy: Union[str, TruncationStrategy] = \"longest_first\",\n",
    "        stride: int = 0,\n",
    "    ) -> Tuple[List[int], List[int], List[int]]:\n",
    "        \"\"\"\n",
    "        Truncates a sequence pair in-place following the strategy.\n",
    "\n",
    "        Args:\n",
    "            ids (:obj:`List[int]`):\n",
    "                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
    "                and ``convert_tokens_to_ids`` methods.\n",
    "            pair_ids (:obj:`List[int]`, `optional`):\n",
    "                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
    "                and ``convert_tokens_to_ids`` methods.\n",
    "            num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):\n",
    "                Number of tokens to remove using the truncation strategy.\n",
    "            truncation_strategy (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
    "                The strategy to follow for truncation. Can be:\n",
    "\n",
    "                * :obj:`'longest_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
    "                  to the maximum acceptable input length for the model if that argument is not provided. This will\n",
    "                  truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
    "                  sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
    "                  the maximum acceptable input length for the model if that argument is not provided. This will only\n",
    "                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
    "                  to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
    "                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
    "                  greater than the model maximum admissible input size).\n",
    "            stride (:obj:`int`, `optional`, defaults to 0):\n",
    "                If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
    "                sequence returned. The value of this argument defines the number of additional tokens.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Tuple[List[int], List[int], List[int]]`: The truncated ``ids``, the truncated ``pair_ids`` and the\n",
    "            list of overflowing tokens.\n",
    "        \"\"\"\n",
    "        if num_tokens_to_remove <= 0:\n",
    "            return ids, pair_ids, []\n",
    "\n",
    "        if not isinstance(truncation_strategy, TruncationStrategy):\n",
    "            truncation_strategy = TruncationStrategy(truncation_strategy)\n",
    "\n",
    "        overflowing_tokens = []\n",
    "        if truncation_strategy == TruncationStrategy.LONGEST_FIRST:\n",
    "            for _ in range(num_tokens_to_remove):\n",
    "                if pair_ids is None or len(ids) > len(pair_ids):\n",
    "                    if not overflowing_tokens:\n",
    "                        window_len = min(len(ids), stride + 1)\n",
    "                    else:\n",
    "                        window_len = 1\n",
    "                    overflowing_tokens.extend(ids[-window_len:])\n",
    "                    ids = ids[:-1]\n",
    "                else:\n",
    "                    if not overflowing_tokens:\n",
    "                        window_len = min(len(pair_ids), stride + 1)\n",
    "                    else:\n",
    "                        window_len = 1\n",
    "                    overflowing_tokens.extend(pair_ids[-window_len:])\n",
    "                    pair_ids = pair_ids[:-1]\n",
    "        elif truncation_strategy == TruncationStrategy.ONLY_FIRST:\n",
    "            if len(ids) > num_tokens_to_remove:\n",
    "                window_len = min(len(ids), stride + num_tokens_to_remove)\n",
    "                overflowing_tokens = ids[-window_len:]\n",
    "                ids = ids[:-num_tokens_to_remove]\n",
    "            else:\n",
    "                logger.error(\n",
    "                    f\"We need to remove {num_tokens_to_remove} to truncate the input\"\n",
    "                    f\"but the first sequence has a length {len(ids)}. \"\n",
    "                    f\"Please select another truncation strategy than {truncation_strategy}, \"\n",
    "                    f\"for instance 'longest_first' or 'only_second'.\"\n",
    "                )\n",
    "        elif truncation_strategy == TruncationStrategy.ONLY_SECOND and pair_ids is not None:\n",
    "            if len(pair_ids) > num_tokens_to_remove:\n",
    "                window_len = min(len(pair_ids), stride + num_tokens_to_remove)\n",
    "                overflowing_tokens = pair_ids[-window_len:]\n",
    "                pair_ids = pair_ids[:-num_tokens_to_remove]\n",
    "            else:\n",
    "                logger.error(\n",
    "                    f\"We need to remove {num_tokens_to_remove} to truncate the input\"\n",
    "                    f\"but the second sequence has a length {len(pair_ids)}. \"\n",
    "                    f\"Please select another truncation strategy than {truncation_strategy}, \"\n",
    "                    f\"for instance 'longest_first' or 'only_first'.\"\n",
    "                )\n",
    "\n",
    "        return (ids, pair_ids, overflowing_tokens)\n",
    "\n",
    "    def _pad(\n",
    "        self,\n",
    "        encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],\n",
    "        max_length: Optional[int] = None,\n",
    "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Pad encoded inputs (on left/right and up to predefined length or max length in the batch)\n",
    "\n",
    "        Args:\n",
    "            encoded_inputs: Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).\n",
    "            max_length: maximum length of the returned list and optionally padding length (see below).\n",
    "                Will truncate by taking into account the special tokens.\n",
    "            padding_strategy: PaddingStrategy to use for padding.\n",
    "\n",
    "                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch\n",
    "                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)\n",
    "                - PaddingStrategy.DO_NOT_PAD: Do not pad\n",
    "                The tokenizer padding sides are defined in self.padding_side:\n",
    "\n",
    "                    - 'left': pads on the left of the sequences\n",
    "                    - 'right': pads on the right of the sequences\n",
    "            pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n",
    "                This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n",
    "                >= 7.5 (Volta).\n",
    "            return_attention_mask: (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n",
    "        \"\"\"\n",
    "        # Load from model defaults\n",
    "        if return_attention_mask is None:\n",
    "            return_attention_mask = \"attention_mask\" in self.model_input_names\n",
    "\n",
    "        required_input = encoded_inputs[self.model_input_names[0]]\n",
    "\n",
    "        if padding_strategy == PaddingStrategy.LONGEST:\n",
    "            max_length = len(required_input)\n",
    "\n",
    "        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "\n",
    "        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length\n",
    "\n",
    "        if needs_to_be_padded:\n",
    "            difference = max_length - len(required_input)\n",
    "            if self.padding_side == \"right\":\n",
    "                if return_attention_mask:\n",
    "                    encoded_inputs[\"attention_mask\"] = [1] * len(required_input) + [0] * difference\n",
    "                if \"token_type_ids\" in encoded_inputs:\n",
    "                    encoded_inputs[\"token_type_ids\"] = (\n",
    "                        encoded_inputs[\"token_type_ids\"] + [self.pad_token_type_id] * difference\n",
    "                    )\n",
    "                if \"special_tokens_mask\" in encoded_inputs:\n",
    "                    encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n",
    "                encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n",
    "            elif self.padding_side == \"left\":\n",
    "                if return_attention_mask:\n",
    "                    encoded_inputs[\"attention_mask\"] = [0] * difference + [1] * len(required_input)\n",
    "                if \"token_type_ids\" in encoded_inputs:\n",
    "                    encoded_inputs[\"token_type_ids\"] = [self.pad_token_type_id] * difference + encoded_inputs[\n",
    "                        \"token_type_ids\"\n",
    "                    ]\n",
    "                if \"special_tokens_mask\" in encoded_inputs:\n",
    "                    encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n",
    "                encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n",
    "            else:\n",
    "                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n",
    "        elif return_attention_mask and \"attention_mask\" not in encoded_inputs:\n",
    "            encoded_inputs[\"attention_mask\"] = [1] * len(required_input)\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Converts a sequence of token ids in a single string. The most simple way to do it is ``\" \".join(tokens)`` but\n",
    "        we often want to remove sub-word tokenization artifacts at the same time\n",
    "\n",
    "        Args:\n",
    "            tokens (:obj:`List[str]`): The token to join in a string.\n",
    "        Return: The joined tokens.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def batch_decode(\n",
    "        self,\n",
    "        sequences: Union[List[int], List[List[int]], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n",
    "        skip_special_tokens: bool = False,\n",
    "        clean_up_tokenization_spaces: bool = True,\n",
    "        **kwargs\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convert a list of lists of token ids into a list of strings by calling decode.\n",
    "\n",
    "        Args:\n",
    "            sequences (:obj:`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
    "                List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
    "            skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to remove special tokens in the decoding.\n",
    "            clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to clean up the tokenization spaces.\n",
    "            kwargs (additional keyword arguments, `optional`):\n",
    "                Will be passed to the underlying model specific decode method.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[str]`: The list of decoded sentences.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.decode(\n",
    "                seq,\n",
    "                skip_special_tokens=skip_special_tokens,\n",
    "                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "                **kwargs,\n",
    "            )\n",
    "            for seq in sequences\n",
    "        ]\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n",
    "        skip_special_tokens: bool = False,\n",
    "        clean_up_tokenization_spaces: bool = True,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
    "        tokens and clean up tokenization spaces.\n",
    "\n",
    "        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
    "\n",
    "        Args:\n",
    "            token_ids (:obj:`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
    "                List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
    "            skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to remove special tokens in the decoding.\n",
    "            clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to clean up the tokenization spaces.\n",
    "            kwargs (additional keyword arguments, `optional`):\n",
    "                Will be passed to the underlying model specific decode method.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`str`: The decoded sentence.\n",
    "        \"\"\"\n",
    "        # Convert inputs to python lists\n",
    "        token_ids = to_py_obj(token_ids)\n",
    "\n",
    "        return self._decode(\n",
    "            token_ids=token_ids,\n",
    "            skip_special_tokens=skip_special_tokens,\n",
    "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _decode(\n",
    "        self,\n",
    "        token_ids: Union[int, List[int]],\n",
    "        skip_special_tokens: bool = False,\n",
    "        clean_up_tokenization_spaces: bool = True,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of ids of the first sequence.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                List of ids of the second sequence.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        assert already_has_special_tokens and token_ids_1 is None, (\n",
    "            \"You cannot use ``already_has_special_tokens=False`` with this tokenizer. \"\n",
    "            \"Please use a slow (full python) tokenizer to activate this argument.\"\n",
    "            \"Or set `return_special_tokens_mask=True` when calling the encoding method \"\n",
    "            \"to get the special tokens mask in any tokenizer. \"\n",
    "        )\n",
    "\n",
    "        all_special_ids = self.all_special_ids  # cache the property\n",
    "\n",
    "        special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]\n",
    "\n",
    "        return special_tokens_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_up_tokenization(out_string: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
    "\n",
    "        Args:\n",
    "            out_string (:obj:`str`): The text to clean up.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`str`: The cleaned-up string.\n",
    "        \"\"\"\n",
    "        out_string = (\n",
    "            out_string.replace(\" .\", \".\")\n",
    "            .replace(\" ?\", \"?\")\n",
    "            .replace(\" !\", \"!\")\n",
    "            .replace(\" ,\", \",\")\n",
    "            .replace(\" ' \", \"'\")\n",
    "            .replace(\" n't\", \"n't\")\n",
    "            .replace(\" 'm\", \"'m\")\n",
    "            .replace(\" 's\", \"'s\")\n",
    "            .replace(\" 've\", \"'ve\")\n",
    "            .replace(\" 're\", \"'re\")\n",
    "        )\n",
    "        return out_string\n",
    "\n",
    "    def _eventual_warn_about_too_long_sequence(self, ids: List[int], max_length: Optional[int], verbose: bool):\n",
    "        \"\"\"\n",
    "        Depending on the input and internal state we might trigger a warning about a sequence that is too long for it's\n",
    "        corresponding model\n",
    "\n",
    "        Args:\n",
    "            ids (:obj:`List[str]`): The ids produced by the tokenization\n",
    "            max_length (:obj:`int`, `optional`): The max_length desired (does not trigger a warning if it is set)\n",
    "            verbose (:obj:`bool`): Whether or not to print more information and warnings.\n",
    "\n",
    "        \"\"\"\n",
    "        if max_length is None and len(ids) > self.model_max_length and verbose:\n",
    "            if not self.deprecation_warnings.get(\"sequence-length-is-longer-than-the-specified-maximum\", False):\n",
    "                logger.warning(\n",
    "                    \"Token indices sequence length is longer than the specified maximum sequence length \"\n",
    "                    \"for this model ({} > {}). Running this sequence through the model will result in \"\n",
    "                    \"indexing errors\".format(len(ids), self.model_max_length)\n",
    "                )\n",
    "            self.deprecation_warnings[\"sequence-length-is-longer-than-the-specified-maximum\"] = True\n",
    "\n",
    "    @contextmanager\n",
    "    def as_target_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
    "        sequence-to-sequence models that need a slightly different processing for the labels.\n",
    "        \"\"\"\n",
    "        yield\n",
    "\n",
    "    def prepare_seq2seq_batch(\n",
    "        self,\n",
    "        src_texts: List[str],\n",
    "        tgt_texts: Optional[List[str]] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        max_target_length: Optional[int] = None,\n",
    "        padding: str = \"longest\",\n",
    "        return_tensors: str = None,\n",
    "        truncation: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
    "\n",
    "        Arguments:\n",
    "            src_texts (:obj:`List[str]`):\n",
    "                List of documents to summarize or source language texts.\n",
    "            tgt_texts (:obj:`list`, `optional`):\n",
    "                List of summaries or target language texts.\n",
    "            max_length (:obj:`int`, `optional`):\n",
    "                Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
    "                left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum length\n",
    "                is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
    "                length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
    "            max_target_length (:obj:`int`, `optional`):\n",
    "                Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
    "                to :obj:`None`, this will use the max_length value.\n",
    "            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
    "                Activates and controls padding. Accepts the following values:\n",
    "\n",
    "                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
    "                  single sequence if provided).\n",
    "                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "                  maximum acceptable input length for the model if that argument is not provided.\n",
    "                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "                  different lengths).\n",
    "            return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
    "                If set, will return tensors instead of list of python integers. Acceptable values are:\n",
    "\n",
    "                * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
    "                * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
    "                * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
    "            truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):\n",
    "                Activates and controls truncation. Accepts the following values:\n",
    "\n",
    "                * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
    "                  :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
    "                  provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
    "                  if a pair of sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
    "                  the maximum acceptable input length for the model if that argument is not provided. This will only\n",
    "                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
    "                  to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
    "                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
    "                * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
    "                  sequence lengths greater than the model maximum admissible input size).\n",
    "            **kwargs:\n",
    "                Additional keyword arguments passed along to :obj:`self.__call__`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
    "\n",
    "            - **input_ids** -- List of token ids to be fed to the encoder.\n",
    "            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
    "            - **labels** -- List of token ids for tgt_texts.\n",
    "\n",
    "            The full set of keys ``[input_ids, attention_mask, labels]``, will only be returned if tgt_texts is passed.\n",
    "            Otherwise, input_ids, attention_mask will be the only keys.\n",
    "        \"\"\"\n",
    "        # mBART-specific kwargs that should be ignored by other models.\n",
    "        kwargs.pop(\"src_lang\", None)\n",
    "        kwargs.pop(\"tgt_lang\", None)\n",
    "        if max_length is None:\n",
    "            max_length = self.model_max_length\n",
    "        model_inputs = self(\n",
    "            src_texts,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=return_tensors,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if tgt_texts is None:\n",
    "            return model_inputs\n",
    "        # Process tgt_texts\n",
    "        if max_target_length is None:\n",
    "            max_target_length = max_length\n",
    "        with self.as_target_tokenizer():\n",
    "            labels = self(\n",
    "                tgt_texts,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=return_tensors,\n",
    "                padding=padding,\n",
    "                max_length=max_target_length,\n",
    "                truncation=truncation,\n",
    "                **kwargs,\n",
    "            )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:42.992268Z",
     "start_time": "2021-02-10T14:49:42.937355Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    " Tokenization classes for python tokenizers. For fast tokenizers (provided by HuggingFace's tokenizers library) see\n",
    " tokenization_utils_fast.py\n",
    "\"\"\"\n",
    "import itertools\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, overload\n",
    "\n",
    "# from .file_utils import add_end_docstrings\n",
    "# from .tokenization_utils_base import (\n",
    "#     ENCODE_KWARGS_DOCSTRING,\n",
    "#     ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING,\n",
    "#     INIT_TOKENIZER_DOCSTRING,\n",
    "#     AddedToken,\n",
    "#     BatchEncoding,\n",
    "#     EncodedInput,\n",
    "#     EncodedInputPair,\n",
    "#     PaddingStrategy,\n",
    "#     PreTokenizedInput,\n",
    "#     PreTokenizedInputPair,\n",
    "#     PreTrainedTokenizerBase,\n",
    "#     TensorType,\n",
    "#     TextInput,\n",
    "#     TextInputPair,\n",
    "#     TruncationStrategy,\n",
    "# )\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "logger =get_logger(__name__)\n",
    "\n",
    "# Slow tokenizers are saved in a vocabulary plus three separated files\n",
    "SPECIAL_TOKENS_MAP_FILE = \"special_tokens_map.json\"\n",
    "ADDED_TOKENS_FILE = \"added_tokens.json\"\n",
    "TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n",
    "\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `char` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically control characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `char` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `char` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_end_of_word(text):\n",
    "    \"\"\"Checks whether the last character in text is one of a punctuation, control or whitespace character.\"\"\"\n",
    "    last_char = text[-1]\n",
    "    return bool(_is_control(last_char) | _is_punctuation(last_char) | _is_whitespace(last_char))\n",
    "\n",
    "\n",
    "def _is_start_of_word(text):\n",
    "    \"\"\"Checks whether the first character in text is one of a punctuation, control or whitespace character.\"\"\"\n",
    "    first_char = text[0]\n",
    "    return bool(_is_control(first_char) | _is_punctuation(first_char) | _is_whitespace(first_char))\n",
    "\n",
    "\n",
    "@add_end_docstrings(INIT_TOKENIZER_DOCSTRING, \"\"\"    .. automethod:: __call__\"\"\")\n",
    "class PreTrainedTokenizer(PreTrainedTokenizerBase):\n",
    "    \"\"\"\n",
    "    Base class for all slow tokenizers.\n",
    "\n",
    "    Inherits from :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase`.\n",
    "\n",
    "    Handle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading\n",
    "    pretrained tokenizers as well as adding tokens to the vocabulary.\n",
    "\n",
    "    This class also contain the added tokens in a unified way on top of all tokenizers so we don't have to handle the\n",
    "    specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Added tokens - We store this for both slow and fast tokenizers\n",
    "        # until the serialization of Fast tokenizers is updated\n",
    "        self.added_tokens_encoder: Dict[str, int] = {}\n",
    "        self.added_tokens_decoder: Dict[int, str] = {}\n",
    "        self.unique_no_split_tokens: List[str] = []\n",
    "\n",
    "    @property\n",
    "    def is_fast(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        :obj:`int`: Size of the base vocabulary (without the added tokens).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_added_vocab(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Dict[str, int]`: The added tokens.\n",
    "        \"\"\"\n",
    "        return self.added_tokens_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Size of the full vocabulary with the added tokens.\n",
    "        \"\"\"\n",
    "        return self.vocab_size + len(self.added_tokens_encoder)\n",
    "\n",
    "    def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
    "        it with indices starting from length of the current vocabulary.\n",
    "\n",
    "        Args:\n",
    "            new_tokens (:obj:`List[str]`or :obj:`List[tokenizers.AddedToken]`):\n",
    "                Token(s) to add in vocabulary. A token is only added if it's not already in the vocabulary (tested by\n",
    "                checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
    "            special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not the tokens should be added as special tokens.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: The number of tokens actually added to the vocabulary.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
    "            print('We have added', num_added_toks, 'tokens')\n",
    "            # Note: resize_token_embeddings expects to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        \"\"\"\n",
    "        new_tokens = [str(tok) for tok in new_tokens]\n",
    "\n",
    "        tokens_to_add = []\n",
    "        for token in new_tokens:\n",
    "            assert isinstance(token, str)\n",
    "            if not special_tokens and hasattr(self, \"do_lower_case\") and self.do_lower_case:\n",
    "                token = token.lower()\n",
    "            if (\n",
    "                token != self.unk_token\n",
    "                and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token)\n",
    "                and token not in tokens_to_add\n",
    "            ):\n",
    "                tokens_to_add.append(token)\n",
    "                if self.verbose:\n",
    "                    logger.info(\"Adding %s to the vocabulary\", token)\n",
    "\n",
    "        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(tokens_to_add))\n",
    "        added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}\n",
    "        self.added_tokens_encoder.update(added_tok_encoder)\n",
    "        self.added_tokens_decoder.update(added_tok_decoder)\n",
    "\n",
    "        # Make sure we don't split on any special tokens (even they were already in the vocab before e.g. for Albert)\n",
    "        if special_tokens:\n",
    "            self.unique_no_split_tokens = sorted(set(self.unique_no_split_tokens).union(set(new_tokens)))\n",
    "        else:\n",
    "            # Or on the newly added tokens\n",
    "            self.unique_no_split_tokens = sorted(set(self.unique_no_split_tokens).union(set(tokens_to_add)))\n",
    "\n",
    "        return len(tokens_to_add)\n",
    "\n",
    "    def num_special_tokens_to_add(self, pair: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of added tokens when encoding a sequence with special tokens.\n",
    "\n",
    "        .. note::\n",
    "            This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not\n",
    "            put this inside your training loop.\n",
    "\n",
    "        Args:\n",
    "            pair (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
    "                sequence.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: Number of special tokens added to sequences.\n",
    "        \"\"\"\n",
    "        token_ids_0 = []\n",
    "        token_ids_1 = []\n",
    "        return len(self.build_inputs_with_special_tokens(token_ids_0, token_ids_1 if pair else None))\n",
    "\n",
    "    def tokenize(self, text: TextInput, **kwargs) -> List[str]:\n",
    "        \"\"\"\n",
    "        Converts a string in a sequence of tokens, using the tokenizer.\n",
    "\n",
    "        Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies\n",
    "        (BPE/SentencePieces/WordPieces). Takes care of added tokens.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`):\n",
    "                The sequence to be encoded.\n",
    "            **kwargs (additional keyword arguments):\n",
    "                Passed along to the model-specific ``prepare_for_tokenization`` preprocessing method.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[str]`: The list of tokens.\n",
    "        \"\"\"\n",
    "        # Simple mapping string => AddedToken for special tokens with specific tokenization behaviors\n",
    "        all_special_tokens_extended = dict(\n",
    "            (str(t), t) for t in self.all_special_tokens_extended if isinstance(t, AddedToken)\n",
    "        )\n",
    "\n",
    "        text, kwargs = self.prepare_for_tokenization(text, **kwargs)\n",
    "\n",
    "        if kwargs:\n",
    "            logger.warning(f\"Keyword arguments {kwargs} not recognized.\")\n",
    "\n",
    "        # TODO: should this be in the base class?\n",
    "        if hasattr(self, \"do_lower_case\") and self.do_lower_case:\n",
    "            # convert non-special tokens to lowercase\n",
    "            escaped_special_toks = [re.escape(s_tok) for s_tok in self.all_special_tokens]\n",
    "            pattern = r\"(\" + r\"|\".join(escaped_special_toks) + r\")|\" + r\"(.+?)\"\n",
    "            text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)\n",
    "\n",
    "        def split_on_token(tok, text):\n",
    "            result = []\n",
    "            tok_extended = all_special_tokens_extended.get(tok, None)\n",
    "            split_text = text.split(tok)\n",
    "            full_word = \"\"\n",
    "            for i, sub_text in enumerate(split_text):\n",
    "                # AddedToken can control whitespace stripping around them.\n",
    "                # We use them for GPT2 and Roberta to have different behavior depending on the special token\n",
    "                # Cf. https://github.com/huggingface/transformers/pull/2778\n",
    "                # and https://github.com/huggingface/transformers/issues/3788\n",
    "                if isinstance(tok_extended, AddedToken):\n",
    "                    if tok_extended.single_word:\n",
    "                        # Try to avoid splitting on token\n",
    "                        if (\n",
    "                            i < len(split_text) - 1\n",
    "                            and not _is_end_of_word(sub_text)\n",
    "                            and not _is_start_of_word(split_text[i + 1])\n",
    "                        ):\n",
    "                            # Don't extract the special token\n",
    "                            full_word += sub_text + tok\n",
    "                        elif full_word:\n",
    "                            full_word += sub_text\n",
    "                            result.append(full_word)\n",
    "                            full_word = \"\"\n",
    "                            continue\n",
    "                    # Strip white spaces on the right\n",
    "                    if tok_extended.rstrip and i > 0:\n",
    "                        # A bit counter-intuitive but we strip the left of the string\n",
    "                        # since tok_extended.rstrip means the special token is eating all white spaces on its right\n",
    "                        sub_text = sub_text.lstrip()\n",
    "                    # Strip white spaces on the left\n",
    "                    if tok_extended.lstrip and i < len(split_text) - 1:\n",
    "                        sub_text = sub_text.rstrip()  # Opposite here\n",
    "                else:\n",
    "                    # We strip left and right by default\n",
    "                    if i < len(split_text) - 1:\n",
    "                        sub_text = sub_text.rstrip()\n",
    "                    if i > 0:\n",
    "                        sub_text = sub_text.lstrip()\n",
    "\n",
    "                if i == 0 and not sub_text:\n",
    "                    result.append(tok)\n",
    "                elif i == len(split_text) - 1:\n",
    "                    if sub_text:\n",
    "                        result.append(sub_text)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    if sub_text:\n",
    "                        result.append(sub_text)\n",
    "                    result.append(tok)\n",
    "            return result\n",
    "\n",
    "        def split_on_tokens(tok_list, text):\n",
    "            if not text.strip():\n",
    "                return []\n",
    "            if not tok_list:\n",
    "                return self._tokenize(text)\n",
    "\n",
    "            tokenized_text = []\n",
    "            text_list = [text]\n",
    "            for tok in tok_list:\n",
    "                tokenized_text = []\n",
    "                for sub_text in text_list:\n",
    "                    if sub_text not in self.unique_no_split_tokens:\n",
    "                        tokenized_text.extend(split_on_token(tok, sub_text))\n",
    "                    else:\n",
    "                        tokenized_text.append(sub_text)\n",
    "                text_list = tokenized_text\n",
    "\n",
    "            return list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    (\n",
    "                        self._tokenize(token) if token not in self.unique_no_split_tokens else [token]\n",
    "                        for token in tokenized_text\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        no_split_token = self.unique_no_split_tokens\n",
    "        tokenized_text = split_on_tokens(no_split_token, text)\n",
    "        return tokenized_text\n",
    "\n",
    "    def _tokenize(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Converts a string in a sequence of tokens (string), using the tokenizer. Split in words for word-based\n",
    "        vocabulary or sub-words for sub-word-based vocabularies (BPE/SentencePieces/WordPieces).\n",
    "\n",
    "        Do NOT take care of added tokens.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
    "        vocabulary.\n",
    "\n",
    "        Args:\n",
    "            tokens (:obj:`str` or :obj:`List[str]`): One or several token(s) to convert to token id(s).\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int` or :obj:`List[int]`: The token id or list of token ids.\n",
    "        \"\"\"\n",
    "        if tokens is None:\n",
    "            return None\n",
    "\n",
    "        if isinstance(tokens, str):\n",
    "            return self._convert_token_to_id_with_added_voc(tokens)\n",
    "\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self._convert_token_to_id_with_added_voc(token))\n",
    "        return ids\n",
    "\n",
    "    def _convert_token_to_id_with_added_voc(self, token):\n",
    "        if token is None:\n",
    "            return None\n",
    "\n",
    "        if token in self.added_tokens_encoder:\n",
    "            return self.added_tokens_encoder[token]\n",
    "        return self._convert_token_to_id(token)\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _encode_plus(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
    "        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        def get_input_ids(text):\n",
    "            if isinstance(text, str):\n",
    "                tokens = self.tokenize(text, **kwargs)\n",
    "                return self.convert_tokens_to_ids(tokens)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], str):\n",
    "                if is_split_into_words:\n",
    "                    tokens = list(\n",
    "                        itertools.chain(*(self.tokenize(t, is_split_into_words=True, **kwargs) for t in text))\n",
    "                    )\n",
    "                    return self.convert_tokens_to_ids(tokens)\n",
    "                else:\n",
    "                    return self.convert_tokens_to_ids(text)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n",
    "                return text\n",
    "            else:\n",
    "                if is_split_into_words:\n",
    "                    raise ValueError(\n",
    "                        f\"Input {text} is not valid. Should be a string or a list/tuple of strings when `is_split_into_words=True`.\"\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\n",
    "                    )\n",
    "\n",
    "        if return_offsets_mapping:\n",
    "            raise NotImplementedError(\n",
    "                \"return_offset_mapping is not available when using Python tokenizers.\"\n",
    "                \"To use this feature, change your tokenizer to one deriving from \"\n",
    "                \"transformers.PreTrainedTokenizerFast.\"\n",
    "                \"More information on available tokenizers at \"\n",
    "                \"https://github.com/huggingface/transformers/pull/2674\"\n",
    "            )\n",
    "\n",
    "        first_ids = get_input_ids(text)\n",
    "        second_ids = get_input_ids(text_pair) if text_pair is not None else None\n",
    "\n",
    "        return self.prepare_for_model(\n",
    "            first_ids,\n",
    "            pair_ids=second_ids,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            padding=padding_strategy.value,\n",
    "            truncation=truncation_strategy.value,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=return_tensors,\n",
    "            prepend_batch_axis=True,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "            return_token_type_ids=return_token_type_ids,\n",
    "            return_overflowing_tokens=return_overflowing_tokens,\n",
    "            return_special_tokens_mask=return_special_tokens_mask,\n",
    "            return_length=return_length,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def _batch_encode_plus(\n",
    "        self,\n",
    "        batch_text_or_text_pairs: Union[\n",
    "            List[TextInput],\n",
    "            List[TextInputPair],\n",
    "            List[PreTokenizedInput],\n",
    "            List[PreTokenizedInputPair],\n",
    "            List[EncodedInput],\n",
    "            List[EncodedInputPair],\n",
    "        ],\n",
    "        add_special_tokens: bool = True,\n",
    "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
    "        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        def get_input_ids(text):\n",
    "            if isinstance(text, str):\n",
    "                tokens = self.tokenize(text, **kwargs)\n",
    "                return self.convert_tokens_to_ids(tokens)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], str):\n",
    "                if is_split_into_words:\n",
    "                    tokens = list(\n",
    "                        itertools.chain(*(self.tokenize(t, is_split_into_words=True, **kwargs) for t in text))\n",
    "                    )\n",
    "                    return self.convert_tokens_to_ids(tokens)\n",
    "                else:\n",
    "                    return self.convert_tokens_to_ids(text)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n",
    "                return text\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\n",
    "                )\n",
    "\n",
    "        if return_offsets_mapping:\n",
    "            raise NotImplementedError(\n",
    "                \"return_offset_mapping is not available when using Python tokenizers.\"\n",
    "                \"To use this feature, change your tokenizer to one deriving from \"\n",
    "                \"transformers.PreTrainedTokenizerFast.\"\n",
    "            )\n",
    "\n",
    "        input_ids = []\n",
    "        for ids_or_pair_ids in batch_text_or_text_pairs:\n",
    "            if not isinstance(ids_or_pair_ids, (list, tuple)):\n",
    "                ids, pair_ids = ids_or_pair_ids, None\n",
    "            elif is_split_into_words and not isinstance(ids_or_pair_ids[0], (list, tuple)):\n",
    "                ids, pair_ids = ids_or_pair_ids, None\n",
    "            else:\n",
    "                ids, pair_ids = ids_or_pair_ids\n",
    "\n",
    "            first_ids = get_input_ids(ids)\n",
    "            second_ids = get_input_ids(pair_ids) if pair_ids is not None else None\n",
    "            input_ids.append((first_ids, second_ids))\n",
    "\n",
    "        batch_outputs = self._batch_prepare_for_model(\n",
    "            input_ids,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            padding_strategy=padding_strategy,\n",
    "            truncation_strategy=truncation_strategy,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "            return_token_type_ids=return_token_type_ids,\n",
    "            return_overflowing_tokens=return_overflowing_tokens,\n",
    "            return_special_tokens_mask=return_special_tokens_mask,\n",
    "            return_length=return_length,\n",
    "            return_tensors=return_tensors,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        return BatchEncoding(batch_outputs)\n",
    "\n",
    "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
    "    def _batch_prepare_for_model(\n",
    "        self,\n",
    "        batch_ids_pairs: List[Union[PreTokenizedInputPair, Tuple[List[int], None]]],\n",
    "        add_special_tokens: bool = True,\n",
    "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
    "        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[str] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
    "        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
    "        manages a moving window (with user defined stride) for overflowing tokens\n",
    "\n",
    "        Args:\n",
    "            batch_ids_pairs: list of tokenized input ids or input ids pairs\n",
    "        \"\"\"\n",
    "\n",
    "        batch_outputs = {}\n",
    "        for first_ids, second_ids in batch_ids_pairs:\n",
    "            outputs = self.prepare_for_model(\n",
    "                first_ids,\n",
    "                second_ids,\n",
    "                add_special_tokens=add_special_tokens,\n",
    "                padding=PaddingStrategy.DO_NOT_PAD.value,  # we pad in batch afterward\n",
    "                truncation=truncation_strategy.value,\n",
    "                max_length=max_length,\n",
    "                stride=stride,\n",
    "                pad_to_multiple_of=None,  # we pad in batch afterward\n",
    "                return_attention_mask=False,  # we pad in batch afterward\n",
    "                return_token_type_ids=return_token_type_ids,\n",
    "                return_overflowing_tokens=return_overflowing_tokens,\n",
    "                return_special_tokens_mask=return_special_tokens_mask,\n",
    "                return_length=return_length,\n",
    "                return_tensors=None,  # We convert the whole batch to tensors at the end\n",
    "                prepend_batch_axis=False,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "\n",
    "            for key, value in outputs.items():\n",
    "                if key not in batch_outputs:\n",
    "                    batch_outputs[key] = []\n",
    "                batch_outputs[key].append(value)\n",
    "\n",
    "        batch_outputs = self.pad(\n",
    "            batch_outputs,\n",
    "            padding=padding_strategy.value,\n",
    "            max_length=max_length,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "        )\n",
    "\n",
    "        batch_outputs = BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
    "\n",
    "        return batch_outputs\n",
    "\n",
    "    def prepare_for_tokenization(\n",
    "        self, text: str, is_split_into_words: bool = False, **kwargs\n",
    "    ) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Performs any necessary transformations before tokenization.\n",
    "\n",
    "        This method should pop the arguments from kwargs and return the remaining :obj:`kwargs` as well. We test the\n",
    "        :obj:`kwargs` at the end of the encoding process to be sure all the arguments have been used.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`):\n",
    "                The text to prepare.\n",
    "            is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not the text has been pretokenized.\n",
    "            kwargs:\n",
    "                Keyword arguments to use for the tokenization.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.\n",
    "        \"\"\"\n",
    "        return (text, kwargs)\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List, token_ids_1: Optional[List] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of ids of the first sequence.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                List of ids of the second sequence.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        return [0] * ((len(token_ids_1) if token_ids_1 else 0) + len(token_ids_0))\n",
    "\n",
    "    @overload\n",
    "    def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = False) -> str:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def convert_ids_to_tokens(self, ids: List[int], skip_special_tokens: bool = False) -> List[str]:\n",
    "        ...\n",
    "\n",
    "    def convert_ids_to_tokens(\n",
    "        self, ids: Union[int, List[int]], skip_special_tokens: bool = False\n",
    "    ) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
    "        added tokens.\n",
    "\n",
    "        Args:\n",
    "            ids (:obj:`int` or :obj:`List[int]`):\n",
    "                The token id (or token ids) to convert to tokens.\n",
    "            skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to remove special tokens in the decoding.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`str` or :obj:`List[str]`: The decoded token(s).\n",
    "        \"\"\"\n",
    "        if isinstance(ids, int):\n",
    "            if ids in self.added_tokens_decoder:\n",
    "                return self.added_tokens_decoder[ids]\n",
    "            else:\n",
    "                return self._convert_id_to_token(ids)\n",
    "        tokens = []\n",
    "        for index in ids:\n",
    "            index = int(index)\n",
    "            if skip_special_tokens and index in self.all_special_ids:\n",
    "                continue\n",
    "            if index in self.added_tokens_decoder:\n",
    "                tokens.append(self.added_tokens_decoder[index])\n",
    "            else:\n",
    "                tokens.append(self._convert_id_to_token(index))\n",
    "        return tokens\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def _decode(\n",
    "        self,\n",
    "        token_ids: List[int],\n",
    "        skip_special_tokens: bool = False,\n",
    "        clean_up_tokenization_spaces: bool = True,\n",
    "        spaces_between_special_tokens: bool = True,\n",
    "    ) -> str:\n",
    "        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "        # To avoid mixing byte-level and unicode for byte-level BPT\n",
    "        # we need to build string separately for added tokens and byte-level tokens\n",
    "        # cf. https://github.com/huggingface/transformers/issues/1133\n",
    "        sub_texts = []\n",
    "        current_sub_text = []\n",
    "        for token in filtered_tokens:\n",
    "            if skip_special_tokens and token in self.all_special_ids:\n",
    "                continue\n",
    "            if token in self.added_tokens_encoder:\n",
    "                if current_sub_text:\n",
    "                    sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n",
    "                    current_sub_text = []\n",
    "                sub_texts.append(token)\n",
    "            else:\n",
    "                current_sub_text.append(token)\n",
    "        if current_sub_text:\n",
    "            sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n",
    "\n",
    "        if spaces_between_special_tokens:\n",
    "            text = \" \".join(sub_texts)\n",
    "        else:\n",
    "            text = \"\".join(sub_texts)\n",
    "\n",
    "        if clean_up_tokenization_spaces:\n",
    "            clean_text = self.clean_up_tokenization(text)\n",
    "            return clean_text\n",
    "        else:\n",
    "            return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:47.486983Z",
     "start_time": "2021-02-10T14:49:47.443128Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Tokenization classes for Bert.\"\"\"\n",
    "\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import unicodedata\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n",
    "# from ...utils import logging\n",
    "\n",
    "\n",
    "logger =get_logger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\",\n",
    "        \"bert-large-uncased\": \"https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt\",\n",
    "        \"bert-base-cased\": \"https://huggingface.co/bert-base-cased/resolve/main/vocab.txt\",\n",
    "        \"bert-large-cased\": \"https://huggingface.co/bert-large-cased/resolve/main/vocab.txt\",\n",
    "        \"bert-base-multilingual-uncased\": \"https://huggingface.co/bert-base-multilingual-uncased/resolve/main/vocab.txt\",\n",
    "        \"bert-base-multilingual-cased\": \"https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt\",\n",
    "        \"bert-base-chinese\": \"https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\",\n",
    "        \"bert-base-german-cased\": \"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\",\n",
    "        \"bert-large-uncased-whole-word-masking\": \"https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt\",\n",
    "        \"bert-large-cased-whole-word-masking\": \"https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/vocab.txt\",\n",
    "        \"bert-large-uncased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt\",\n",
    "        \"bert-large-cased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt\",\n",
    "        \"bert-base-cased-finetuned-mrpc\": \"https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/vocab.txt\",\n",
    "        \"bert-base-german-dbmdz-cased\": \"https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/vocab.txt\",\n",
    "        \"bert-base-german-dbmdz-uncased\": \"https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/vocab.txt\",\n",
    "        \"TurkuNLP/bert-base-finnish-cased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt\",\n",
    "        \"TurkuNLP/bert-base-finnish-uncased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/vocab.txt\",\n",
    "        \"wietsedv/bert-base-dutch-cased\": \"https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/vocab.txt\",\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"bert-base-uncased\": 512,\n",
    "    \"bert-large-uncased\": 512,\n",
    "    \"bert-base-cased\": 512,\n",
    "    \"bert-large-cased\": 512,\n",
    "    \"bert-base-multilingual-uncased\": 512,\n",
    "    \"bert-base-multilingual-cased\": 512,\n",
    "    \"bert-base-chinese\": 512,\n",
    "    \"bert-base-german-cased\": 512,\n",
    "    \"bert-large-uncased-whole-word-masking\": 512,\n",
    "    \"bert-large-cased-whole-word-masking\": 512,\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": 512,\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": 512,\n",
    "    \"bert-base-cased-finetuned-mrpc\": 512,\n",
    "    \"bert-base-german-dbmdz-cased\": 512,\n",
    "    \"bert-base-german-dbmdz-uncased\": 512,\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\": 512,\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\": 512,\n",
    "    \"wietsedv/bert-base-dutch-cased\": 512,\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"bert-base-uncased\": {\"do_lower_case\": True},\n",
    "    \"bert-large-uncased\": {\"do_lower_case\": True},\n",
    "    \"bert-base-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-large-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-base-multilingual-uncased\": {\"do_lower_case\": True},\n",
    "    \"bert-base-multilingual-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-base-chinese\": {\"do_lower_case\": False},\n",
    "    \"bert-base-german-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-large-uncased-whole-word-masking\": {\"do_lower_case\": True},\n",
    "    \"bert-large-cased-whole-word-masking\": {\"do_lower_case\": False},\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": {\"do_lower_case\": True},\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": {\"do_lower_case\": False},\n",
    "    \"bert-base-cased-finetuned-mrpc\": {\"do_lower_case\": False},\n",
    "    \"bert-base-german-dbmdz-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-base-german-dbmdz-uncased\": {\"do_lower_case\": True},\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\": {\"do_lower_case\": False},\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\": {\"do_lower_case\": True},\n",
    "    \"wietsedv/bert-base-dutch-cased\": {\"do_lower_case\": False},\n",
    "}\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class BertTokenizer(PreTrainedTokenizer):\n",
    "    r\"\"\"\n",
    "    Construct a BERT tokenizer. Based on WordPiece.\n",
    "\n",
    "    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n",
    "    Users should refer to this superclass for more information regarding those methods.\n",
    "\n",
    "    Args:\n",
    "        vocab_file (:obj:`str`):\n",
    "            File containing the vocabulary.\n",
    "        do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to lowercase the input when tokenizing.\n",
    "        do_basic_tokenize (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to do basic tokenization before WordPiece.\n",
    "        never_split (:obj:`Iterable`, `optional`):\n",
    "            Collection of tokens which will never be split during tokenization. Only has an effect when\n",
    "            :obj:`do_basic_tokenize=True`\n",
    "        unk_token (:obj:`str`, `optional`, defaults to :obj:`\"[UNK]\"`):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        sep_token (:obj:`str`, `optional`, defaults to :obj:`\"[SEP]\"`):\n",
    "            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
    "            sequence classification or for a text and a question for question answering. It is also used as the last\n",
    "            token of a sequence built with special tokens.\n",
    "        pad_token (:obj:`str`, `optional`, defaults to :obj:`\"[PAD]\"`):\n",
    "            The token used for padding, for example when batching sequences of different lengths.\n",
    "        cls_token (:obj:`str`, `optional`, defaults to :obj:`\"[CLS]\"`):\n",
    "            The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
    "            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
    "        mask_token (:obj:`str`, `optional`, defaults to :obj:`\"[MASK]\"`):\n",
    "            The token used for masking values. This is the token used when training this model with masked language\n",
    "            modeling. This is the token which the model will try to predict.\n",
    "        tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to tokenize Chinese characters.\n",
    "\n",
    "            This should likely be deactivated for Japanese (see this `issue\n",
    "            <https://github.com/huggingface/transformers/issues/328>`__).\n",
    "        strip_accents: (:obj:`bool`, `optional`):\n",
    "            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
    "            value for :obj:`lowercase` (as in the original BERT).\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        do_lower_case=True,\n",
    "        do_basic_tokenize=True,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        tokenize_chinese_chars=True,\n",
    "        strip_accents=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            do_lower_case=do_lower_case,\n",
    "            do_basic_tokenize=do_basic_tokenize,\n",
    "            never_split=never_split,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "            strip_accents=strip_accents,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file)\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.do_basic_tokenize = do_basic_tokenize\n",
    "        if do_basic_tokenize:\n",
    "            self.basic_tokenizer = BasicTokenizer(\n",
    "                do_lower_case=do_lower_case,\n",
    "                never_split=never_split,\n",
    "                tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "                strip_accents=strip_accents,\n",
    "            )\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
    "\n",
    "    @property\n",
    "    def do_lower_case(self):\n",
    "        return self.basic_tokenizer.do_lower_case\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.vocab, **self.added_tokens_encoder)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        if self.do_basic_tokenize:\n",
    "            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
    "\n",
    "                # If the token is part of the never_split set\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                else:\n",
    "                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n",
    "        else:\n",
    "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
    "        return split_tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
    "        adding special tokens. A BERT sequence has the following format:\n",
    "\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs to which the special tokens will be added.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formatted with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n",
    "        pair mask has the following format:\n",
    "\n",
    "        ::\n",
    "\n",
    "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "            | first sequence    | second sequence |\n",
    "\n",
    "        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
    "            sequence(s).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
    "        index = 0\n",
    "        if os.path.isdir(save_directory):\n",
    "            vocab_file = os.path.join(\n",
    "                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
    "            )\n",
    "        else:\n",
    "            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)\n",
    "\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "    \"\"\"\n",
    "    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n",
    "\n",
    "    Args:\n",
    "        do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to lowercase the input when tokenizing.\n",
    "        never_split (:obj:`Iterable`, `optional`):\n",
    "            Collection of tokens which will never be split during tokenization. Only has an effect when\n",
    "            :obj:`do_basic_tokenize=True`\n",
    "        tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to tokenize Chinese characters.\n",
    "\n",
    "            This should likely be deactivated for Japanese (see this `issue\n",
    "            <https://github.com/huggingface/transformers/issues/328>`__).\n",
    "        strip_accents: (:obj:`bool`, `optional`):\n",
    "            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
    "            value for :obj:`lowercase` (as in the original BERT).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True, strip_accents=None):\n",
    "        if never_split is None:\n",
    "            never_split = []\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = set(never_split)\n",
    "        self.tokenize_chinese_chars = tokenize_chinese_chars\n",
    "        self.strip_accents = strip_accents\n",
    "\n",
    "    def tokenize(self, text, never_split=None):\n",
    "        \"\"\"\n",
    "        Basic Tokenization of a piece of text. Split on \"white spaces\" only, for sub-word tokenization, see\n",
    "        WordPieceTokenizer.\n",
    "\n",
    "        Args:\n",
    "            **never_split**: (`optional`) list of str\n",
    "                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n",
    "                :func:`PreTrainedTokenizer.tokenize`) List of token not to split.\n",
    "        \"\"\"\n",
    "        # union() returns a new set by concatenating the two sets.\n",
    "        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n",
    "        text = self._clean_text(text)\n",
    "\n",
    "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "        # models. This is also applied to the English models now, but it doesn't\n",
    "        # matter since the English models were not trained on any Chinese data\n",
    "        # and generally don't have any Chinese data in them (there are Chinese\n",
    "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "        # words in the English Wikipedia.).\n",
    "        if self.tokenize_chinese_chars:\n",
    "            text = self._tokenize_chinese_chars(text)\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if token not in never_split:\n",
    "                if self.do_lower_case:\n",
    "                    token = token.lower()\n",
    "                    if self.strip_accents is not False:\n",
    "                        token = self._run_strip_accents(token)\n",
    "                elif self.strip_accents:\n",
    "                    token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token, never_split))\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text, never_split=None):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        if never_split is not None and text in never_split:\n",
    "            return [text]\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "        #\n",
    "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "        # space-separated words, so they are not treated specially and handled\n",
    "        # like the all of the other languages.\n",
    "        if (\n",
    "            (cp >= 0x4E00 and cp <= 0x9FFF)\n",
    "            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
    "            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
    "            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
    "            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
    "            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
    "            or (cp >= 0xF900 and cp <= 0xFAFF)\n",
    "            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
    "        ):  #\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n",
    "        tokenization using the given vocabulary.\n",
    "\n",
    "        For example, :obj:`input = \"unaffable\"` wil return as output :obj:`[\"un\", \"##aff\", \"##able\"]`.\n",
    "\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer`.\n",
    "\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:51.699050Z",
     "start_time": "2021-02-10T14:49:51.689930Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" BERT model configuration \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/config.json\",\n",
    "    \"bert-large-uncased\": \"https://huggingface.co/bert-large-uncased/resolve/main/config.json\",\n",
    "    \"bert-base-cased\": \"https://huggingface.co/bert-base-cased/resolve/main/config.json\",\n",
    "    \"bert-large-cased\": \"https://huggingface.co/bert-large-cased/resolve/main/config.json\",\n",
    "    \"bert-base-multilingual-uncased\": \"https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json\",\n",
    "    \"bert-base-multilingual-cased\": \"https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json\",\n",
    "    \"bert-base-chinese\": \"https://huggingface.co/bert-base-chinese/resolve/main/config.json\",\n",
    "    \"bert-base-german-cased\": \"https://huggingface.co/bert-base-german-cased/resolve/main/config.json\",\n",
    "    \"bert-large-uncased-whole-word-masking\": \"https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json\",\n",
    "    \"bert-large-cased-whole-word-masking\": \"https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/config.json\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json\",\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/config.json\",\n",
    "    \"bert-base-cased-finetuned-mrpc\": \"https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/config.json\",\n",
    "    \"bert-base-german-dbmdz-cased\": \"https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/config.json\",\n",
    "    \"bert-base-german-dbmdz-uncased\": \"https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese\": \"https://huggingface.co/cl-tohoku/bert-base-japanese/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-char\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-char/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-char-whole-word-masking/resolve/main/config.json\",\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json\",\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/config.json\",\n",
    "    \"wietsedv/bert-base-dutch-cased\": \"https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/config.json\",\n",
    "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
    "}\n",
    "\n",
    "\n",
    "class BertConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a :class:`~transformers.BertModel` or a\n",
    "    :class:`~transformers.TFBertModel`. It is used to instantiate a BERT model according to the specified arguments,\n",
    "    defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration\n",
    "    to that of the BERT `bert-base-uncased <https://huggingface.co/bert-base-uncased>`__ architecture.\n",
    "\n",
    "    Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model\n",
    "    outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\n",
    "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
    "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\n",
    "            :class:`~transformers.TFBertModel`.\n",
    "        hidden_size (:obj:`int`, `optional`, defaults to 768):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\n",
    "            Number of hidden layers in the Transformer encoder.\n",
    "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\n",
    "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
    "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string,\n",
    "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\n",
    "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
    "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\n",
    "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
    "            just in case (e.g., 512 or 1024 or 2048).\n",
    "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\n",
    "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\n",
    "            :class:`~transformers.TFBertModel`.\n",
    "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\n",
    "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\n",
    "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\n",
    "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\n",
    "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\n",
    "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\n",
    "            <https://arxiv.org/abs/2009.13658>`__.\n",
    "        use_cache (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if ``config.is_decoder=True``.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> from transformers import BertModel, BertConfig\n",
    "\n",
    "        >>> # Initializing a BERT bert-base-uncased style configuration\n",
    "        >>> configuration = BertConfig()\n",
    "\n",
    "        >>> # Initializing a model from the bert-base-uncased style configuration\n",
    "        >>> model = BertModel(configuration)\n",
    "\n",
    "        >>> # Accessing the model configuration\n",
    "        >>> configuration = model.config\n",
    "    \"\"\"\n",
    "    model_type = \"bert\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        gradient_checkpointing=False,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        use_cache=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.use_cache = use_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:55.639108Z",
     "start_time": "2021-02-10T14:49:55.314842Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from packaging import version\n",
    "\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def _gelu_python(x):\n",
    "    \"\"\"\n",
    "    Original Implementation of the GELU activation function in Google BERT repo when initially created. For\n",
    "    information: OpenAI GPT's GELU is slightly different (and gives slightly different results): 0.5 * x * (1 +\n",
    "    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) This is now written in C in\n",
    "    torch.nn.functional Also see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def gelu_new(x):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "\n",
    "if version.parse(torch.__version__) < version.parse(\"1.4\"):\n",
    "    gelu = _gelu_python\n",
    "else:\n",
    "    gelu = F.gelu\n",
    "\n",
    "\n",
    "def gelu_fast(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(x * 0.7978845608 * (1.0 + 0.044715 * x * x)))\n",
    "\n",
    "\n",
    "def _silu_python(x):\n",
    "    \"\"\"\n",
    "    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n",
    "    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n",
    "    Approximation in Reinforcement Learning (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n",
    "    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n",
    "    later.\n",
    "    \"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "if version.parse(torch.__version__) < version.parse(\"1.7\"):\n",
    "    silu = _silu_python\n",
    "else:\n",
    "    silu = F.silu\n",
    "\n",
    "\n",
    "def mish(x):\n",
    "    return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "\n",
    "\n",
    "def linear_act(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "ACT2FN = {\n",
    "    \"relu\": F.relu,\n",
    "    \"silu\": silu,\n",
    "    \"swish\": silu,\n",
    "    \"gelu\": gelu,\n",
    "    \"tanh\": torch.tanh,\n",
    "    \"gelu_new\": gelu_new,\n",
    "    \"gelu_fast\": gelu_fast,\n",
    "    \"mish\": mish,\n",
    "    \"linear\": linear_act,\n",
    "    \"sigmoid\": torch.sigmoid,\n",
    "}\n",
    "\n",
    "\n",
    "def get_activation(activation_string):\n",
    "    if activation_string in ACT2FN:\n",
    "        return ACT2FN[activation_string]\n",
    "    else:\n",
    "        raise KeyError(\"function {} not found in ACT2FN mapping {}\".format(activation_string, list(ACT2FN.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:49:58.765467Z",
     "start_time": "2021-02-10T14:49:58.717586Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "# from .file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs, with potential hidden states and attentions.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithPooling(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that also contains a pooling of the last hidden states.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\n",
    "            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n",
    "            Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n",
    "            prediction (classification) objective during pretraining.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    pooler_output: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithPast(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "\n",
    "            If :obj:`past_key_values` is used only the last hidden-state of the sequences of shape :obj:`(batch_size,\n",
    "            1, hidden_size)` is output.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
    "            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
    "            encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
    "            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithCrossAttentions(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs, with potential hidden states and attentions.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that also contains a pooling of the last hidden states.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\n",
    "            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n",
    "            Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n",
    "            prediction (classification) objective during pretraining.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
    "            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
    "            encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
    "            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    pooler_output: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "\n",
    "            If :obj:`past_key_values` is used only the last hidden-state of the sequences of shape :obj:`(batch_size,\n",
    "            1, hidden_size)` is output.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
    "            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
    "            encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
    "            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential\n",
    "    decoding.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n",
    "\n",
    "            If :obj:`past_key_values` is used only the last hidden-state of the sequences of shape :obj:`(batch_size,\n",
    "            1, hidden_size)` is output.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "            shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
    "        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for causal language model (or autoregressive) outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Language modeling loss (for next-token prediction).\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutputWithPast(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for causal language model (or autoregressive) outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Language modeling loss (for next-token prediction).\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tupel(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutputWithCrossAttentions(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for causal language model (or autoregressive) outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Language modeling loss (for next-token prediction).\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Cross attentions weights after the attention softmax, used to compute the weighted average in the\n",
    "            cross-attention heads.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` tuples of length :obj:`config.n_layers`, with each tuple containing the\n",
    "            cached key, value states of the self-attention and the cross-attention layers if model is used in\n",
    "            encoder-decoder setting. Only relevant if ``config.is_decoder = True``.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SequenceClassifierOutputWithPast(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sentence classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tupel(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MaskedLMOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for masked language models outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Masked language modeling (MLM) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqLMOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for sequence-to-sequence language models outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Language modeling loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "            shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
    "        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NextSentencePredictorOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of models predicting if two sentences are consecutive or not.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`next_sentence_label` is provided):\n",
    "            Next sequence prediction (classification) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SequenceClassifierOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sentence classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqSequenceClassifierOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sequence-to-sequence sentence classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "            shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
    "        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultipleChoiceModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of multiple choice models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape `(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Classification loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_choices)`):\n",
    "            `num_choices` is the second dimension of the input tensors. (see `input_ids` above).\n",
    "\n",
    "            Classification scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenClassifierOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of token classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when ``labels`` is provided) :\n",
    "            Classification loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.num_labels)`):\n",
    "            Classification scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestionAnsweringModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of question answering models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n",
    "        start_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Span-start scores (before SoftMax).\n",
    "        end_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Span-end scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    start_logits: torch.FloatTensor = None\n",
    "    end_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqQuestionAnsweringModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sequence-to-sequence question answering models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n",
    "        start_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Span-start scores (before SoftMax).\n",
    "        end_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Span-end scores (before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "            shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
    "        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    start_logits: torch.FloatTensor = None\n",
    "    end_logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate logits  process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:50:02.708516Z",
     "start_time": "2021-02-10T14:50:02.659260Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import inspect\n",
    "import math\n",
    "from abc import ABC\n",
    "from typing import Callable, Iterable, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# from .file_utils import add_start_docstrings\n",
    "\n",
    "\n",
    "LOGITS_PROCESSOR_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.vocab_size)`):\n",
    "            Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
    "            or scores for each vocabulary token after SoftMax.\n",
    "        kwargs:\n",
    "            Additional logits processor specific kwargs.\n",
    "\n",
    "    Return:\n",
    "        :obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.vocab_size)`: The processed prediction scores.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LogitsProcessor(ABC):\n",
    "    \"\"\"Abstract base class for all logit processors that can be applied during generation.\"\"\"\n",
    "\n",
    "    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Torch method for processing logits.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n",
    "        )\n",
    "\n",
    "\n",
    "class LogitsWarper(ABC):\n",
    "    \"\"\"Abstract base class for all logit warpers that can be applied during generation with multinomial sampling.\"\"\"\n",
    "\n",
    "    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Torch method for warping logits.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n",
    "        )\n",
    "\n",
    "\n",
    "class LogitsProcessorList(list):\n",
    "    \"\"\"\n",
    "    This class can be used to create a list of :class:`~transformers.LogitsProcessor` or\n",
    "    :class:`~transformers.LogitsWarper` to subsequently process a :obj:`scores` input tensor. This class inherits from\n",
    "    list and adds a specific `__call__` method to apply each :class:`~transformers.LogitsProcessor` or\n",
    "    :class:`~transformers.LogitsProcessor` to the inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        for processor in self:\n",
    "            function_args = inspect.signature(processor.__call__).parameters\n",
    "            if len(function_args) > 2:\n",
    "                assert all(\n",
    "                    arg in kwargs for arg in list(function_args.keys())[2:]\n",
    "                ), f\"Make sure that all the required parameters: {list(function_args.keys())} for {processor.__class__} are passed to the logits processor.\"\n",
    "                scores = processor(input_ids, scores, **kwargs)\n",
    "            else:\n",
    "                scores = processor(input_ids, scores)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class MinLengthLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` enforcing a min-length by setting EOS probability to 0.\n",
    "\n",
    "    Args:\n",
    "        min_length (:obj:`int`):\n",
    "            The minimum length below which the score of :obj:`eos_token_id` is set to :obj:`-float(\"Inf\")`.\n",
    "        eos_token_id (:obj:`int`):\n",
    "            The id of the `end-of-sequence` token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_length: int, eos_token_id: int):\n",
    "        if not isinstance(min_length, int) or min_length < 0:\n",
    "            raise ValueError(f\"`min_length` has to be a positive integer, but is {min_length}\")\n",
    "\n",
    "        if not isinstance(eos_token_id, int) or eos_token_id < 0:\n",
    "            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\n",
    "\n",
    "        self.min_length = min_length\n",
    "        self.eos_token_id = eos_token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        if cur_len < self.min_length:\n",
    "            scores[:, self.eos_token_id] = -float(\"inf\")\n",
    "        return scores\n",
    "\n",
    "\n",
    "class TemperatureLogitsWarper(LogitsWarper):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsWarper` for temperature (exponential scaling output probability distribution).\n",
    "\n",
    "    Args:\n",
    "        temperature (:obj:`float`):\n",
    "            The value used to module the logits distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature: float):\n",
    "        if not isinstance(temperature, float) or not (temperature > 0):\n",
    "            raise ValueError(f\"`temperature` has to be a strictly positive float, but is {temperature}\")\n",
    "\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores = scores / self.temperature\n",
    "        return scores\n",
    "\n",
    "\n",
    "class RepetitionPenaltyLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` enforcing an exponential penalty on repeated sequences.\n",
    "\n",
    "    Args:\n",
    "        repetition_penalty (:obj:`float`):\n",
    "            The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
    "            <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, penalty: float):\n",
    "        if not isinstance(penalty, float) or not (penalty > 0):\n",
    "            raise ValueError(f\"`penalty` has to be a strictly positive float, but is {penalty}\")\n",
    "\n",
    "        self.penalty = penalty\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        score = torch.gather(scores, 1, input_ids)\n",
    "\n",
    "        # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability\n",
    "        score = torch.where(score < 0, score * self.penalty, score / self.penalty)\n",
    "\n",
    "        scores.scatter_(1, input_ids, score)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class TopPLogitsWarper(LogitsWarper):\n",
    "    \"\"\"\n",
    "    :class:`transformers.LogitsWarper` that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <=\n",
    "    prob_cut_off.\n",
    "\n",
    "    Args:\n",
    "        top_p (:obj:`float`):\n",
    "            If set to < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or higher are\n",
    "            kept for generation.\n",
    "        filter_value (:obj:`float`, `optional`, defaults to :obj:`-float(\"Inf\")`):\n",
    "            All filtered values will be set to this float value.\n",
    "        min_tokens_to_keep (:obj:`int`, `optional`, defaults to 1):\n",
    "            Minimum number of tokens that cannot be filtered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
    "        if not isinstance(top_p, float) or (top_p < 0 or top_p > 1.0):\n",
    "            raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n",
    "\n",
    "        self.top_p = top_p\n",
    "        self.filter_value = filter_value\n",
    "        self.min_tokens_to_keep = min_tokens_to_keep\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        sorted_logits, sorted_indices = torch.sort(scores, descending=True)\n",
    "        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > self.top_p\n",
    "        if self.min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., : self.min_tokens_to_keep - 1] = 0\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class TopKLogitsWarper(LogitsWarper):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsWarper` that performs top-k, i.e. restricting to the k highest probability elements.\n",
    "\n",
    "    Args:\n",
    "        top_k (:obj:`int`):\n",
    "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "        filter_value (:obj:`float`, `optional`, defaults to :obj:`-float(\"Inf\")`):\n",
    "            All filtered values will be set to this float value.\n",
    "        min_tokens_to_keep (:obj:`int`, `optional`, defaults to 1):\n",
    "            Minimum number of tokens that cannot be filtered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
    "        if not isinstance(top_k, int) or top_k <= 0:\n",
    "            raise ValueError(f\"`top_k` has to be a strictly positive integer, but is {top_k}\")\n",
    "\n",
    "        self.top_k = top_k\n",
    "        self.filter_value = filter_value\n",
    "        self.min_tokens_to_keep = min_tokens_to_keep\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        top_k = min(max(self.top_k, self.min_tokens_to_keep), scores.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n",
    "        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def _get_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx].tolist()\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "    return generated_ngrams\n",
    "\n",
    "\n",
    "def _get_generated_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
    "    # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
    "    start_idx = cur_len + 1 - ngram_size\n",
    "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len].tolist())\n",
    "    return banned_ngrams.get(ngram_idx, [])\n",
    "\n",
    "\n",
    "def _calc_banned_ngram_tokens(\n",
    "    ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int\n",
    ") -> List[Iterable[int]]:\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    if cur_len + 1 < ngram_size:\n",
    "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "\n",
    "    generated_ngrams = _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
    "\n",
    "    banned_tokens = [\n",
    "        _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
    "        for hypo_idx in range(num_hypos)\n",
    "    ]\n",
    "    return banned_tokens\n",
    "\n",
    "\n",
    "class NoRepeatNGramLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces no repetition of n-grams. See `Fairseq\n",
    "    <https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345>`__.\n",
    "\n",
    "    Args:\n",
    "        ngram_size (:obj:`int`):\n",
    "            All ngrams of size :obj:`ngram_size` can only occur once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ngram_size: int):\n",
    "        if not isinstance(ngram_size, int) or ngram_size <= 0:\n",
    "            raise ValueError(f\"`ngram_size` has to be a strictly positive integer, but is {ngram_size}\")\n",
    "        self.ngram_size = ngram_size\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        num_batch_hypotheses = scores.shape[0]\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        banned_batch_tokens = _calc_banned_ngram_tokens(self.ngram_size, input_ids, num_batch_hypotheses, cur_len)\n",
    "\n",
    "        for i, banned_tokens in enumerate(banned_batch_tokens):\n",
    "            scores[i, banned_tokens] = -float(\"inf\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class EncoderNoRepeatNGramLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces no repetition of encoder input ids n-grams for the decoder ids.\n",
    "    See `ParlAI <https://github.com/facebookresearch/ParlAI/blob/master/parlai/core/torch_generator_agent.py#L1350>`__.\n",
    "\n",
    "    Args:\n",
    "        encoder_ngram_size (:obj:`int`):\n",
    "            All ngrams of size :obj:`ngram_size` can only occur within the encoder input ids.\n",
    "        encoder_input_ids (:obj:`int`):\n",
    "            The encoder_input_ids that should not be repeated within the decoder ids.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_ngram_size: int, encoder_input_ids: torch.LongTensor):\n",
    "        if not isinstance(encoder_ngram_size, int) or encoder_ngram_size <= 0:\n",
    "            raise ValueError(\n",
    "                f\"`encoder_ngram_size` has to be a strictly positive integer, but is {encoder_ngram_size}\"\n",
    "            )\n",
    "        self.ngram_size = encoder_ngram_size\n",
    "        if len(encoder_input_ids.shape) == 1:\n",
    "            encoder_input_ids = encoder_input_ids.unsqueeze(0)\n",
    "        self.batch_size = encoder_input_ids.shape[0]\n",
    "        self.generated_ngrams = _get_ngrams(encoder_ngram_size, encoder_input_ids, self.batch_size)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # B x num_beams\n",
    "        num_hypos = scores.shape[0]\n",
    "        num_beams = num_hypos // self.batch_size\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        banned_batch_tokens = [\n",
    "            _get_generated_ngrams(\n",
    "                self.generated_ngrams[hypo_idx // num_beams], input_ids[hypo_idx], self.ngram_size, cur_len\n",
    "            )\n",
    "            for hypo_idx in range(num_hypos)\n",
    "        ]\n",
    "\n",
    "        for i, banned_tokens in enumerate(banned_batch_tokens):\n",
    "            scores[i, banned_tokens] = -float(\"inf\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class NoBadWordsLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces that specified sequences will never be sampled.\n",
    "\n",
    "    Args:\n",
    "        bad_words_ids (:obj:`List[List[int]]`):\n",
    "            List of list of token ids that are not allowed to be generated. In order to get the tokens of the words\n",
    "            that should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
    "            add_prefix_space=True).input_ids`.\n",
    "        eos_token_id (:obj:`int`):\n",
    "            The id of the `end-of-sequence` token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bad_words_ids: Iterable[Iterable[int]], eos_token_id: int):\n",
    "\n",
    "        if not isinstance(bad_words_ids, List) or len(bad_words_ids) == 0:\n",
    "            raise ValueError(f\"`bad_words_ids` has to be a non-emtpy list, but is {bad_words_ids}.\")\n",
    "        if any(not isinstance(bad_word_ids, list) for bad_word_ids in bad_words_ids):\n",
    "            raise ValueError(f\"`bad_words_ids` has to be a list of lists, but is {bad_words_ids}.\")\n",
    "        if any(\n",
    "            any((not isinstance(token_id, (int, np.integer)) or token_id < 0) for token_id in bad_word_ids)\n",
    "            for bad_word_ids in bad_words_ids\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Each list in `bad_words_ids` has to be a list of positive integers, but is {bad_words_ids}.\"\n",
    "            )\n",
    "\n",
    "        self.bad_words_ids = list(filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], bad_words_ids))\n",
    "\n",
    "        for banned_token_seq in self.bad_words_ids:\n",
    "            assert len(banned_token_seq) > 0, \"Banned words token sequences {} cannot have an empty list\".format(\n",
    "                bad_words_ids\n",
    "            )\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        banned_tokens = self._calc_banned_bad_words_ids(input_ids)\n",
    "        scores = self._set_scores_to_inf_for_banned_tokens(scores, banned_tokens)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def _tokens_match(self, prev_tokens: torch.LongTensor, tokens: List[int]) -> bool:\n",
    "        if len(tokens) == 0:\n",
    "            # if bad word tokens is just one token always ban it\n",
    "            return True\n",
    "        elif len(tokens) > len(prev_tokens):\n",
    "            # if bad word tokens are longer then prev input_ids they can't be equal\n",
    "            return False\n",
    "        elif prev_tokens[-len(tokens) :].tolist() == tokens:\n",
    "            # if tokens match\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _calc_banned_bad_words_ids(self, prev_input_ids: Iterable[int]) -> Iterable[int]:\n",
    "        banned_tokens = []\n",
    "        for prev_input_ids_slice in prev_input_ids:\n",
    "            banned_tokens_slice = []\n",
    "            for banned_token_seq in self.bad_words_ids:\n",
    "                if self._tokens_match(prev_input_ids_slice, banned_token_seq[:-1]) is False:\n",
    "                    # if tokens do not match continue\n",
    "                    continue\n",
    "\n",
    "                banned_tokens_slice.append(banned_token_seq[-1])\n",
    "\n",
    "            banned_tokens.append(banned_tokens_slice)\n",
    "\n",
    "        return banned_tokens\n",
    "\n",
    "    def _set_scores_to_inf_for_banned_tokens(self, scores: torch.Tensor, banned_tokens: List[List[int]]) -> None:\n",
    "        \"\"\"\n",
    "        Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be a\n",
    "        list of list of banned tokens to ban in the format [[batch index, vocabulary position],...\n",
    "\n",
    "        Args:\n",
    "            scores: logits distribution of shape (batch size, vocabulary size)\n",
    "            banned_tokens: list of list of tokens to ban of length (batch_size)\n",
    "        \"\"\"\n",
    "        banned_mask_list = []\n",
    "        for idx, batch_banned_tokens in enumerate(banned_tokens):\n",
    "            for token in batch_banned_tokens:\n",
    "                banned_mask_list.append([idx, token])\n",
    "        if not banned_mask_list:\n",
    "            return scores\n",
    "\n",
    "        banned_mask = torch.LongTensor(banned_mask_list)\n",
    "        indices = torch.ones(len(banned_mask))\n",
    "        # A sparse tensor is generated from a list of coordinates: [[0, 1], [0, 2], [2, 0]]. A conversion to dense tensor generates:\n",
    "        # [ 0  1  1 ]\n",
    "        # [ 0  0  0 ]\n",
    "        # [ 1  0  0 ]\n",
    "\n",
    "        banned_mask = (\n",
    "            torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n",
    "        )\n",
    "        scores = scores.masked_fill(banned_mask, -float(\"inf\"))\n",
    "        return scores\n",
    "\n",
    "\n",
    "class PrefixConstrainedLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces contrained generation and is useful for prefix-conditioned\n",
    "    constrained generation. See `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__ for more\n",
    "    information.\n",
    "\n",
    "    Args:\n",
    "        prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`):\n",
    "            This function constraints the beam search to allowed tokens only at each step. This function takes 2\n",
    "            arguments :obj:`inputs_ids` and the batch ID :obj:`batch_id`. It has to return a list with the allowed\n",
    "            tokens for the next generation step conditioned on the previously generated tokens :obj:`inputs_ids` and\n",
    "            the batch ID :obj:`batch_id`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]], num_beams: int):\n",
    "        self._prefix_allowed_tokens_fn = prefix_allowed_tokens_fn\n",
    "        self._num_beams = num_beams\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        mask = torch.full_like(scores, -math.inf)\n",
    "        for batch_id, beam_sent in enumerate(input_ids.view(-1, self._num_beams, input_ids.shape[-1])):\n",
    "            for beam_id, sent in enumerate(beam_sent):\n",
    "                mask[batch_id * self._num_beams + beam_id, self._prefix_allowed_tokens_fn(batch_id, sent)] = 0\n",
    "\n",
    "        return scores + mask\n",
    "\n",
    "\n",
    "class HammingDiversityLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces diverse beam search. Note that this logits processor is only\n",
    "    effective for :meth:`transformers.PretrainedModel.group_beam_search`. See `Diverse Beam Search: Decoding Diverse\n",
    "    Solutions from Neural Sequence Models <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "\n",
    "    Args:\n",
    "        diversity_penalty (:obj:`float`):\n",
    "            This value is subtracted from a beam's score if it generates a token same as any beam from other group at a\n",
    "            particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is enabled.\n",
    "        num_beams (:obj:`int`):\n",
    "            Number of beams used for group beam search. See `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for\n",
    "            more details.\n",
    "        num_beam_groups (:obj:`int`):\n",
    "            Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
    "            beams. See `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, diversity_penalty: float, num_beams: int, num_beam_groups: int):\n",
    "        if not isinstance(diversity_penalty, float) or (not diversity_penalty > 0.0):\n",
    "            raise ValueError(\"`diversity_penalty` should be a float strictly larger than 0.\")\n",
    "        self._diversity_penalty = diversity_penalty\n",
    "        if not isinstance(num_beams, int) or num_beams < 2:\n",
    "            raise ValueError(\"`num_beams` should be an integer strictly larger than 1.\")\n",
    "        self._num_beams = num_beams\n",
    "        if not isinstance(num_beam_groups, int) or num_beam_groups < 2:\n",
    "            raise ValueError(\"`num_beam_groups` should be an integer strictly larger than 1.\")\n",
    "        if num_beam_groups > num_beams:\n",
    "            raise ValueError(\"`beam_groups` has to be smaller or equal to `num_beams`.\")\n",
    "        self._num_sub_beams = num_beams // num_beam_groups\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        scores: torch.FloatTensor,\n",
    "        current_tokens: torch.LongTensor,\n",
    "        beam_group_idx: int,\n",
    "    ) -> torch.FloatTensor:\n",
    "        # hamming diversity: penalise using same token in current group which was used in previous groups at\n",
    "        # the same time step\n",
    "        batch_size = current_tokens.shape[0] // self._num_beams\n",
    "        group_start_idx = beam_group_idx * self._num_sub_beams\n",
    "        group_end_idx = min(group_start_idx + self._num_sub_beams, self._num_beams)\n",
    "        group_size = group_end_idx - group_start_idx\n",
    "        vocab_size = scores.shape[-1]\n",
    "\n",
    "        if group_start_idx == 0:\n",
    "            return scores\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            # predicted tokens of last time step of previous groups\n",
    "            previous_group_tokens = current_tokens[\n",
    "                batch_idx * self._num_beams : batch_idx * self._num_beams + group_start_idx\n",
    "            ]\n",
    "            token_frequency = torch.bincount(previous_group_tokens, minlength=vocab_size).to(scores.device)\n",
    "            scores[batch_idx * group_size : (batch_idx + 1) * group_size] -= self._diversity_penalty * token_frequency\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:50:06.281469Z",
     "start_time": "2021-02-10T14:50:06.249165Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import UserDict\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "# from .file_utils import add_start_docstrings\n",
    "\n",
    "\n",
    "PROCESS_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size * num_beams, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using any class inheriting from :class:`~transformers.PretrainedTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        next_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n",
    "            Current scores of the top :obj:`2 * num_beams` non-finished beam hypotheses.\n",
    "        next_tokens (:obj:`torch.LongTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n",
    "            :obj:`input_ids` of the tokens corresponding to the top :obj:`2 * num_beams` non-finished beam hypotheses.\n",
    "        next_indices (:obj:`torch.LongTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n",
    "            Beam indices indicating to which beam hypothesis the :obj:`next_tokens` correspond.\n",
    "        pad_token_id (:obj:`int`, `optional`):\n",
    "            The id of the `padding` token.\n",
    "        eos_token_id (:obj:`int`, `optional`):\n",
    "            The id of the `end-of-sequence` token.\n",
    "\n",
    "    Return:\n",
    "        :obj:`UserDict`: A dictionary composed of the fields as defined above:\n",
    "\n",
    "            - **next_beam_scores** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Updated\n",
    "              scores of all non-finished beams.\n",
    "            - **next_beam_tokens** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Next tokens\n",
    "              to be added to the non-finished beam_hypotheses.\n",
    "            - **next_beam_indices** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Beam indices\n",
    "              indicating to which beam the next tokens shall be added.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "FINALIZE_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size * num_beams, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using any class inheriting from :class:`~transformers.PretrainedTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        final_beam_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n",
    "            The final scores of all non-finished beams.\n",
    "        final_beam_tokens (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n",
    "            The last tokens to be added to the non-finished beam_hypotheses.\n",
    "        final_beam_indices (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n",
    "            The beam indices indicating to which beam the :obj:`final_beam_tokens` shall be added.\n",
    "        pad_token_id (:obj:`int`, `optional`):\n",
    "            The id of the `padding` token.\n",
    "        eos_token_id (:obj:`int`, `optional`):\n",
    "            The id of the `end-of-sequence` token.\n",
    "\n",
    "    Return:\n",
    "        :obj:`torch.LongTensor` of shape :obj:`(batch_size * num_return_sequences, sequence_length)`: The generated\n",
    "        sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or shorter if all\n",
    "        batches finished early due to the :obj:`eos_token_id`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BeamScorer(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all beam scorers that are used for :meth:`~transformers.PretrainedModel.beam_search` and\n",
    "    :meth:`~transformers.PretrainedModel.beam_sample`.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    @add_start_docstrings(PROCESS_INPUTS_DOCSTRING)\n",
    "    def process(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        **kwargs\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        raise NotImplementedError(\"This is an abstract method.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    @add_start_docstrings(FINALIZE_INPUTS_DOCSTRING)\n",
    "    def finalize(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        **kwargs\n",
    "    ) -> torch.LongTensor:\n",
    "        raise NotImplementedError(\"This is an abstract method.\")\n",
    "\n",
    "\n",
    "class BeamSearchScorer(BeamScorer):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.BeamScorer` implementing standard beam search decoding.\n",
    "\n",
    "    Adapted in part from `Facebook's XLM beam search code\n",
    "    <https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529>`__.\n",
    "\n",
    "    Reference for the diverse beam search algorithm and implementation `Ashwin Kalyan's DBS implementation\n",
    "    <https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua>`__\n",
    "\n",
    "    Args:\n",
    "        batch_size (:obj:`int`):\n",
    "            Batch Size of :obj:`input_ids` for which standard beam search decoding is run in parallel.\n",
    "        max_length (:obj:`int`):\n",
    "            The maximum length of the sequence to be generated.\n",
    "        num_beams (:obj:`int`):\n",
    "            Number of beams for beam search.\n",
    "        device (:obj:`torch.device`):\n",
    "            Defines the device type (*e.g.*, :obj:`\"cpu\"` or :obj:`\"cuda\"`) on which this instance of\n",
    "            :obj:`BeamSearchScorer` will be allocated.\n",
    "        length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "            Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
    "            model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
    "            sequences.\n",
    "        do_early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
    "        num_beam_hyps_to_keep (:obj:`int`, `optional`, defaults to 1):\n",
    "            The number of beam hypotheses that shall be returned upon calling\n",
    "            :meth:`~transformer.BeamSearchScorer.finalize`.\n",
    "        num_beam_groups (:obj:`int`):\n",
    "            Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
    "            beams. See `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        max_length: int,\n",
    "        num_beams: int,\n",
    "        device: torch.device,\n",
    "        length_penalty: Optional[float] = 1.0,\n",
    "        do_early_stopping: Optional[bool] = False,\n",
    "        num_beam_hyps_to_keep: Optional[int] = 1,\n",
    "        num_beam_groups: Optional[int] = 1,\n",
    "    ):\n",
    "        self.max_length = max_length\n",
    "        self.num_beams = num_beams\n",
    "        self.device = device\n",
    "        self.length_penalty = length_penalty\n",
    "        self.do_early_stopping = do_early_stopping\n",
    "        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n",
    "        self.num_beam_groups = num_beam_groups\n",
    "        self.group_size = self.num_beams // self.num_beam_groups\n",
    "\n",
    "        self._is_init = False\n",
    "        self._beam_hyps = [\n",
    "            BeamHypotheses(\n",
    "                num_beams=self.num_beams,\n",
    "                max_length=self.max_length,\n",
    "                length_penalty=self.length_penalty,\n",
    "                early_stopping=self.do_early_stopping,\n",
    "            )\n",
    "            for _ in range(batch_size)\n",
    "        ]\n",
    "        self._done = torch.tensor([False for _ in range(batch_size)], dtype=torch.bool, device=self.device)\n",
    "\n",
    "        if not isinstance(num_beams, int) or num_beams <= 1:\n",
    "            raise ValueError(\n",
    "                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1, one should make use of `greedy_search` instead.\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\n",
    "            raise ValueError(\n",
    "                f\"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` \"\n",
    "                f\"has to be divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def is_done(self) -> bool:\n",
    "        return self._done.all()\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        batch_size = len(self._beam_hyps)\n",
    "        assert batch_size == (input_ids.shape[0] // self.group_size)\n",
    "\n",
    "        device = input_ids.device\n",
    "        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n",
    "        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n",
    "        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n",
    "\n",
    "        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n",
    "            if self._done[batch_idx]:\n",
    "                assert (\n",
    "                    len(beam_hyp) >= self.num_beams\n",
    "                ), \"Batch can only be done if at least {} beams have been generated\".format(self.num_beams)\n",
    "                assert (\n",
    "                    eos_token_id is not None and pad_token_id is not None\n",
    "                ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\n",
    "                # pad the batch\n",
    "                next_beam_scores[batch_idx, :] = 0\n",
    "                next_beam_tokens[batch_idx, :] = pad_token_id\n",
    "                next_beam_indices[batch_idx, :] = 0\n",
    "                continue\n",
    "\n",
    "            # next tokens for this sentence\n",
    "            beam_idx = 0\n",
    "            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n",
    "                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n",
    "            ):\n",
    "                batch_beam_idx = batch_idx * self.group_size + next_index\n",
    "                # add to generated hypotheses if end of sentence\n",
    "                if (eos_token_id is not None) and (next_token.item() == eos_token_id):\n",
    "                    # if beam_token does not belong to top num_beams tokens, it should not be added\n",
    "                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n",
    "                    if is_beam_token_worse_than_top_num_beams:\n",
    "                        continue\n",
    "                    beam_hyp.add(\n",
    "                        input_ids[batch_beam_idx].clone(),\n",
    "                        next_score.item(),\n",
    "                    )\n",
    "                else:\n",
    "                    # add next predicted token since it is not eos_token\n",
    "                    next_beam_scores[batch_idx, beam_idx] = next_score\n",
    "                    next_beam_tokens[batch_idx, beam_idx] = next_token\n",
    "                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n",
    "                    beam_idx += 1\n",
    "\n",
    "                # once the beam for next step is full, don't add more tokens to it.\n",
    "                if beam_idx == self.group_size:\n",
    "                    break\n",
    "\n",
    "            if beam_idx < self.group_size:\n",
    "                raise ValueError(\n",
    "                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id: {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n",
    "                )\n",
    "\n",
    "            # Check if we are done so that we can save a pad step if all(done)\n",
    "            self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(\n",
    "                next_scores[batch_idx].max().item(), cur_len\n",
    "            )\n",
    "\n",
    "        return UserDict(\n",
    "            {\n",
    "                \"next_beam_scores\": next_beam_scores.view(-1),\n",
    "                \"next_beam_tokens\": next_beam_tokens.view(-1),\n",
    "                \"next_beam_indices\": next_beam_indices.view(-1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def finalize(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        final_beam_scores: torch.FloatTensor,\n",
    "        final_beam_tokens: torch.LongTensor,\n",
    "        final_beam_indices: torch.LongTensor,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "    ) -> Tuple[torch.LongTensor]:\n",
    "        batch_size = len(self._beam_hyps)\n",
    "\n",
    "        # finalize all open beam hypotheses and add to generated hypotheses\n",
    "        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n",
    "            if self._done[batch_idx]:\n",
    "                continue\n",
    "\n",
    "            # all open beam hypotheses are added to the beam hypothesis\n",
    "            # beam hypothesis class automatically keeps the best beams\n",
    "            for beam_id in range(self.num_beams):\n",
    "                batch_beam_idx = batch_idx * self.num_beams + beam_id\n",
    "                final_score = final_beam_scores[batch_beam_idx].item()\n",
    "                final_tokens = input_ids[batch_beam_idx]\n",
    "                beam_hyp.add(final_tokens, final_score)\n",
    "\n",
    "        # select the best hypotheses\n",
    "        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n",
    "        best = []\n",
    "        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        # retrieve best hypotheses\n",
    "        for i, beam_hyp in enumerate(self._beam_hyps):\n",
    "            sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])\n",
    "            for j in range(self.num_beam_hyps_to_keep):\n",
    "                best_hyp_tuple = sorted_hyps.pop()\n",
    "                best_score = best_hyp_tuple[0]\n",
    "                best_hyp = best_hyp_tuple[1]\n",
    "                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n",
    "\n",
    "                # append to lists\n",
    "                best.append(best_hyp)\n",
    "                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n",
    "\n",
    "        # prepare for adding eos\n",
    "        sent_max_len = min(sent_lengths.max().item() + 1, self.max_length)\n",
    "        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
    "        # shorter batches are padded if needed\n",
    "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
    "            assert pad_token_id is not None, \"`pad_token_id` has to be defined\"\n",
    "            decoded.fill_(pad_token_id)\n",
    "\n",
    "        # fill with hypotheses and eos_token_id if the latter fits in\n",
    "        for i, hypo in enumerate(best):\n",
    "            decoded[i, : sent_lengths[i]] = hypo\n",
    "            if sent_lengths[i] < self.max_length:\n",
    "                decoded[i, sent_lengths[i]] = eos_token_id\n",
    "        return UserDict(\n",
    "            {\n",
    "                \"sequences\": decoded,\n",
    "                \"sequence_scores\": best_scores,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "class BeamHypotheses:\n",
    "    def __init__(self, num_beams: int, max_length: int, length_penalty: float, early_stopping: bool):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length - 1  # ignoring bos_token\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.num_beams = num_beams\n",
    "        self.beams = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.beams)\n",
    "\n",
    "    def add(self, hyp: torch.LongTensor, sum_logprobs: float):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)\n",
    "        if len(self) < self.num_beams or score > self.worst_score:\n",
    "            self.beams.append((score, hyp))\n",
    "            if len(self) > self.num_beams:\n",
    "                sorted_next_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.beams)])\n",
    "                del self.beams[sorted_next_scores[0][1]]\n",
    "                self.worst_score = sorted_next_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs: float, cur_len: int) -> bool:\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst\n",
    "        one in the heap, then we are done with this sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self) < self.num_beams:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            cur_score = best_sum_logprobs / cur_len ** self.length_penalty\n",
    "            ret = self.worst_score >= cur_score\n",
    "            return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:50:11.315681Z",
     "start_time": "2021-02-10T14:50:11.135974Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from .file_utils import ModelOutput\n",
    "# from .generation_beam_search import BeamScorer, BeamSearchScorer\n",
    "# from .generation_logits_process import (\n",
    "#     EncoderNoRepeatNGramLogitsProcessor,\n",
    "#     HammingDiversityLogitsProcessor,\n",
    "#     LogitsProcessorList,\n",
    "#     MinLengthLogitsProcessor,\n",
    "#     NoBadWordsLogitsProcessor,\n",
    "#     NoRepeatNGramLogitsProcessor,\n",
    "#     PrefixConstrainedLogitsProcessor,\n",
    "#     RepetitionPenaltyLogitsProcessor,\n",
    "#     TemperatureLogitsWarper,\n",
    "#     TopKLogitsWarper,\n",
    "#     TopPLogitsWarper,\n",
    "# )\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of\n",
    "            shape :obj:`(batch_size, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchEncoderDecoderOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of encoder-decoder generation models using greedy search. Hidden states and attention\n",
    "    weights of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the\n",
    "    encoder_hidden_states attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of\n",
    "            shape :obj:`(batch_size, config.vocab_size)`).\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,\n",
    "            num_heads, sequence_length, sequence_length)`.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using sampling.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of\n",
    "            shape :obj:`(batch_size*num_return_sequences, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(num_return_sequences*batch_size, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(num_return_sequences*batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleEncoderDecoderOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of encoder-decoder generation models using sampling. Hidden states and attention weights of\n",
    "    the decoder (respectively the encoder) can be accessed via the encoder_attentions and the encoder_hidden_states\n",
    "    attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of\n",
    "            shape :obj:`(batch_size*num_return_sequences, config.vocab_size)`).\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape\n",
    "            :obj:`(batch_size*num_return_sequences, num_heads, sequence_length, sequence_length)`.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size*num_return_sequences, sequence_length, hidden_size)`.\n",
    "        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamSearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using beam search.\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Final beam scores of the generated ``sequences``.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log\n",
    "            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam\n",
    "            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape\n",
    "            :obj:`(batch_size*num_beams*num_return_sequences, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, generated_length,\n",
    "            hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    sequences_scores: Optional[torch.FloatTensor] = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamSearchEncoderDecoderOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of encoder-decoder generation models using beam search. Hidden states and attention weights\n",
    "    of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the encoder_hidden_states\n",
    "    attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Final beam scores of the generated ``sequences``.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log\n",
    "            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam\n",
    "            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape\n",
    "            :obj:`(batch_size*num_beams, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,\n",
    "            num_heads, sequence_length, sequence_length)`.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.\n",
    "        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, num_heads,\n",
    "            generated_length, sequence_length)`.\n",
    "        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, generated_length,\n",
    "            hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    sequences_scores: Optional[torch.FloatTensor] = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamSampleDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using beam sample.\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_return_sequence)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Final beam scores of the generated ``sequences``.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log\n",
    "            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam\n",
    "            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape\n",
    "            :obj:`(batch_size*num_beams*num_return_sequences, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    sequences_scores: Optional[torch.FloatTensor] = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamSampleEncoderDecoderOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of encoder-decoder generation models using beam sampling. Hidden states and attention\n",
    "    weights of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the\n",
    "    encoder_hidden_states attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_beams, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_return_sequence)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Final beam scores of the generated ``sequences``.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log\n",
    "            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam\n",
    "            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape\n",
    "            :obj:`(batch_size*num_beams, config.vocab_size)`).\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,\n",
    "            num_heads, sequence_length, sequence_length)`.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size*num_beams, sequence_length, hidden_size)`.\n",
    "        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    sequences_scores: Optional[torch.FloatTensor] = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "GreedySearchOutput = Union[GreedySearchEncoderDecoderOutput, GreedySearchDecoderOnlyOutput]\n",
    "SampleOutput = Union[SampleEncoderDecoderOutput, SampleDecoderOnlyOutput]\n",
    "BeamSearchOutput = Union[BeamSearchEncoderDecoderOutput, BeamSearchDecoderOnlyOutput]\n",
    "BeamSampleOutput = Union[BeamSampleEncoderDecoderOutput, BeamSampleDecoderOnlyOutput]\n",
    "\n",
    "\n",
    "class GenerationMixin:\n",
    "    \"\"\"\n",
    "    A class containing all of the functions supporting generation, to be used as a mixin in\n",
    "    :class:`~transformers.PreTrainedModel`.\n",
    "    \"\"\"\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids: torch.LongTensor, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to prepare inputs in the\n",
    "        generate method.\n",
    "        \"\"\"\n",
    "        return {\"input_ids\": input_ids}\n",
    "\n",
    "    def adjust_logits_during_generation(self, logits: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to adjust the logits in\n",
    "        the generate method.\n",
    "        \"\"\"\n",
    "        return logits\n",
    "\n",
    "    def _prepare_input_ids_for_generation(self, bos_token_id: int) -> torch.LongTensor:\n",
    "        if bos_token_id is None:\n",
    "            raise ValueError(\"`bos_token_id` has to be defined when no `input_ids` are provided.\")\n",
    "        return torch.ones((1, 1), dtype=torch.long, device=self.device) * bos_token_id\n",
    "\n",
    "    def _prepare_attention_mask_for_generation(\n",
    "        self, input_ids: torch.Tensor, pad_token_id: int, eos_token_id: int\n",
    "    ) -> torch.LongTensor:\n",
    "        is_pad_token_in_inputs_ids = (pad_token_id is not None) and (pad_token_id in input_ids)\n",
    "        is_pad_token_not_equal_to_eos_token_id = (eos_token_id is None) or (\n",
    "            (eos_token_id is not None) and (pad_token_id != eos_token_id)\n",
    "        )\n",
    "        if is_pad_token_in_inputs_ids and is_pad_token_not_equal_to_eos_token_id:\n",
    "            return input_ids.ne(pad_token_id).long()\n",
    "        return input_ids.new_ones(input_ids.shape)\n",
    "\n",
    "    def _prepare_encoder_decoder_kwargs_for_generation(\n",
    "        self, input_ids: torch.LongTensor, model_kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        # retrieve encoder hidden states\n",
    "        encoder = self.get_encoder()\n",
    "        encoder_kwargs = {\n",
    "            argument: value for argument, value in model_kwargs.items() if not argument.startswith(\"decoder_\")\n",
    "        }\n",
    "        model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(input_ids, return_dict=True, **encoder_kwargs)\n",
    "        return model_kwargs\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(\n",
    "        self, input_ids: torch.LongTensor, decoder_start_token_id: int = None, bos_token_id: int = None\n",
    "    ) -> torch.LongTensor:\n",
    "        decoder_start_token_id = self._get_decoder_start_token_id(decoder_start_token_id, bos_token_id)\n",
    "        decoder_input_ids = (\n",
    "            torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)\n",
    "            * decoder_start_token_id\n",
    "        )\n",
    "        return decoder_input_ids\n",
    "\n",
    "    def _get_pad_token_id(self, pad_token_id: int = None, eos_token_id: int = None) -> int:\n",
    "        if pad_token_id is None and eos_token_id is not None:\n",
    "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "            pad_token_id = eos_token_id\n",
    "        return pad_token_id\n",
    "\n",
    "    def _get_decoder_start_token_id(self, decoder_start_token_id: int = None, bos_token_id: int = None) -> int:\n",
    "        decoder_start_token_id = (\n",
    "            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id\n",
    "        )\n",
    "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "\n",
    "        if decoder_start_token_id is not None:\n",
    "            return decoder_start_token_id\n",
    "        elif (\n",
    "            hasattr(self.config, \"decoder\")\n",
    "            and hasattr(self.config.decoder, \"decoder_start_token_id\")\n",
    "            and self.config.decoder.decoder_start_token_id is not None\n",
    "        ):\n",
    "            return self.config.decoder.decoder_start_token_id\n",
    "        elif bos_token_id is not None:\n",
    "            return bos_token_id\n",
    "        elif (\n",
    "            hasattr(self.config, \"decoder\")\n",
    "            and hasattr(self.config.decoder, \"bos_token_id\")\n",
    "            and self.config.decoder.bos_token_id is not None\n",
    "        ):\n",
    "            return self.config.decoder.bos_token_id\n",
    "        raise ValueError(\n",
    "            \"`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _expand_inputs_for_generation(\n",
    "        input_ids: torch.LongTensor,\n",
    "        expand_size: int = 1,\n",
    "        is_encoder_decoder: bool = False,\n",
    "        attention_mask: torch.LongTensor = None,\n",
    "        encoder_outputs: ModelOutput = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:\n",
    "        expanded_return_idx = (\n",
    "            torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(input_ids.device)\n",
    "        )\n",
    "        input_ids = input_ids.index_select(0, expanded_return_idx)\n",
    "\n",
    "        if \"token_type_ids\" in model_kwargs:\n",
    "            token_type_ids = model_kwargs[\"token_type_ids\"]\n",
    "            model_kwargs[\"token_type_ids\"] = token_type_ids.index_select(0, expanded_return_idx)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            model_kwargs[\"attention_mask\"] = attention_mask.index_select(0, expanded_return_idx)\n",
    "\n",
    "        if is_encoder_decoder:\n",
    "            assert encoder_outputs is not None\n",
    "            encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "                0, expanded_return_idx.to(encoder_outputs.last_hidden_state.device)\n",
    "            )\n",
    "            model_kwargs[\"encoder_outputs\"] = encoder_outputs\n",
    "        return input_ids, model_kwargs\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_sequence_length_for_generation(\n",
    "        input_ids: torch.LongTensor, max_length: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "        sequence_lengths = input_ids.new(input_ids.shape[0]).fill_(max_length)\n",
    "\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        return sequence_lengths, unfinished_sequences, cur_len\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_seq_length_for_generation(\n",
    "        sequence_lengths: torch.LongTensor,\n",
    "        unfinished_sequences: torch.LongTensor,\n",
    "        cur_len: int,\n",
    "        is_eos_in_next_token: torch.BoolTensor,\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        # check if sentence is not finished yet\n",
    "        is_sent_unfinished = unfinished_sequences.mul(is_eos_in_next_token.long()).bool()\n",
    "\n",
    "        # update sentence length\n",
    "        sequence_lengths = sequence_lengths.masked_fill(is_sent_unfinished, cur_len)\n",
    "        unfinished_sequences = unfinished_sequences.mul((~is_eos_in_next_token).long())\n",
    "        return sequence_lengths, unfinished_sequences\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_model_kwargs_for_generation(\n",
    "        outputs: ModelOutput, model_kwargs: Dict[str, Any], is_encoder_decoder: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        # update past\n",
    "        if \"past_key_values\" in outputs:\n",
    "            model_kwargs[\"past\"] = outputs.past_key_values\n",
    "        elif \"mems\" in outputs:\n",
    "            model_kwargs[\"past\"] = outputs.mems\n",
    "        elif \"past_buckets_states\" in outputs:\n",
    "            model_kwargs[\"past\"] = outputs.past_buckets_states\n",
    "        else:\n",
    "            model_kwargs[\"past\"] = None\n",
    "\n",
    "        # update token_type_ids with last value\n",
    "        if \"token_type_ids\" in model_kwargs:\n",
    "            token_type_ids = model_kwargs[\"token_type_ids\"]\n",
    "            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        # update attention mask\n",
    "        if not is_encoder_decoder:\n",
    "            if \"attention_mask\" in model_kwargs:\n",
    "                attention_mask = model_kwargs[\"attention_mask\"]\n",
    "                model_kwargs[\"attention_mask\"] = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
    "                )\n",
    "\n",
    "        return model_kwargs\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        raise NotImplementedError(\n",
    "            f\"Make sure that a `_reorder_cache` function is correctly implemented in {self.__class__.__module__} to enable beam search for {self.__class__}\"\n",
    "        )\n",
    "\n",
    "    def _get_logits_warper(\n",
    "        self, top_k: int = None, top_p: float = None, temperature: float = None, num_beams: int = None\n",
    "    ) -> LogitsProcessorList:\n",
    "        \"\"\"\n",
    "        This class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant\n",
    "        :obj:`~transformers.LogitsWarper` instances used for multinomial sampling.\n",
    "        \"\"\"\n",
    "\n",
    "        # init warp parameters\n",
    "        top_k = top_k if top_k is not None else self.config.top_k\n",
    "        top_p = top_p if top_p is not None else self.config.top_p\n",
    "        temperature = temperature if temperature is not None else self.config.temperature\n",
    "        # instantiate warpers list\n",
    "        warpers = LogitsProcessorList()\n",
    "\n",
    "        # the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files\n",
    "        # all samplers can be found in `generation_utils_samplers.py`\n",
    "        if temperature is not None and temperature != 1.0:\n",
    "            warpers.append(TemperatureLogitsWarper(temperature))\n",
    "        if top_k is not None and top_k != 0:\n",
    "            warpers.append(TopKLogitsWarper(top_k=top_k, min_tokens_to_keep=(2 if num_beams > 1 else 1)))\n",
    "        if top_p is not None and top_p < 1.0:\n",
    "            warpers.append(TopPLogitsWarper(top_p=top_p, min_tokens_to_keep=(2 if num_beams > 1 else 1)))\n",
    "        return warpers\n",
    "\n",
    "    def _get_logits_processor(\n",
    "        self,\n",
    "        repetition_penalty: float,\n",
    "        no_repeat_ngram_size: int,\n",
    "        encoder_no_repeat_ngram_size: int,\n",
    "        encoder_input_ids: torch.LongTensor,\n",
    "        bad_words_ids: List[List[int]],\n",
    "        min_length: int,\n",
    "        eos_token_id: int,\n",
    "        prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]],\n",
    "        num_beams: int,\n",
    "        num_beam_groups: int,\n",
    "        diversity_penalty: float,\n",
    "    ) -> LogitsProcessorList:\n",
    "        \"\"\"\n",
    "        This class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant\n",
    "        :obj:`~transformers.LogitsProcessor` instances used to modify the scores of the language model head.\n",
    "        \"\"\"\n",
    "\n",
    "        # init warp parameters\n",
    "        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n",
    "        no_repeat_ngram_size = (\n",
    "            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n",
    "        )\n",
    "        encoder_no_repeat_ngram_size = (\n",
    "            encoder_no_repeat_ngram_size\n",
    "            if encoder_no_repeat_ngram_size is not None\n",
    "            else self.config.encoder_no_repeat_ngram_size\n",
    "        )\n",
    "        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\n",
    "        min_length = min_length if min_length is not None else self.config.min_length\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        diversity_penalty = diversity_penalty if diversity_penalty is not None else self.config.diversity_penalty\n",
    "        # instantiate processors list\n",
    "        processors = LogitsProcessorList()\n",
    "\n",
    "        # the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files\n",
    "        # all samplers can be found in `generation_utils_samplers.py`\n",
    "        if diversity_penalty is not None and diversity_penalty > 0.0:\n",
    "            processors.append(\n",
    "                HammingDiversityLogitsProcessor(\n",
    "                    diversity_penalty=diversity_penalty, num_beams=num_beams, num_beam_groups=num_beam_groups\n",
    "                )\n",
    "            )\n",
    "        if repetition_penalty is not None and repetition_penalty != 1.0:\n",
    "            processors.append(RepetitionPenaltyLogitsProcessor(penalty=repetition_penalty))\n",
    "        if no_repeat_ngram_size is not None and no_repeat_ngram_size > 0:\n",
    "            processors.append(NoRepeatNGramLogitsProcessor(no_repeat_ngram_size))\n",
    "        if encoder_no_repeat_ngram_size is not None and encoder_no_repeat_ngram_size > 0:\n",
    "            if self.config.is_encoder_decoder:\n",
    "                processors.append(EncoderNoRepeatNGramLogitsProcessor(encoder_no_repeat_ngram_size, encoder_input_ids))\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"It's impossible to use `encoder_no_repeat_ngram_size` with decoder-only architecture\"\n",
    "                )\n",
    "        if bad_words_ids is not None:\n",
    "            processors.append(NoBadWordsLogitsProcessor(bad_words_ids, eos_token_id))\n",
    "        if min_length is not None and eos_token_id is not None and min_length > -1:\n",
    "            processors.append(MinLengthLogitsProcessor(min_length, eos_token_id))\n",
    "        if prefix_allowed_tokens_fn is not None:\n",
    "            processors.append(PrefixConstrainedLogitsProcessor(prefix_allowed_tokens_fn, num_beams))\n",
    "        return processors\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        min_length: Optional[int] = None,\n",
    "        do_sample: Optional[bool] = None,\n",
    "        early_stopping: Optional[bool] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        repetition_penalty: Optional[float] = None,\n",
    "        bad_words_ids: Optional[Iterable[int]] = None,\n",
    "        bos_token_id: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        length_penalty: Optional[float] = None,\n",
    "        no_repeat_ngram_size: Optional[int] = None,\n",
    "        encoder_no_repeat_ngram_size: Optional[int] = None,\n",
    "        num_return_sequences: Optional[int] = None,\n",
    "        decoder_start_token_id: Optional[int] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        num_beam_groups: Optional[int] = None,\n",
    "        diversity_penalty: Optional[float] = None,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[GreedySearchOutput, SampleOutput, BeamSearchOutput, BeamSampleOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
    "        multinomial sampling, beam-search decoding, and beam-search multinomial sampling.\n",
    "\n",
    "        Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the\n",
    "        attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values\n",
    "        indicated are the default values of those config.\n",
    "\n",
    "        Most of these parameters are explained in more detail in `this blog post\n",
    "        <https://huggingface.co/blog/how-to-generate>`__.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            min_length (:obj:`int`, `optional`, defaults to 10):\n",
    "                The minimum length of the sequence to be generated.\n",
    "            do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "            early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
    "            num_beams (:obj:`int`, `optional`, defaults to 1):\n",
    "                Number of beams for beam search. 1 means no beam search.\n",
    "            temperature (:obj:`float`, `optional`, defaults tp 1.0):\n",
    "                The value used to module the next token probabilities.\n",
    "            top_k (:obj:`int`, `optional`, defaults to 50):\n",
    "                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                If set to float < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or\n",
    "                higher are kept for generation.\n",
    "            repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
    "                <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            bos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `beginning-of-sequence` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
    "                model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
    "                sequences.\n",
    "            no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
    "                If set to int > 0, all ngrams of that size can only occur once.\n",
    "            encoder_no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
    "                If set to int > 0, all ngrams of that size that occur in the ``encoder_input_ids`` cannot occur in the\n",
    "                ``decoder_input_ids``.\n",
    "            bad_words_ids(:obj:`List[List[int]]`, `optional`):\n",
    "                List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
    "                should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
    "                add_prefix_space=True).input_ids`.\n",
    "            num_return_sequences(:obj:`int`, `optional`, defaults to 1):\n",
    "                The number of independently computed returned sequences for each element in the batch.\n",
    "            attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
    "                tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same\n",
    "                shape as :obj:`input_ids` that masks the pad token. `What are attention masks?\n",
    "                <../glossary.html#attention-mask>`__\n",
    "            decoder_start_token_id (:obj:`int`, `optional`):\n",
    "                If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
    "            use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
    "                speed up decoding.\n",
    "            num_beam_groups (:obj:`int`, `optional`, defaults to 1):\n",
    "                Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
    "                beams. `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "            diversity_penalty (:obj:`float`, `optional`, defaults to 0.0):\n",
    "                This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
    "                at a particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is\n",
    "                enabled.\n",
    "            prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`):\n",
    "                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
    "                provided no constraint is applied. This function takes 2 arguments :obj:`inputs_ids` and the batch ID\n",
    "                :obj:`batch_id`. It has to return a list with the allowed tokens for the next generation step\n",
    "                conditioned on the previously generated tokens :obj:`inputs_ids` and the batch ID :obj:`batch_id`. This\n",
    "                argument is useful for constrained generation conditioned on the prefix, as described in\n",
    "                `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If the\n",
    "                model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific\n",
    "                kwargs should be prefixed with `decoder_`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.file_utils.ModelOutput` or :obj:`torch.LongTensor`: A\n",
    "            :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when\n",
    "            ``config.return_dict_in_generate=True``) or a :obj:`torch.FloatTensor`.\n",
    "\n",
    "                If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the\n",
    "                possible :class:`~transformers.file_utils.ModelOutput` types are:\n",
    "\n",
    "                    - :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`\n",
    "\n",
    "                If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible\n",
    "                :class:`~transformers.file_utils.ModelOutput` types are:\n",
    "\n",
    "                    - :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.SampleEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput`\n",
    "\n",
    "        Examples::\n",
    "            >>> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "            >>> # do greedy decoding without providing a prompt\n",
    "            >>> outputs = model.generate(max_length=40)\n",
    "            >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "            >>> document = (\n",
    "            ... \"at least two people were killed in a suspected bomb attack on a passenger bus \"\n",
    "            ... \"in the strife-torn southern philippines on monday , the military said.\"\n",
    "            ... )\n",
    "            >>> # encode input contex\n",
    "            >>> input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
    "            >>> # generate 3 independent sequences using beam search decoding (5 beams)\n",
    "            >>> # with T5 encoder-decoder model conditioned on short news article.\n",
    "            >>> outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "            >>> input_context = \"The dog\"\n",
    "            >>> # encode input context\n",
    "            >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "            >>> # generate 3 candidates using sampling\n",
    "            >>> outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"ctrl\")\n",
    "            >>> # \"Legal\" is one of the control codes for ctrl\n",
    "            >>> input_context = \"Legal My neighbor is\"\n",
    "            >>> # encode input context\n",
    "            >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "            >>> outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)\n",
    "            >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "            >>> input_context = \"My cute dog\"\n",
    "            >>> # get tokens of words that should not be generated\n",
    "            >>> bad_words_ids = [tokenizer(bad_word, add_prefix_space=True).input_ids for bad_word in [\"idiot\", \"stupid\", \"shut up\"]]\n",
    "            >>> # encode input context\n",
    "            >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "            >>> # generate sequences without allowing bad_words to be generated\n",
    "            >>> outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)\n",
    "            >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # set init values\n",
    "        num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
    "        num_beam_groups = num_beam_groups if num_beam_groups is not None else self.config.num_beam_groups\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
    "        num_return_sequences = (\n",
    "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
    "        )\n",
    "\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"output_attentions\"] = output_attentions\n",
    "        model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "\n",
    "        if input_ids is None:\n",
    "            # init `input_ids` with bos_token_id\n",
    "            input_ids = self._prepare_input_ids_for_generation(bos_token_id)\n",
    "\n",
    "        if model_kwargs.get(\"attention_mask\", None) is None:\n",
    "            # init `attention_mask` depending on `pad_token_id`\n",
    "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
    "                input_ids, pad_token_id, eos_token_id\n",
    "            )\n",
    "\n",
    "        # special case if pad_token_id is not defined\n",
    "        if pad_token_id is None and eos_token_id is not None:\n",
    "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "            pad_token_id = eos_token_id\n",
    "\n",
    "        # Storing encoder_input_ids for logits_processor that could use them\n",
    "        encoder_input_ids = input_ids if self.config.is_encoder_decoder else None\n",
    "\n",
    "        if self.config.is_encoder_decoder:\n",
    "            # add encoder_outputs to model_kwargs\n",
    "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)\n",
    "\n",
    "            # set input_ids as decoder_input_ids\n",
    "            if \"decoder_input_ids\" in model_kwargs:\n",
    "                input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "            else:\n",
    "                input_ids = self._prepare_decoder_input_ids_for_generation(\n",
    "                    input_ids, decoder_start_token_id=decoder_start_token_id, bos_token_id=bos_token_id\n",
    "                )\n",
    "\n",
    "            if \"encoder_outputs\" not in model_kwargs or not isinstance(model_kwargs[\"encoder_outputs\"], ModelOutput):\n",
    "                raise ValueError(\"Make sure that `model_kwargs` include `encoder_outputs` of type `ModelOutput`.\")\n",
    "\n",
    "        if input_ids.shape[-1] >= max_length:\n",
    "            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "            logger.warning(\n",
    "                f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}.\"\n",
    "                \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
    "            )\n",
    "\n",
    "        # determine generation mode\n",
    "        is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False\n",
    "        is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True\n",
    "        is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False\n",
    "        is_beam_sample_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is True\n",
    "        is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1)\n",
    "        if num_beam_groups > num_beams:\n",
    "            raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
    "        if is_group_beam_gen_mode and do_sample is True:\n",
    "            raise ValueError(\n",
    "                \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
    "            )\n",
    "\n",
    "        # set model_kwargs\n",
    "        model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "        # get distribution pre_processing samplers\n",
    "        logits_processor = self._get_logits_processor(\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
    "            encoder_input_ids=encoder_input_ids,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            min_length=min_length,\n",
    "            eos_token_id=eos_token_id,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            num_beams=num_beams,\n",
    "            num_beam_groups=num_beam_groups,\n",
    "            diversity_penalty=diversity_penalty,\n",
    "        )\n",
    "\n",
    "        if is_greedy_gen_mode:\n",
    "            if num_return_sequences > 1:\n",
    "                raise ValueError(\n",
    "                    f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
    "                )\n",
    "\n",
    "            # greedy search\n",
    "            return self.greedy_search(\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_sample_gen_mode:\n",
    "            # get probability distribution warper\n",
    "            logits_warper = self._get_logits_warper(\n",
    "                top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "            )\n",
    "\n",
    "            # expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids,\n",
    "                expand_size=num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # sample\n",
    "            return self.sample(\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=logits_warper,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_beam_gen_mode:\n",
    "            batch_size = input_ids.shape[0]\n",
    "\n",
    "            length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "            early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "\n",
    "            if num_return_sequences > num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "                num_beam_hyps_to_keep=num_return_sequences,\n",
    "            )\n",
    "            # interleave with `num_beams`\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "            )\n",
    "            return self.beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_beam_sample_gen_mode:\n",
    "            logits_warper = self._get_logits_warper(\n",
    "                top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "            )\n",
    "\n",
    "            batch_size = input_ids.shape[0] * num_return_sequences\n",
    "\n",
    "            length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "            )\n",
    "\n",
    "            # interleave with `num_beams * num_return_sequences`\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids,\n",
    "                expand_size=num_beams * num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            return self.beam_sample(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=logits_warper,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_group_beam_gen_mode:\n",
    "            batch_size = input_ids.shape[0]\n",
    "\n",
    "            length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "            early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "\n",
    "            if num_return_sequences > num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            if num_beams % num_beam_groups != 0:\n",
    "                raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
    "\n",
    "            diverse_beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "                num_beam_hyps_to_keep=num_return_sequences,\n",
    "                num_beam_groups=num_beam_groups,\n",
    "            )\n",
    "            # interleave with `num_beams`\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "            )\n",
    "            return self.group_beam_search(\n",
    "                input_ids,\n",
    "                diverse_beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "    def greedy_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[GreedySearchOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using greedy decoding.\n",
    "\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\n",
    "            model_kwargs:\n",
    "                Additional model specific keyword arguments will be forwarded to the :obj:`forward` function of the\n",
    "                model. If model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ... AutoTokenizer,\n",
    "            ... AutoModelForCausalLM,\n",
    "            ... LogitsProcessorList,\n",
    "            ... MinLengthLogitsProcessor,\n",
    "            ... )\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "            >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "            >>> model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "            >>> input_prompt = \"Today is a beautiful day, and\"\n",
    "            >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.greedy_search(input_ids, logits_processor=logits_processor)\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        # init sequence length tensors\n",
    "        sequence_lengths, unfinished_sequences, cur_len = self._init_sequence_length_for_generation(\n",
    "            input_ids, max_length\n",
    "        )\n",
    "\n",
    "        while cur_len < max_length:\n",
    "            # prepare model inputs\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_logits,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # pre-process distribution\n",
    "            next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "            # argmax\n",
    "            next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "\n",
    "            # add code that transfomers next_tokens to tokens_to_add\n",
    "            if eos_token_id is not None:\n",
    "                assert pad_token_id is not None, \"If eos_token_id is defined, make sure that pad_token_id is defined.\"\n",
    "                next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)\n",
    "\n",
    "            # add token and increase length by one\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "\n",
    "            # update sequence length\n",
    "            if eos_token_id is not None:\n",
    "                sequence_lengths, unfinished_sequences = self._update_seq_length_for_generation(\n",
    "                    sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id\n",
    "                )\n",
    "\n",
    "            # update model kwargs\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "\n",
    "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
    "            if unfinished_sequences.max() == 0:\n",
    "                break\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return GreedySearchEncoderDecoderOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return GreedySearchDecoderOnlyOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return input_ids\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        logits_warper: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[SampleOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using multinomial sampling.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            logits_warper (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsWarper` used to warp the prediction score distribution of the language\n",
    "                modeling head applied before multinomial sampling at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
    "                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.SampleEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.SampleDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.SampleEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ...    AutoTokenizer,\n",
    "            ...    AutoModelForCausalLM,\n",
    "            ...    LogitsProcessorList,\n",
    "            ...    MinLengthLogitsProcessor,\n",
    "            ...    TopKLogitsWarper,\n",
    "            ...    TemperatureLogitsWarper,\n",
    "            ... )\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "            >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "            >>> model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "            >>> input_prompt = \"Today is a beautiful day, and\"\n",
    "            >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
    "            ... ])\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_warper = LogitsProcessorList([\n",
    "            ...     TopKLogitsWarper(50),\n",
    "            ...     TemperatureLogitsWarper(0.7),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.sample(input_ids, logits_processor=logits_processor, logits_warper=logits_warper)\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        logits_warper = logits_warper if logits_warper is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        # init sequence length tensors\n",
    "        sequence_lengths, unfinished_sequences, cur_len = self._init_sequence_length_for_generation(\n",
    "            input_ids, max_length\n",
    "        )\n",
    "\n",
    "        # auto-regressive generation\n",
    "        while cur_len < max_length:\n",
    "            # prepare model inputs\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # pre-process distribution\n",
    "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "            next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_scores,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # sample\n",
    "            probs = F.softmax(next_token_scores, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "            # add code that transfomers next_tokens to tokens_to_add\n",
    "            if eos_token_id is not None:\n",
    "                assert pad_token_id is not None, \"If eos_token_id is defined, make sure that pad_token_id is defined.\"\n",
    "                next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)\n",
    "\n",
    "            # add token and increase length by one\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            # update sequence length\n",
    "            if eos_token_id is not None:\n",
    "                sequence_lengths, unfinished_sequences = self._update_seq_length_for_generation(\n",
    "                    sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id\n",
    "                )\n",
    "\n",
    "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
    "            if unfinished_sequences.max() == 0:\n",
    "                break\n",
    "\n",
    "            # update model kwargs\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return SampleEncoderDecoderOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return SampleDecoderOnlyOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return input_ids\n",
    "\n",
    "    def beam_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[BeamSearchOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using beam search decoding.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            beam_scorer (:obj:`BeamScorer`):\n",
    "                An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
    "                constructed, stored and sorted during generation. For more information, the documentation of\n",
    "                :class:`~transformers.BeamScorer` should be read.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
    "                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utilsBeamSearchDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ...    AutoTokenizer,\n",
    "            ...    AutoModelForSeq2SeqLM,\n",
    "            ...    LogitsProcessorList,\n",
    "            ...    MinLengthLogitsProcessor,\n",
    "            ...    BeamSearchScorer,\n",
    "            ... )\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "            >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
    "            >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "            >>> # lets run beam search using 3 beams\n",
    "            >>> num_beams = 3\n",
    "            >>> # define decoder start token ids\n",
    "            >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "            >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "            >>> # add encoder_outputs to model keyword arguments\n",
    "            >>> model_kwargs = {\n",
    "            ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
    "            ... }\n",
    "\n",
    "            >>> # instantiate beam scorer\n",
    "            >>> beam_scorer = BeamSearchScorer(\n",
    "            ...     batch_size=1,\n",
    "            ...     max_length=model.config.max_length,\n",
    "            ...     num_beams=num_beams,\n",
    "            ...     device=model.device,\n",
    "            ... )\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "        assert (\n",
    "            num_beams * batch_size == batch_beam_size\n",
    "        ), \"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "        beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while cur_len < max_length:\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # adjust tokens for Bart, *e.g.*\n",
    "            next_token_logits = self.adjust_logits_during_generation(\n",
    "                next_token_logits, cur_len=cur_len, max_length=max_length\n",
    "            )\n",
    "\n",
    "            next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            next_token_scores = logits_processor(input_ids, next_token_scores)\n",
    "            next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_scores,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # reshape for beam search\n",
    "            vocab_size = next_token_scores.shape[-1]\n",
    "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "            next_token_scores, next_tokens = torch.topk(\n",
    "                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n",
    "            )\n",
    "\n",
    "            next_indices = next_tokens // vocab_size\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "\n",
    "            # stateless\n",
    "            beam_outputs = beam_scorer.process(\n",
    "                input_ids,\n",
    "                next_token_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "            )\n",
    "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            if model_kwargs[\"past\"] is not None:\n",
    "                model_kwargs[\"past\"] = self._reorder_cache(model_kwargs[\"past\"], beam_idx)\n",
    "\n",
    "            if beam_scorer.is_done:\n",
    "                break\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"] = None\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return BeamSearchEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return BeamSearchDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]\n",
    "\n",
    "    def beam_sample(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        logits_warper: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[BeamSampleOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using beam search with multinomial sampling.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            beam_scorer (:obj:`BeamScorer`):\n",
    "                A derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
    "                constructed, stored and sorted during generation. For more information, the documentation of\n",
    "                :class:`~transformers.BeamScorer` should be read.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            logits_warper (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsWarper` used to warp the prediction score distribution of the language\n",
    "                modeling head applied before multinomial sampling at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
    "                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ...     AutoTokenizer,\n",
    "            ...     AutoModelForSeq2SeqLM,\n",
    "            ...     LogitsProcessorList,\n",
    "            ...     MinLengthLogitsProcessor,\n",
    "            ...     TopKLogitsWarper,\n",
    "            ...     TemperatureLogitsWarper,\n",
    "            ...     BeamSearchScorer,\n",
    "            ... )\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "            >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
    "            >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            >>> # lets run beam search using 3 beams\n",
    "            >>> num_beams = 3\n",
    "            >>> # define decoder start token ids\n",
    "            >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "            >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "            >>> # add encoder_outputs to model keyword arguments\n",
    "            >>> model_kwargs = {\n",
    "            ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
    "            ... }\n",
    "\n",
    "            >>> # instantiate beam scorer\n",
    "            >>> beam_scorer = BeamSearchScorer(\n",
    "            ...     batch_size=1,\n",
    "            ...     max_length=model.config.max_length,\n",
    "            ...     num_beams=num_beams,\n",
    "            ...     device=model.device,\n",
    "            ... )\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)\n",
    "            ... ])\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_warper = LogitsProcessorList([\n",
    "            ...     TopKLogitsWarper(50),\n",
    "            ...     TemperatureLogitsWarper(0.7),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.beam_sample(\n",
    "            ...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n",
    "            ... )\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while cur_len < max_length:\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # adjust token scores (a no-op by default)\n",
    "            next_token_logits = self.adjust_logits_during_generation(\n",
    "                next_token_logits, cur_len=cur_len, max_length=max_length\n",
    "            )\n",
    "\n",
    "            next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            next_token_scores = logits_processor(input_ids, next_token_scores)\n",
    "            next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\n",
    "            next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_scores,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # reshape for beam search\n",
    "            vocab_size = next_token_scores.shape[-1]\n",
    "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "            probs = F.softmax(next_token_scores, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)\n",
    "            next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n",
    "\n",
    "            next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n",
    "            next_tokens = torch.gather(next_tokens, -1, _indices)\n",
    "\n",
    "            next_indices = next_tokens // vocab_size\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "\n",
    "            # stateless\n",
    "            beam_outputs = beam_scorer.process(\n",
    "                input_ids,\n",
    "                next_token_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "            )\n",
    "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            if model_kwargs[\"past\"] is not None:\n",
    "                model_kwargs[\"past\"] = self._reorder_cache(model_kwargs[\"past\"], beam_idx)\n",
    "\n",
    "            if beam_scorer.is_done:\n",
    "                break\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"] = None\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return BeamSearchEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return BeamSearchDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]\n",
    "\n",
    "    def group_beam_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using beam search decoding.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            beam_scorer (:obj:`BeamScorer`):\n",
    "                An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
    "                constructed, stored and sorted during generation. For more information, the documentation of\n",
    "                :class:`~transformers.BeamScorer` should be read.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs that will be forwarded to the :obj:`forward` function of the model. If\n",
    "                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
    "            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ...    AutoTokenizer,\n",
    "            ...    AutoModelForSeq2SeqLM,\n",
    "            ...    LogitsProcessorList,\n",
    "            ...    MinLengthLogitsProcessor,\n",
    "            ...    HammingDiversityLogitsProcessor,\n",
    "            ...    BeamSearchScorer,\n",
    "            ... )\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "            >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
    "            >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "            >>> # lets run diverse beam search using 6 beams\n",
    "            >>> num_beams = 6\n",
    "            >>> # define decoder start token ids\n",
    "            >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "            >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "            >>> # add encoder_outputs to model keyword arguments\n",
    "            >>> model_kwargs = {\n",
    "            ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
    "            ... }\n",
    "\n",
    "            >>> # instantiate beam scorer\n",
    "            >>> beam_scorer = BeamSearchScorer(\n",
    "            ...     batch_size=1,\n",
    "            ...     max_length=model.config.max_length,\n",
    "            ...     num_beams=num_beams,\n",
    "            ...     device=model.device,\n",
    "            ...     num_beam_groups=3\n",
    "            ... )\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n",
    "            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.group_beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "        num_beam_groups = beam_scorer.num_beam_groups\n",
    "        num_sub_beams = num_beams // num_beam_groups\n",
    "        device = input_ids.device\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "        assert (\n",
    "            num_beams * batch_size == batch_beam_size\n",
    "        ), f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "\n",
    "        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n",
    "        # initialise score of first beam of each group with 0 and the rest with 1e-9. This ensures that the beams in\n",
    "        # the same group don't produce same tokens everytime.\n",
    "        beam_scores[:, ::num_sub_beams] = 0\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while cur_len < max_length:\n",
    "            # predicted tokens in cur_len step\n",
    "            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n",
    "\n",
    "            # indices which will form the beams in the next time step\n",
    "            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)\n",
    "\n",
    "            # do one decoder step on all beams of all sentences in batch\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "            for beam_group_idx in range(num_beam_groups):\n",
    "                group_start_idx = beam_group_idx * num_sub_beams\n",
    "                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)\n",
    "                group_size = group_end_idx - group_start_idx\n",
    "\n",
    "                # indices of beams of current group among all sentences in batch\n",
    "                batch_group_indices = []\n",
    "\n",
    "                if output_scores:\n",
    "                    processed_score = torch.zeros_like(outputs.logits[:, -1, :])\n",
    "\n",
    "                for batch_idx in range(batch_size):\n",
    "                    batch_group_indices.extend(\n",
    "                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]\n",
    "                    )\n",
    "                group_input_ids = input_ids[batch_group_indices]\n",
    "\n",
    "                # select outputs of beams of current group only\n",
    "                next_token_logits = outputs.logits[batch_group_indices, -1, :]\n",
    "\n",
    "                # adjust tokens for Bart, *e.g.*\n",
    "                next_token_logits = self.adjust_logits_during_generation(\n",
    "                    next_token_logits, cur_len=cur_len, max_length=max_length\n",
    "                )\n",
    "\n",
    "                next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * group_size, vocab_size)\n",
    "                vocab_size = next_token_scores.shape[-1]\n",
    "\n",
    "                next_token_scores = logits_processor(\n",
    "                    group_input_ids, next_token_scores, current_tokens=current_tokens, beam_group_idx=beam_group_idx\n",
    "                )\n",
    "                next_token_scores = next_token_scores + beam_scores[batch_group_indices].unsqueeze(-1).expand_as(\n",
    "                    next_token_scores\n",
    "                )\n",
    "\n",
    "                if output_scores:\n",
    "                    processed_score[batch_group_indices] = next_token_scores\n",
    "\n",
    "                # reshape for beam search\n",
    "                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)\n",
    "\n",
    "                next_token_scores, next_tokens = torch.topk(\n",
    "                    next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True\n",
    "                )\n",
    "\n",
    "                next_indices = next_tokens // vocab_size\n",
    "                next_tokens = next_tokens % vocab_size\n",
    "\n",
    "                # stateless\n",
    "                beam_outputs = beam_scorer.process(\n",
    "                    group_input_ids,\n",
    "                    next_token_scores,\n",
    "                    next_tokens,\n",
    "                    next_indices,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id,\n",
    "                )\n",
    "                beam_scores[batch_group_indices] = beam_outputs[\"next_beam_scores\"]\n",
    "                beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "                beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "                input_ids[batch_group_indices] = group_input_ids[beam_idx]\n",
    "                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "                current_tokens[batch_group_indices] = group_input_ids[:, -1]\n",
    "\n",
    "                # (beam_idx // group_size) -> batch_idx\n",
    "                # (beam_idx % group_size) -> offset of idx inside the group\n",
    "                reordering_indices[batch_group_indices] = (\n",
    "                    num_beams * (beam_idx // group_size) + group_start_idx + (beam_idx % group_size)\n",
    "                )\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (processed_score,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            if model_kwargs[\"past\"] is not None:\n",
    "                model_kwargs[\"past\"] = self._reorder_cache(model_kwargs[\"past\"], reordering_indices)\n",
    "\n",
    "            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "            if beam_scorer.is_done:\n",
    "                break\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"]\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return BeamSearchEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return BeamSearchDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(\n",
    "    logits: torch.FloatTensor,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    "    filter_value: float = -float(\"Inf\"),\n",
    "    min_tokens_to_keep: int = 1,\n",
    ") -> torch.FloatTensor:\n",
    "    \"\"\"\n",
    "    Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "\n",
    "    Args:\n",
    "        logits: logits distribution shape (batch size, vocabulary size)\n",
    "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        logits = TopKLogitsWarper(top_k=top_k, filter_value=filter_value, min_tokens_to_keep=min_tokens_to_keep)(\n",
    "            None, logits\n",
    "        )\n",
    "\n",
    "    if 0 <= top_p <= 1.0:\n",
    "        logits = TopPLogitsWarper(top_p=top_p, min_tokens_to_keep=min_tokens_to_keep)(None, logits)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:50:15.137156Z",
     "start_time": "2021-02-10T14:50:14.972680Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, device, dtype, nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from .activations import get_activation\n",
    "# from .configuration_utils import PretrainedConfig\n",
    "# from .file_utils import (\n",
    "#     DUMMY_INPUTS,\n",
    "#     TF2_WEIGHTS_NAME,\n",
    "#     TF_WEIGHTS_NAME,\n",
    "#     WEIGHTS_NAME,\n",
    "#     ModelOutput,\n",
    "#     cached_path,\n",
    "#     hf_bucket_url,\n",
    "#     is_remote_url,\n",
    "#     is_torch_tpu_available,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "# from .generation_utils import GenerationMixin\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "try:\n",
    "    from torch.nn import Identity\n",
    "except ImportError:\n",
    "    # Older PyTorch compatibility\n",
    "    class Identity(nn.Module):\n",
    "        r\"\"\"A placeholder identity operator that is argument-insensitive.\"\"\"\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input):\n",
    "            return input\n",
    "\n",
    "\n",
    "def find_pruneable_heads_and_indices(\n",
    "    heads: List[int], n_heads: int, head_size: int, already_pruned_heads: Set[int]\n",
    ") -> Tuple[Set[int], torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Finds the heads and their indices taking :obj:`already_pruned_heads` into account.\n",
    "\n",
    "    Args:\n",
    "        heads (:obj:`List[int]`): List of the indices of heads to prune.\n",
    "        n_heads (:obj:`int`): The number of heads in the model.\n",
    "        head_size (:obj:`int`): The size of each head.\n",
    "        already_pruned_heads (:obj:`Set[int]`): A set of already pruned heads.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`Tuple[Set[int], torch.LongTensor]`: A tuple with the remaining heads and their corresponding indices.\n",
    "    \"\"\"\n",
    "    mask = torch.ones(n_heads, head_size)\n",
    "    heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads\n",
    "    for head in heads:\n",
    "        # Compute how many pruned heads are before the head and move the index accordingly\n",
    "        head = head - sum(1 if h < head else 0 for h in already_pruned_heads)\n",
    "        mask[head] = 0\n",
    "    mask = mask.view(-1).contiguous().eq(1)\n",
    "    index: torch.LongTensor = torch.arange(len(mask))[mask].long()\n",
    "    return heads, index\n",
    "\n",
    "\n",
    "class ModuleUtilsMixin:\n",
    "    \"\"\"\n",
    "    A few utilities for :obj:`torch.nn.Modules`, to be used as a mixin.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _hook_rss_memory_pre_forward(module, *args, **kwargs):\n",
    "        try:\n",
    "            import psutil\n",
    "        except (ImportError):\n",
    "            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n",
    "\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem = process.memory_info()\n",
    "        module.mem_rss_pre_forward = mem.rss\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _hook_rss_memory_post_forward(module, *args, **kwargs):\n",
    "        try:\n",
    "            import psutil\n",
    "        except (ImportError):\n",
    "            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n",
    "\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem = process.memory_info()\n",
    "        module.mem_rss_post_forward = mem.rss\n",
    "        mem_rss_diff = module.mem_rss_post_forward - module.mem_rss_pre_forward\n",
    "        module.mem_rss_diff = mem_rss_diff + (module.mem_rss_diff if hasattr(module, \"mem_rss_diff\") else 0)\n",
    "        return None\n",
    "\n",
    "    def add_memory_hooks(self):\n",
    "        \"\"\"\n",
    "        Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n",
    "\n",
    "        Increase in memory consumption is stored in a :obj:`mem_rss_diff` attribute for each module and can be reset to\n",
    "        zero with :obj:`model.reset_memory_hooks_state()`.\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            module.register_forward_pre_hook(self._hook_rss_memory_pre_forward)\n",
    "            module.register_forward_hook(self._hook_rss_memory_post_forward)\n",
    "        self.reset_memory_hooks_state()\n",
    "\n",
    "    def reset_memory_hooks_state(self):\n",
    "        \"\"\"\n",
    "        Reset the :obj:`mem_rss_diff` attribute of each module (see\n",
    "        :func:`~transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks`).\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            module.mem_rss_diff = 0\n",
    "            module.mem_rss_post_forward = 0\n",
    "            module.mem_rss_pre_forward = 0\n",
    "\n",
    "    @property\n",
    "    def device(self) -> device:\n",
    "        \"\"\"\n",
    "        :obj:`torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n",
    "        device).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return next(self.parameters()).device\n",
    "        except StopIteration:\n",
    "            # For nn.DataParallel compatibility in PyTorch 1.5\n",
    "\n",
    "            def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n",
    "                tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
    "                return tuples\n",
    "\n",
    "            gen = self._named_members(get_members_fn=find_tensor_attributes)\n",
    "            first_tuple = next(gen)\n",
    "            return first_tuple[1].device\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> dtype:\n",
    "        \"\"\"\n",
    "        :obj:`torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return next(self.parameters()).dtype\n",
    "        except StopIteration:\n",
    "            # For nn.DataParallel compatibility in PyTorch 1.5\n",
    "\n",
    "            def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n",
    "                tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
    "                return tuples\n",
    "\n",
    "            gen = self._named_members(get_members_fn=find_tensor_attributes)\n",
    "            first_tuple = next(gen)\n",
    "            return first_tuple[1].dtype\n",
    "\n",
    "    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Invert an attention mask (e.g., switches 0. and 1.).\n",
    "\n",
    "        Args:\n",
    "            encoder_attention_mask (:obj:`torch.Tensor`): An attention mask.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: The inverted attention mask.\n",
    "        \"\"\"\n",
    "        if encoder_attention_mask.dim() == 3:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
    "        if encoder_attention_mask.dim() == 2:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
    "        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n",
    "        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow\n",
    "        # /transformer/transformer_layers.py#L270\n",
    "        # encoder_extended_attention_mask = (encoder_extended_attention_mask ==\n",
    "        # encoder_extended_attention_mask.transpose(-1, -2))\n",
    "        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "\n",
    "        if self.dtype == torch.float16:\n",
    "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e4\n",
    "        elif self.dtype == torch.float32:\n",
    "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e9\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"{} not recognized. `dtype` should be set to either `torch.float32` or `torch.float16`\".format(\n",
    "                    self.dtype\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return encoder_extended_attention_mask\n",
    "\n",
    "    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device) -> Tensor:\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (:obj:`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (:obj:`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "            device: (:obj:`torch.device`):\n",
    "                The device of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "        \"\"\"\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            if self.config.is_decoder:\n",
    "                batch_size, seq_length = input_shape\n",
    "                seq_ids = torch.arange(seq_length, device=device)\n",
    "                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "                # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
    "                # causal and attention masks must have same type with pytorch version < 1.3\n",
    "                causal_mask = causal_mask.to(attention_mask.dtype)\n",
    "\n",
    "                if causal_mask.shape[1] < attention_mask.shape[1]:\n",
    "                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
    "                    causal_mask = torch.cat(\n",
    "                        [\n",
    "                            torch.ones(\n",
    "                                (batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype\n",
    "                            ),\n",
    "                            causal_mask,\n",
    "                        ],\n",
    "                        axis=-1,\n",
    "                    )\n",
    "\n",
    "                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            else:\n",
    "                extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "    def get_head_mask(\n",
    "        self, head_mask: Optional[Tensor], num_hidden_layers: int, is_attention_chunked: bool = False\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Prepare the head mask if needed.\n",
    "\n",
    "        Args:\n",
    "            head_mask (:obj:`torch.Tensor` with shape :obj:`[num_heads]` or :obj:`[num_hidden_layers x num_heads]`, `optional`):\n",
    "                The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n",
    "            num_hidden_layers (:obj:`int`):\n",
    "                The number of hidden layers in the model.\n",
    "            is_attention_chunked: (:obj:`bool`, `optional, defaults to :obj:`False`):\n",
    "                Whether or not the attentions scores are computed by chunks or not.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` with shape :obj:`[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or\n",
    "            list with :obj:`[None]` for each layer.\n",
    "        \"\"\"\n",
    "        if head_mask is not None:\n",
    "            head_mask = self._convert_head_mask_to_5d(head_mask, num_hidden_layers)\n",
    "            if is_attention_chunked is True:\n",
    "                head_mask = head_mask.unsqueeze(-1)\n",
    "        else:\n",
    "            head_mask = [None] * num_hidden_layers\n",
    "\n",
    "        return head_mask\n",
    "\n",
    "    def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):\n",
    "        \"\"\"-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]\"\"\"\n",
    "        if head_mask.dim() == 1:\n",
    "            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            head_mask = head_mask.expand(num_hidden_layers, -1, -1, -1, -1)\n",
    "        elif head_mask.dim() == 2:\n",
    "            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
    "        assert head_mask.dim() == 5, f\"head_mask.dim != 5, instead {head_mask.dim()}\"\n",
    "        head_mask = head_mask.to(dtype=self.dtype)  # switch to float if need + fp16 compatibility\n",
    "        return head_mask\n",
    "\n",
    "    def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Get number of (optionally, trainable or non-embeddings) parameters in the module.\n",
    "\n",
    "        Args:\n",
    "            only_trainable (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return only the number of trainable parameters\n",
    "\n",
    "            exclude_embeddings (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return only the number of non-embeddings parameters\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: The number of parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        def parameter_filter(x):\n",
    "            return (x.requires_grad or not only_trainable) and not (\n",
    "                isinstance(x, torch.nn.Embedding) and exclude_embeddings\n",
    "            )\n",
    "\n",
    "        params = filter(parameter_filter, self.parameters()) if only_trainable else self.parameters()\n",
    "        return sum(p.numel() for p in params)\n",
    "\n",
    "    def estimate_tokens(self, input_dict: Dict[str, Union[torch.Tensor, Any]]) -> int:\n",
    "        \"\"\"\n",
    "        Helper function to estimate the total number of tokens from the model inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs (:obj:`dict`): The model inputs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: The total number of tokens.\n",
    "        \"\"\"\n",
    "        token_inputs = [tensor for key, tensor in input_dict.items() if \"input\" in key]\n",
    "        if token_inputs:\n",
    "            return sum([token_input.numel() for token_input in token_inputs])\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"Could not estimate the number of tokens of the input, floating-point operations will not be computed\"\n",
    "            )\n",
    "            return 0\n",
    "\n",
    "    def floating_point_ops(\n",
    "        self, input_dict: Dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n",
    "        batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n",
    "        tokens (valid if :obj:`12 * d_model << sequence_length`) as laid out in `this paper\n",
    "        <https://arxiv.org/pdf/2001.08361.pdf>`__ section 2.1. Should be overridden for transformers with parameter\n",
    "        re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n",
    "\n",
    "        Args:\n",
    "            batch_size (:obj:`int`):\n",
    "                The batch size for the forward pass.\n",
    "\n",
    "            sequence_length (:obj:`int`):\n",
    "                The number of tokens in each line of the batch.\n",
    "\n",
    "            exclude_embeddings (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to count embedding and softmax operations.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: The number of floating-point operations.\n",
    "        \"\"\"\n",
    "\n",
    "        return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)\n",
    "\n",
    "\n",
    "class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):\n",
    "    r\"\"\"\n",
    "    Base class for all models.\n",
    "\n",
    "    :class:`~transformers.PreTrainedModel` takes care of storing the configuration of the models and handles methods\n",
    "    for loading, downloading and saving models as well as a few methods common to all models to:\n",
    "\n",
    "        * resize the input embeddings,\n",
    "        * prune heads in the self-attention heads.\n",
    "\n",
    "    Class attributes (overridden by derived classes):\n",
    "\n",
    "        - **config_class** (:class:`~transformers.PretrainedConfig`) -- A subclass of\n",
    "          :class:`~transformers.PretrainedConfig` to use as configuration class for this model architecture.\n",
    "        - **load_tf_weights** (:obj:`Callable`) -- A python `method` for loading a TensorFlow checkpoint in a PyTorch\n",
    "          model, taking as arguments:\n",
    "\n",
    "            - **model** (:class:`~transformers.PreTrainedModel`) -- An instance of the model on which to load the\n",
    "              TensorFlow checkpoint.\n",
    "            - **config** (:class:`~transformers.PreTrainedConfig`) -- An instance of the configuration associated to\n",
    "              the model.\n",
    "            - **path** (:obj:`str`) -- A path to the TensorFlow checkpoint.\n",
    "\n",
    "        - **base_model_prefix** (:obj:`str`) -- A string indicating the attribute associated to the base model in\n",
    "          derived classes of the same architecture adding modules on top of the base model.\n",
    "        - **is_parallelizable** (:obj:`bool`) -- A flag indicating whether this model supports model parallelization.\n",
    "    \"\"\"\n",
    "    config_class = None\n",
    "    base_model_prefix = \"\"\n",
    "    # a list of re pattern of tensor names to ignore from the model when loading the model weights\n",
    "    # (and avoid unnecessary warnings).\n",
    "    _keys_to_ignore_on_load_missing = None\n",
    "    # a list of re pattern of tensor names to ignore from the weights when loading the model weights\n",
    "    # (and avoid unnecessary warnings).\n",
    "    _keys_to_ignore_on_load_unexpected = None\n",
    "    # a list of of tensor names to ignore when saving the model (useful for keys that aren't\n",
    "    # trained, but which are deterministic)\n",
    "    _keys_to_ignore_on_save = None\n",
    "\n",
    "    is_parallelizable = False\n",
    "\n",
    "    @property\n",
    "    def dummy_inputs(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :obj:`Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n",
    "        \"\"\"\n",
    "        return {\"input_ids\": torch.tensor(DUMMY_INPUTS)}\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            raise ValueError(\n",
    "                \"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"\n",
    "                \"To create a model from a pretrained model use \"\n",
    "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "                    self.__class__.__name__, self.__class__.__name__\n",
    "                )\n",
    "            )\n",
    "        # Save config and origin of the pretrained weights if given in model\n",
    "        self.config = config\n",
    "        self.name_or_path = config.name_or_path\n",
    "\n",
    "    @property\n",
    "    def base_model(self) -> nn.Module:\n",
    "        \"\"\"\n",
    "        :obj:`torch.nn.Module`: The main body of the model.\n",
    "        \"\"\"\n",
    "        return getattr(self, self.base_model_prefix, self)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Returns the model's input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`nn.Module`: A torch module mapping vocabulary to hidden states.\n",
    "        \"\"\"\n",
    "        base_model = getattr(self, self.base_model_prefix, self)\n",
    "        if base_model is not self:\n",
    "            return base_model.get_input_embeddings()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def set_input_embeddings(self, value: nn.Module):\n",
    "        \"\"\"\n",
    "        Set model's input embeddings.\n",
    "\n",
    "        Args:\n",
    "            value (:obj:`nn.Module`): A module mapping vocabulary to hidden states.\n",
    "        \"\"\"\n",
    "        base_model = getattr(self, self.base_model_prefix, self)\n",
    "        if base_model is not self:\n",
    "            base_model.set_input_embeddings(value)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_output_embeddings(self) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Returns the model's output embeddings.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`nn.Module`: A torch module mapping hidden states to vocabulary.\n",
    "        \"\"\"\n",
    "        return None  # Overwrite for models with output embeddings\n",
    "\n",
    "    def tie_weights(self):\n",
    "        \"\"\"\n",
    "        Tie the weights between the input embeddings and the output embeddings.\n",
    "\n",
    "        If the :obj:`torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning\n",
    "        the weights instead.\n",
    "        \"\"\"\n",
    "        output_embeddings = self.get_output_embeddings()\n",
    "        if output_embeddings is not None and self.config.tie_word_embeddings:\n",
    "            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
    "\n",
    "        if self.config.is_encoder_decoder and self.config.tie_encoder_decoder:\n",
    "            if hasattr(self, self.base_model_prefix):\n",
    "                self = getattr(self, self.base_model_prefix)\n",
    "            self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)\n",
    "\n",
    "    @staticmethod\n",
    "    def _tie_encoder_decoder_weights(encoder: nn.Module, decoder: nn.Module, base_model_prefix: str):\n",
    "        uninitialized_encoder_weights: List[str] = []\n",
    "        if decoder.__class__ != encoder.__class__:\n",
    "            logger.info(\n",
    "                f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder weights are correctly initialized.\"\n",
    "            )\n",
    "\n",
    "        def tie_encoder_to_decoder_recursively(\n",
    "            decoder_pointer: nn.Module,\n",
    "            encoder_pointer: nn.Module,\n",
    "            module_name: str,\n",
    "            uninitialized_encoder_weights: List[str],\n",
    "            depth=0,\n",
    "        ):\n",
    "            assert isinstance(decoder_pointer, nn.Module) and isinstance(\n",
    "                encoder_pointer, nn.Module\n",
    "            ), f\"{decoder_pointer} and {encoder_pointer} have to be of type torch.nn.Module\"\n",
    "            if hasattr(decoder_pointer, \"weight\"):\n",
    "                assert hasattr(encoder_pointer, \"weight\")\n",
    "                encoder_pointer.weight = decoder_pointer.weight\n",
    "                if hasattr(decoder_pointer, \"bias\"):\n",
    "                    assert hasattr(encoder_pointer, \"bias\")\n",
    "                    encoder_pointer.bias = decoder_pointer.bias\n",
    "                return\n",
    "\n",
    "            encoder_modules = encoder_pointer._modules\n",
    "            decoder_modules = decoder_pointer._modules\n",
    "            if len(decoder_modules) > 0:\n",
    "                assert (\n",
    "                    len(encoder_modules) > 0\n",
    "                ), f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n",
    "\n",
    "                all_encoder_weights = set([module_name + \"/\" + sub_name for sub_name in encoder_modules.keys()])\n",
    "                encoder_layer_pos = 0\n",
    "                for name, module in decoder_modules.items():\n",
    "                    if name.isdigit():\n",
    "                        encoder_name = str(int(name) + encoder_layer_pos)\n",
    "                        decoder_name = name\n",
    "                        if not isinstance(decoder_modules[decoder_name], type(encoder_modules[encoder_name])) and len(\n",
    "                            encoder_modules\n",
    "                        ) != len(decoder_modules):\n",
    "                            # this can happen if the name corresponds to the position in a list module list of layers\n",
    "                            # in this case the decoder has added a cross-attention that the encoder does not have\n",
    "                            # thus skip this step and subtract one layer pos from encoder\n",
    "                            encoder_layer_pos -= 1\n",
    "                            continue\n",
    "                    elif name not in encoder_modules:\n",
    "                        continue\n",
    "                    elif depth > 500:\n",
    "                        raise ValueError(\n",
    "                            \"Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is a circular dependency between two or more `nn.Modules` of your model.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        decoder_name = encoder_name = name\n",
    "                    tie_encoder_to_decoder_recursively(\n",
    "                        decoder_modules[decoder_name],\n",
    "                        encoder_modules[encoder_name],\n",
    "                        module_name + \"/\" + name,\n",
    "                        uninitialized_encoder_weights,\n",
    "                        depth=depth + 1,\n",
    "                    )\n",
    "                    all_encoder_weights.remove(module_name + \"/\" + encoder_name)\n",
    "\n",
    "                uninitialized_encoder_weights += list(all_encoder_weights)\n",
    "\n",
    "        # tie weights recursively\n",
    "        tie_encoder_to_decoder_recursively(decoder, encoder, base_model_prefix, uninitialized_encoder_weights)\n",
    "        if len(uninitialized_encoder_weights) > 0:\n",
    "            logger.warning(\n",
    "                f\"The following encoder weights were not tied to the decoder {uninitialized_encoder_weights}\"\n",
    "            )\n",
    "\n",
    "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n",
    "        \"\"\"Tie or clone module weights depending of whether we are using TorchScript or not\"\"\"\n",
    "        if self.config.torchscript:\n",
    "            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n",
    "        else:\n",
    "            output_embeddings.weight = input_embeddings.weight\n",
    "\n",
    "        if getattr(output_embeddings, \"bias\", None) is not None:\n",
    "            output_embeddings.bias.data = torch.nn.functional.pad(\n",
    "                output_embeddings.bias.data,\n",
    "                (\n",
    "                    0,\n",
    "                    output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
    "                ),\n",
    "                \"constant\",\n",
    "                0,\n",
    "            )\n",
    "        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n",
    "            output_embeddings.out_features = input_embeddings.num_embeddings\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> torch.nn.Embedding:\n",
    "        \"\"\"\n",
    "        Resizes input token embeddings matrix of the model if :obj:`new_num_tokens != config.vocab_size`.\n",
    "\n",
    "        Takes care of tying weights embeddings afterwards if the model class has a :obj:`tie_weights()` method.\n",
    "\n",
    "        Arguments:\n",
    "            new_num_tokens (:obj:`int`, `optional`):\n",
    "                The number of new tokens in the embedding matrix. Increasing the size will add newly initialized\n",
    "                vectors at the end. Reducing the size will remove vectors from the end. If not provided or :obj:`None`,\n",
    "                just returns a pointer to the input tokens :obj:`torch.nn.Embedding` module of the model without doing\n",
    "                anything.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n",
    "        \"\"\"\n",
    "        model_embeds = self._resize_token_embeddings(new_num_tokens)\n",
    "        if new_num_tokens is None:\n",
    "            return model_embeds\n",
    "\n",
    "        # Update base model and current model config\n",
    "        self.config.vocab_size = new_num_tokens\n",
    "        self.vocab_size = new_num_tokens\n",
    "\n",
    "        # Tie weights again if needed\n",
    "        self.tie_weights()\n",
    "\n",
    "        return model_embeds\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        old_embeddings = self.get_input_embeddings()\n",
    "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
    "        self.set_input_embeddings(new_embeddings)\n",
    "\n",
    "        # if word embeddings are not tied, make sure that lm head is resized as well\n",
    "        if self.get_output_embeddings() is not None and not self.config.tie_word_embeddings:\n",
    "            old_lm_head = self.get_output_embeddings()\n",
    "            new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens)\n",
    "            self.set_output_embeddings(new_lm_head)\n",
    "\n",
    "        return self.get_input_embeddings()\n",
    "\n",
    "    def _get_resized_embeddings(\n",
    "        self, old_embeddings: torch.nn.Embedding, new_num_tokens: Optional[int] = None\n",
    "    ) -> torch.nn.Embedding:\n",
    "        \"\"\"\n",
    "        Build a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly\n",
    "        initialized vectors at the end. Reducing the size will remove vectors from the end\n",
    "\n",
    "        Args:\n",
    "            old_embeddings (:obj:`torch.nn.Embedding`):\n",
    "                Old embeddings to be resized.\n",
    "            new_num_tokens (:obj:`int`, `optional`):\n",
    "                New number of tokens in the embedding matrix.\n",
    "\n",
    "                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove\n",
    "                vectors from the end. If not provided or :obj:`None`, just returns a pointer to the input tokens\n",
    "                :obj:`torch.nn.Embedding`` module of the model without doing anything.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.nn.Embedding`: Pointer to the resized Embedding Module or the old Embedding Module if\n",
    "            :obj:`new_num_tokens` is :obj:`None`\n",
    "        \"\"\"\n",
    "        if new_num_tokens is None:\n",
    "            return old_embeddings\n",
    "\n",
    "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "        if old_num_tokens == new_num_tokens:\n",
    "            return old_embeddings\n",
    "\n",
    "        if not isinstance(old_embeddings, nn.Embedding):\n",
    "            raise TypeError(\n",
    "                f\"Old embeddings are of type {type(old_embeddings)}, which is not an instance of {nn.Embedding}.\"\n",
    "                f\"You should either use a different resize function or make sure that `old_embeddings` are an instance of {nn.Embedding}.\"\n",
    "            )\n",
    "\n",
    "        # Build new embeddings\n",
    "        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device)\n",
    "\n",
    "        # initialize all new embeddings (in particular added tokens)\n",
    "        self._init_weights(new_embeddings)\n",
    "\n",
    "        # Copy token embeddings from the previous weights\n",
    "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
    "        new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n",
    "\n",
    "        return new_embeddings\n",
    "\n",
    "    def _get_resized_lm_head(\n",
    "        self, old_lm_head: torch.nn.Linear, new_num_tokens: Optional[int] = None, transposed: Optional[bool] = False\n",
    "    ) -> torch.nn.Linear:\n",
    "        \"\"\"\n",
    "        Build a resized Linear Module from a provided old Linear Module. Increasing the size will add newly initialized\n",
    "        vectors at the end. Reducing the size will remove vectors from the end\n",
    "\n",
    "        Args:\n",
    "            old_lm_head (:obj:`torch.nn.Linear`):\n",
    "                Old lm head liner layer to be resized.\n",
    "            new_num_tokens (:obj:`int`, `optional`):\n",
    "                New number of tokens in the linear matrix.\n",
    "\n",
    "                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove\n",
    "                vectors from the end. If not provided or :obj:`None`, just returns a pointer to the input tokens\n",
    "                :obj:`torch.nn.Linear`` module of the model without doing anything.\n",
    "            transposed (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether ``old_lm_head`` is transposed or not. If True ``old_lm_head.size()`` is ``lm_head_dim,\n",
    "                vocab_size`` else ``vocab_size, lm_head_dim``.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.nn.Linear`: Pointer to the resized Linear Module or the old Linear Module if\n",
    "            :obj:`new_num_tokens` is :obj:`None`\n",
    "        \"\"\"\n",
    "        if new_num_tokens is None:\n",
    "            return old_lm_head\n",
    "\n",
    "        old_num_tokens, old_lm_head_dim = (\n",
    "            old_lm_head.weight.size() if not transposed else old_lm_head.weight.t().size()\n",
    "        )\n",
    "\n",
    "        if old_num_tokens == new_num_tokens:\n",
    "            return old_lm_head\n",
    "\n",
    "        if not isinstance(old_lm_head, nn.Linear):\n",
    "            raise TypeError(\n",
    "                f\"Old language model head is of type {type(old_lm_head)}, which is not an instance of {nn.Linear}.\"\n",
    "                f\"You should either use a different resize function or make sure that `old_embeddings` are an instance of {nn.Linear}.\"\n",
    "            )\n",
    "\n",
    "        # Build new lm head\n",
    "        new_lm_head_shape = (old_lm_head_dim, new_num_tokens) if not transposed else (new_num_tokens, old_lm_head_dim)\n",
    "        has_new_lm_head_bias = old_lm_head.bias is not None\n",
    "        new_lm_head = nn.Linear(*new_lm_head_shape, bias=has_new_lm_head_bias).to(self.device)\n",
    "\n",
    "        # initialize new lm head (in particular added tokens)\n",
    "        self._init_weights(new_lm_head)\n",
    "\n",
    "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
    "\n",
    "        # Copy old lm head weights to new lm head\n",
    "        if not transposed:\n",
    "            new_lm_head.weight.data[:num_tokens_to_copy, :] = old_lm_head.weight.data[:num_tokens_to_copy, :]\n",
    "        else:\n",
    "            new_lm_head.weight.data[:, :num_tokens_to_copy] = old_lm_head.weight.data[:, :num_tokens_to_copy]\n",
    "\n",
    "        # Copy bias weights to new lm head\n",
    "        if has_new_lm_head_bias:\n",
    "            new_lm_head.bias.data[:num_tokens_to_copy] = old_lm_head.bias.data[:num_tokens_to_copy]\n",
    "\n",
    "        return new_lm_head\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes and prunes weights if needed.\n",
    "        \"\"\"\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Prune heads if needed\n",
    "        if self.config.pruned_heads:\n",
    "            self.prune_heads(self.config.pruned_heads)\n",
    "\n",
    "        # Tie weights if needed\n",
    "        self.tie_weights()\n",
    "\n",
    "    def prune_heads(self, heads_to_prune: Dict[int, List[int]]):\n",
    "        \"\"\"\n",
    "        Prunes heads of the base model.\n",
    "\n",
    "        Arguments:\n",
    "            heads_to_prune (:obj:`Dict[int, List[int]]`):\n",
    "                Dictionary with keys being selected layer indices (:obj:`int`) and associated values being the list of\n",
    "                heads to prune in said layer (list of :obj:`int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads\n",
    "                0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n",
    "        \"\"\"\n",
    "        # save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            union_heads = set(self.config.pruned_heads.get(layer, [])) | set(heads)\n",
    "            self.config.pruned_heads[layer] = list(union_heads)  # Unfortunately we have to store it as list for JSON\n",
    "\n",
    "        self.base_model._prune_heads(heads_to_prune)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike]):\n",
    "        \"\"\"\n",
    "        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
    "        `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
    "\n",
    "        Arguments:\n",
    "            save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Directory to which to save. Will be created if it doesn't exist.\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_directory):\n",
    "            logger.error(\"Provided path ({}) should be a directory, not a file\".format(save_directory))\n",
    "            return\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Only save the model itself if we are using distributed training\n",
    "        model_to_save = self.module if hasattr(self, \"module\") else self\n",
    "\n",
    "        # Attach architecture to the config\n",
    "        model_to_save.config.architectures = [model_to_save.__class__.__name__]\n",
    "\n",
    "        state_dict = model_to_save.state_dict()\n",
    "\n",
    "        # Handle the case where some state_dict keys shouldn't be saved\n",
    "        if self._keys_to_ignore_on_save is not None:\n",
    "            state_dict = {k: v for k, v in state_dict.items() if k not in self._keys_to_ignore_on_save}\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
    "\n",
    "        if getattr(self.config, \"xla_device\", False) and is_torch_tpu_available():\n",
    "            import torch_xla.core.xla_model as xm\n",
    "\n",
    "            if xm.is_master_ordinal():\n",
    "                # Save configuration file\n",
    "                model_to_save.config.save_pretrained(save_directory)\n",
    "            # xm.save takes care of saving only from master\n",
    "            xm.save(state_dict, output_model_file)\n",
    "        else:\n",
    "            model_to_save.config.save_pretrained(save_directory)\n",
    "            torch.save(state_dict, output_model_file)\n",
    "\n",
    "        logger.info(\"Model weights saved in {}\".format(output_model_file))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        r\"\"\"\n",
    "        Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
    "\n",
    "        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated). To\n",
    "        train the model, you should first set it back in training mode with ``model.train()``.\n",
    "\n",
    "        The warning `Weights from XXX not initialized from pretrained model` means that the weights of XXX do not come\n",
    "        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
    "        task.\n",
    "\n",
    "        The warning `Weights from XXX not used in YYY` means that the layer XXX is not used by YYY, therefore those\n",
    "        weights are discarded.\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
    "                Can be either:\n",
    "\n",
    "                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n",
    "                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n",
    "                      a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                    - A path to a `directory` containing model weights saved using\n",
    "                      :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n",
    "                    - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In\n",
    "                      this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided\n",
    "                      as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in\n",
    "                      a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
    "                    - :obj:`None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
    "                      arguments ``config`` and ``state_dict``).\n",
    "            model_args (sequence of positional arguments, `optional`):\n",
    "                All remaning positional arguments will be passed to the underlying model's ``__init__`` method.\n",
    "            config (:obj:`Union[PretrainedConfig, str, os.PathLike]`, `optional`):\n",
    "                Can be either:\n",
    "\n",
    "                    - an instance of a class derived from :class:`~transformers.PretrainedConfig`,\n",
    "                    - a string or path valid as input to :func:`~transformers.PretrainedConfig.from_pretrained`.\n",
    "\n",
    "                Configuration for the model to use instead of an automatically loaded configuation. Configuration can\n",
    "                be automatically loaded when:\n",
    "\n",
    "                    - The model is a model provided by the library (loaded with the `model id` string of a pretrained\n",
    "                      model).\n",
    "                    - The model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded\n",
    "                      by supplying the save directory.\n",
    "                    - The model is loaded by supplying a local directory as ``pretrained_model_name_or_path`` and a\n",
    "                      configuration JSON file named `config.json` is found in the directory.\n",
    "            state_dict (:obj:`Dict[str, torch.Tensor]`, `optional`):\n",
    "                A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
    "\n",
    "                This option can be used if you want to create a model from a pretrained configuration but load your own\n",
    "                weights. In this case though, you should check if using\n",
    "                :func:`~transformers.PreTrainedModel.save_pretrained` and\n",
    "                :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n",
    "            cache_dir (:obj:`Union[str, os.PathLike]`, `optional`):\n",
    "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
    "                standard cache should not be used.\n",
    "            from_tf (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
    "                ``pretrained_model_name_or_path`` argument).\n",
    "            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
    "                cached versions if they exist.\n",
    "            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
    "                file exists.\n",
    "            proxies (:obj:`Dict[str, str], `optional`):\n",
    "                A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
    "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
    "            output_loading_info(:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
    "            local_files_only(:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to only look at local files (i.e., do not try to download the model).\n",
    "            use_auth_token (:obj:`str` or `bool`, `optional`):\n",
    "                The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
    "                generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
    "            revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
    "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
    "                git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
    "                identifier allowed by git.\n",
    "            mirror(:obj:`str`, `optional`, defaults to :obj:`None`):\n",
    "                Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
    "                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
    "                Please refer to the mirror site for more information.\n",
    "            kwargs (remaining dictionary of keyword arguments, `optional`):\n",
    "                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
    "                :obj:`output_attentions=True`). Behaves differently depending on whether a ``config`` is provided or\n",
    "                automatically loaded:\n",
    "\n",
    "                    - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the\n",
    "                      underlying model's ``__init__`` method (we assume all relevant updates to the configuration have\n",
    "                      already been done)\n",
    "                    - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class\n",
    "                      initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of\n",
    "                      ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute\n",
    "                      with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration\n",
    "                      attribute will be passed to the underlying model's ``__init__`` function.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import BertConfig, BertModel\n",
    "            >>> # Download model and configuration from huggingface.co and cache.\n",
    "            >>> model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            >>> # Model was saved using `save_pretrained('./test/saved_model/')` (for example purposes, not runnable).\n",
    "            >>> model = BertModel.from_pretrained('./test/saved_model/')\n",
    "            >>> # Update configuration during loading.\n",
    "            >>> model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "            >>> assert model.config.output_attentions == True\n",
    "            >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
    "            >>> config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n",
    "            >>> model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n",
    "        \"\"\"\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        state_dict = kwargs.pop(\"state_dict\", None)\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        from_tf = kwargs.pop(\"from_tf\", False)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
    "        revision = kwargs.pop(\"revision\", None)\n",
    "        mirror = kwargs.pop(\"mirror\", None)\n",
    "\n",
    "        # Load config if we don't provide a configuration\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            config_path = config if config is not None else pretrained_model_name_or_path\n",
    "            config, model_kwargs = cls.config_class.from_pretrained(\n",
    "                config_path,\n",
    "                *model_args,\n",
    "                cache_dir=cache_dir,\n",
    "                return_unused_kwargs=True,\n",
    "                force_download=force_download,\n",
    "                resume_download=resume_download,\n",
    "                proxies=proxies,\n",
    "                local_files_only=local_files_only,\n",
    "                use_auth_token=use_auth_token,\n",
    "                revision=revision,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            model_kwargs = kwargs\n",
    "\n",
    "        # Load model\n",
    "        if pretrained_model_name_or_path is not None:\n",
    "            pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
    "            if os.path.isdir(pretrained_model_name_or_path):\n",
    "                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")):\n",
    "                    # Load from a TF 1.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n",
    "                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n",
    "                    # Load from a TF 2.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n",
    "                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n",
    "                    # Load from a PyTorch checkpoint\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
    "                else:\n",
    "                    raise EnvironmentError(\n",
    "                        \"Error no file named {} found in directory {} or `from_tf` set to False\".format(\n",
    "                            [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + \".index\"],\n",
    "                            pretrained_model_name_or_path,\n",
    "                        )\n",
    "                    )\n",
    "            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "                archive_file = pretrained_model_name_or_path\n",
    "            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n",
    "                assert (\n",
    "                    from_tf\n",
    "                ), \"We found a TensorFlow checkpoint at {}, please set from_tf to True to load from this checkpoint\".format(\n",
    "                    pretrained_model_name_or_path + \".index\"\n",
    "                )\n",
    "                archive_file = pretrained_model_name_or_path + \".index\"\n",
    "            else:\n",
    "                archive_file = hf_bucket_url(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    filename=(TF2_WEIGHTS_NAME if from_tf else WEIGHTS_NAME),\n",
    "                    revision=revision,\n",
    "                    mirror=mirror,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                # Load from URL or cache if already cached\n",
    "                resolved_archive_file = cached_path(\n",
    "                    archive_file,\n",
    "                    cache_dir=cache_dir,\n",
    "                    force_download=force_download,\n",
    "                    proxies=proxies,\n",
    "                    resume_download=resume_download,\n",
    "                    local_files_only=local_files_only,\n",
    "                    use_auth_token=use_auth_token,\n",
    "                )\n",
    "            except EnvironmentError as err:\n",
    "                logger.error(err)\n",
    "                msg = (\n",
    "                    f\"Can't load weights for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
    "                    f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                    f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\n",
    "                )\n",
    "                raise EnvironmentError(msg)\n",
    "\n",
    "            if resolved_archive_file == archive_file:\n",
    "                logger.info(\"loading weights file {}\".format(archive_file))\n",
    "            else:\n",
    "                logger.info(\"loading weights file {} from cache at {}\".format(archive_file, resolved_archive_file))\n",
    "        else:\n",
    "            resolved_archive_file = None\n",
    "\n",
    "        config.name_or_path = pretrained_model_name_or_path\n",
    "\n",
    "        # Instantiate model.\n",
    "        model = cls(config, *model_args, **model_kwargs)\n",
    "\n",
    "        if state_dict is None and not from_tf:\n",
    "            try:\n",
    "                state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
    "            except Exception:\n",
    "                raise OSError(\n",
    "                    f\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\n",
    "                    f\"at '{resolved_archive_file}'\"\n",
    "                    \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n",
    "                )\n",
    "\n",
    "        missing_keys = []\n",
    "        unexpected_keys = []\n",
    "        error_msgs = []\n",
    "\n",
    "        if from_tf:\n",
    "            if resolved_archive_file.endswith(\".index\"):\n",
    "                # Load from a TensorFlow 1.X checkpoint - provided by original authors\n",
    "                model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the '.index'\n",
    "            else:\n",
    "                # Load from our TensorFlow 2.0 checkpoints\n",
    "                try:\n",
    "                    from .modeling_tf_pytorch_utils import load_tf2_checkpoint_in_pytorch_model\n",
    "\n",
    "                    model = load_tf2_checkpoint_in_pytorch_model(model, resolved_archive_file, allow_missing_keys=True)\n",
    "                except ImportError:\n",
    "                    logger.error(\n",
    "                        \"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see \"\n",
    "                        \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n",
    "                    )\n",
    "                    raise\n",
    "        else:\n",
    "            # Convert old format to new format if needed from a PyTorch state_dict\n",
    "            old_keys = []\n",
    "            new_keys = []\n",
    "            for key in state_dict.keys():\n",
    "                new_key = None\n",
    "                if \"gamma\" in key:\n",
    "                    new_key = key.replace(\"gamma\", \"weight\")\n",
    "                if \"beta\" in key:\n",
    "                    new_key = key.replace(\"beta\", \"bias\")\n",
    "                if new_key:\n",
    "                    old_keys.append(key)\n",
    "                    new_keys.append(new_key)\n",
    "            for old_key, new_key in zip(old_keys, new_keys):\n",
    "                state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "            # copy state_dict so _load_from_state_dict can modify it\n",
    "            metadata = getattr(state_dict, \"_metadata\", None)\n",
    "            state_dict = state_dict.copy()\n",
    "            if metadata is not None:\n",
    "                state_dict._metadata = metadata\n",
    "\n",
    "            # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n",
    "            # so we need to apply the function recursively.\n",
    "            def load(module: nn.Module, prefix=\"\"):\n",
    "                local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "                module._load_from_state_dict(\n",
    "                    state_dict,\n",
    "                    prefix,\n",
    "                    local_metadata,\n",
    "                    True,\n",
    "                    missing_keys,\n",
    "                    unexpected_keys,\n",
    "                    error_msgs,\n",
    "                )\n",
    "                for name, child in module._modules.items():\n",
    "                    if child is not None:\n",
    "                        load(child, prefix + name + \".\")\n",
    "\n",
    "            # Make sure we are able to load base models as well as derived models (with heads)\n",
    "            start_prefix = \"\"\n",
    "            model_to_load = model\n",
    "            has_prefix_module = any(s.startswith(cls.base_model_prefix) for s in state_dict.keys())\n",
    "            if not hasattr(model, cls.base_model_prefix) and has_prefix_module:\n",
    "                start_prefix = cls.base_model_prefix + \".\"\n",
    "            if hasattr(model, cls.base_model_prefix) and not has_prefix_module:\n",
    "                model_to_load = getattr(model, cls.base_model_prefix)\n",
    "\n",
    "            load(model_to_load, prefix=start_prefix)\n",
    "\n",
    "            if model.__class__.__name__ != model_to_load.__class__.__name__:\n",
    "                base_model_state_dict = model_to_load.state_dict().keys()\n",
    "                head_model_state_dict_without_base_prefix = [\n",
    "                    key.split(cls.base_model_prefix + \".\")[-1] for key in model.state_dict().keys()\n",
    "                ]\n",
    "                missing_keys.extend(head_model_state_dict_without_base_prefix - base_model_state_dict)\n",
    "\n",
    "            # Some models may have keys that are not in the state by design, removing them before needlessly warning\n",
    "            # the user.\n",
    "            if cls._keys_to_ignore_on_load_missing is not None:\n",
    "                for pat in cls._keys_to_ignore_on_load_missing:\n",
    "                    missing_keys = [k for k in missing_keys if re.search(pat, k) is None]\n",
    "\n",
    "            if cls._keys_to_ignore_on_load_unexpected is not None:\n",
    "                for pat in cls._keys_to_ignore_on_load_unexpected:\n",
    "                    unexpected_keys = [k for k in unexpected_keys if re.search(pat, k) is None]\n",
    "\n",
    "            if len(unexpected_keys) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when \"\n",
    "                    f\"initializing {model.__class__.__name__}: {unexpected_keys}\\n\"\n",
    "                    f\"- This IS expected if you are initializing {model.__class__.__name__} from the checkpoint of a model trained on another task \"\n",
    "                    f\"or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\\n\"\n",
    "                    f\"- This IS NOT expected if you are initializing {model.__class__.__name__} from the checkpoint of a model that you expect \"\n",
    "                    f\"to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n",
    "            if len(missing_keys) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at {pretrained_model_name_or_path} \"\n",
    "                    f\"and are newly initialized: {missing_keys}\\n\"\n",
    "                    f\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at {pretrained_model_name_or_path}.\\n\"\n",
    "                    f\"If your task is similar to the task the model of the checkpoint was trained on, \"\n",
    "                    f\"you can already use {model.__class__.__name__} for predictions without further training.\"\n",
    "                )\n",
    "            if len(error_msgs) > 0:\n",
    "                raise RuntimeError(\n",
    "                    \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n",
    "                        model.__class__.__name__, \"\\n\\t\".join(error_msgs)\n",
    "                    )\n",
    "                )\n",
    "        # make sure token embedding weights are still tied if needed\n",
    "        model.tie_weights()\n",
    "\n",
    "        # Set model in evaluation mode to deactivate DropOut modules by default\n",
    "        model.eval()\n",
    "\n",
    "        if output_loading_info:\n",
    "            loading_info = {\n",
    "                \"missing_keys\": missing_keys,\n",
    "                \"unexpected_keys\": unexpected_keys,\n",
    "                \"error_msgs\": error_msgs,\n",
    "            }\n",
    "            return model, loading_info\n",
    "\n",
    "        if hasattr(config, \"xla_device\") and config.xla_device and is_torch_tpu_available():\n",
    "            import torch_xla.core.xla_model as xm\n",
    "\n",
    "            model = xm.send_cpu_data_to_device(model, xm.xla_device())\n",
    "            model.to(xm.xla_device())\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n",
    "\n",
    "    Basically works like a linear layer but the weights are transposed.\n",
    "\n",
    "    Args:\n",
    "        nf (:obj:`int`): The number of output features.\n",
    "        nx (:obj:`int`): The number of input features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nf, nx):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(*size_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolerStartLogits(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute SQuAD start logits from sequence hidden states.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model, will be used to grab the :obj:`hidden_size` of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.FloatTensor, p_mask: Optional[torch.FloatTensor] = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):\n",
    "                The final hidden states of the model.\n",
    "            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):\n",
    "                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n",
    "                should be masked.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor`: The start logits for SQuAD.\n",
    "        \"\"\"\n",
    "        x = self.dense(hidden_states).squeeze(-1)\n",
    "\n",
    "        if p_mask is not None:\n",
    "            if next(self.parameters()).dtype == torch.float16:\n",
    "                x = x * (1 - p_mask) - 65500 * p_mask\n",
    "            else:\n",
    "                x = x * (1 - p_mask) - 1e30 * p_mask\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolerEndLogits(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute SQuAD end logits from sequence hidden states.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model, will be used to grab the :obj:`hidden_size` of the model and the\n",
    "            :obj:`layer_norm_eps` to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        start_states: Optional[torch.FloatTensor] = None,\n",
    "        start_positions: Optional[torch.LongTensor] = None,\n",
    "        p_mask: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):\n",
    "                The final hidden states of the model.\n",
    "            start_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`, `optional`):\n",
    "                The hidden states of the first tokens for the labeled span.\n",
    "            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                The position of the first token for the labeled span.\n",
    "            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):\n",
    "                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n",
    "                should be masked.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            One of ``start_states`` or ``start_positions`` should be not obj:`None`. If both are set,\n",
    "            ``start_positions`` overrides ``start_states``.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor`: The end logits for SQuAD.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            start_states is not None or start_positions is not None\n",
    "        ), \"One of start_states, start_positions should be not None\"\n",
    "        if start_positions is not None:\n",
    "            slen, hsz = hidden_states.shape[-2:]\n",
    "            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n",
    "            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n",
    "            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n",
    "\n",
    "        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n",
    "        x = self.activation(x)\n",
    "        x = self.LayerNorm(x)\n",
    "        x = self.dense_1(x).squeeze(-1)\n",
    "\n",
    "        if p_mask is not None:\n",
    "            if next(self.parameters()).dtype == torch.float16:\n",
    "                x = x * (1 - p_mask) - 65500 * p_mask\n",
    "            else:\n",
    "                x = x * (1 - p_mask) - 1e30 * p_mask\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolerAnswerClass(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute SQuAD 2.0 answer class from classification and start tokens hidden states.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model, will be used to grab the :obj:`hidden_size` of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        start_states: Optional[torch.FloatTensor] = None,\n",
    "        start_positions: Optional[torch.LongTensor] = None,\n",
    "        cls_index: Optional[torch.LongTensor] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):\n",
    "                The final hidden states of the model.\n",
    "            start_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`, `optional`):\n",
    "                The hidden states of the first tokens for the labeled span.\n",
    "            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                The position of the first token for the labeled span.\n",
    "            cls_index (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Position of the CLS token for each sentence in the batch. If :obj:`None`, takes the last token.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            One of ``start_states`` or ``start_positions`` should be not obj:`None`. If both are set,\n",
    "            ``start_positions`` overrides ``start_states``.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor`: The SQuAD 2.0 answer class.\n",
    "        \"\"\"\n",
    "        # No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.\n",
    "        hsz = hidden_states.shape[-1]\n",
    "        assert (\n",
    "            start_states is not None or start_positions is not None\n",
    "        ), \"One of start_states, start_positions should be not None\"\n",
    "        if start_positions is not None:\n",
    "            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n",
    "            start_states = hidden_states.gather(-2, start_positions).squeeze(-2)  # shape (bsz, hsz)\n",
    "\n",
    "        if cls_index is not None:\n",
    "            cls_index = cls_index[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n",
    "            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, hsz)\n",
    "        else:\n",
    "            cls_token_state = hidden_states[:, -1, :]  # shape (bsz, hsz)\n",
    "\n",
    "        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_1(x).squeeze(-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SquadHeadOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of question answering models using a :class:`~transformers.modeling_utils.SQuADHead`.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned if both :obj:`start_positions` and :obj:`end_positions` are provided):\n",
    "            Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n",
    "            losses.\n",
    "        start_top_log_probs (``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n",
    "        start_top_index (``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Indices for the top config.start_n_top start token possibilities (beam-search).\n",
    "        end_top_log_probs (``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities\n",
    "            (beam-search).\n",
    "        end_top_index (``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n",
    "        cls_logits (``torch.FloatTensor`` of shape ``(batch_size,)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Log probabilities for the ``is_impossible`` label of the answers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    start_top_log_probs: Optional[torch.FloatTensor] = None\n",
    "    start_top_index: Optional[torch.LongTensor] = None\n",
    "    end_top_log_probs: Optional[torch.FloatTensor] = None\n",
    "    end_top_index: Optional[torch.LongTensor] = None\n",
    "    cls_logits: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class SQuADHead(nn.Module):\n",
    "    r\"\"\"\n",
    "    A SQuAD head inspired by XLNet.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model, will be used to grab the :obj:`hidden_size` of the model and the\n",
    "            :obj:`layer_norm_eps` to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.start_n_top = config.start_n_top\n",
    "        self.end_n_top = config.end_n_top\n",
    "\n",
    "        self.start_logits = PoolerStartLogits(config)\n",
    "        self.end_logits = PoolerEndLogits(config)\n",
    "        self.answer_class = PoolerAnswerClass(config)\n",
    "\n",
    "    @replace_return_docstrings(output_type=SquadHeadOutput, config_class=PretrainedConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        start_positions: Optional[torch.LongTensor] = None,\n",
    "        end_positions: Optional[torch.LongTensor] = None,\n",
    "        cls_index: Optional[torch.LongTensor] = None,\n",
    "        is_impossible: Optional[torch.LongTensor] = None,\n",
    "        p_mask: Optional[torch.FloatTensor] = None,\n",
    "        return_dict: bool = False,\n",
    "    ) -> Union[SquadHeadOutput, Tuple[torch.FloatTensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):\n",
    "                Final hidden states of the model on the sequence tokens.\n",
    "            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Positions of the first token for the labeled span.\n",
    "            end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Positions of the last token for the labeled span.\n",
    "            cls_index (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Position of the CLS token for each sentence in the batch. If :obj:`None`, takes the last token.\n",
    "            is_impossible (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Whether the question has a possible answer in the paragraph or not.\n",
    "            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):\n",
    "                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n",
    "                should be masked.\n",
    "            return_dict (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n",
    "            for x in (start_positions, end_positions, cls_index, is_impossible):\n",
    "                if x is not None and x.dim() > 1:\n",
    "                    x.squeeze_(-1)\n",
    "\n",
    "            # during training, compute the end logits based on the ground truth of the start position\n",
    "            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "            if cls_index is not None and is_impossible is not None:\n",
    "                # Predict answerability from the representation of CLS and START\n",
    "                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n",
    "                loss_fct_cls = nn.BCEWithLogitsLoss()\n",
    "                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n",
    "\n",
    "                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n",
    "                total_loss += cls_loss * 0.5\n",
    "\n",
    "            return SquadHeadOutput(loss=total_loss) if return_dict else (total_loss,)\n",
    "\n",
    "        else:\n",
    "            # during inference, compute the end logits based on beam search\n",
    "            bsz, slen, hsz = hidden_states.size()\n",
    "            start_log_probs = F.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n",
    "\n",
    "            start_top_log_probs, start_top_index = torch.topk(\n",
    "                start_log_probs, self.start_n_top, dim=-1\n",
    "            )  # shape (bsz, start_n_top)\n",
    "            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, start_n_top, hsz)\n",
    "            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, start_n_top, hsz)\n",
    "            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, start_n_top, hsz)\n",
    "\n",
    "            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n",
    "                start_states\n",
    "            )  # shape (bsz, slen, start_n_top, hsz)\n",
    "            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n",
    "            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n",
    "            end_log_probs = F.softmax(end_logits, dim=1)  # shape (bsz, slen, start_n_top)\n",
    "\n",
    "            end_top_log_probs, end_top_index = torch.topk(\n",
    "                end_log_probs, self.end_n_top, dim=1\n",
    "            )  # shape (bsz, end_n_top, start_n_top)\n",
    "            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n",
    "            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n",
    "\n",
    "            start_states = torch.einsum(\"blh,bl->bh\", hidden_states, start_log_probs)\n",
    "            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n",
    "\n",
    "            if not return_dict:\n",
    "                return (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits)\n",
    "            else:\n",
    "                return SquadHeadOutput(\n",
    "                    start_top_log_probs=start_top_log_probs,\n",
    "                    start_top_index=start_top_index,\n",
    "                    end_top_log_probs=end_top_log_probs,\n",
    "                    end_top_index=end_top_index,\n",
    "                    cls_logits=cls_logits,\n",
    "                )\n",
    "\n",
    "\n",
    "class SequenceSummary(nn.Module):\n",
    "    r\"\"\"\n",
    "    Compute a single vector summary of a sequence hidden states.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n",
    "            config class of your model for the default values it uses):\n",
    "\n",
    "            - **summary_type** (:obj:`str`) -- The method to use to make this summary. Accepted values are:\n",
    "\n",
    "                - :obj:`\"last\"` -- Take the last token hidden state (like XLNet)\n",
    "                - :obj:`\"first\"` -- Take the first token hidden state (like Bert)\n",
    "                - :obj:`\"mean\"` -- Take the mean of all tokens hidden states\n",
    "                - :obj:`\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n",
    "                - :obj:`\"attn\"` -- Not implemented now, use multi-head attention\n",
    "\n",
    "            - **summary_use_proj** (:obj:`bool`) -- Add a projection after the vector extraction.\n",
    "            - **summary_proj_to_labels** (:obj:`bool`) -- If :obj:`True`, the projection outputs to\n",
    "              :obj:`config.num_labels` classes (otherwise to :obj:`config.hidden_size`).\n",
    "            - **summary_activation** (:obj:`Optional[str]`) -- Set to :obj:`\"tanh\"` to add a tanh activation to the\n",
    "              output, another string or :obj:`None` will add no activation.\n",
    "            - **summary_first_dropout** (:obj:`float`) -- Optional dropout probability before the projection and\n",
    "              activation.\n",
    "            - **summary_last_dropout** (:obj:`float`)-- Optional dropout probability after the projection and\n",
    "              activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.summary_type = getattr(config, \"summary_type\", \"last\")\n",
    "        if self.summary_type == \"attn\":\n",
    "            # We should use a standard multi-head attention module with absolute positional embedding for that.\n",
    "            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n",
    "            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.summary = Identity()\n",
    "        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
    "            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
    "                num_classes = config.num_labels\n",
    "            else:\n",
    "                num_classes = config.hidden_size\n",
    "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "        activation_string = getattr(config, \"summary_activation\", None)\n",
    "        self.activation: Callable = get_activation(activation_string) if activation_string else Identity()\n",
    "\n",
    "        self.first_dropout = Identity()\n",
    "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
    "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
    "\n",
    "        self.last_dropout = Identity()\n",
    "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
    "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Compute a single vector summary of a sequence hidden states.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`[batch_size, seq_len, hidden_size]`):\n",
    "                The hidden states of the last layer.\n",
    "            cls_index (:obj:`torch.LongTensor` of shape :obj:`[batch_size]` or :obj:`[batch_size, ...]` where ... are optional leading dimensions of :obj:`hidden_states`, `optional`):\n",
    "                Used if :obj:`summary_type == \"cls_index\"` and takes the last token of the sequence as classification\n",
    "                token.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor`: The summary of the sequence hidden states.\n",
    "        \"\"\"\n",
    "        if self.summary_type == \"last\":\n",
    "            output = hidden_states[:, -1]\n",
    "        elif self.summary_type == \"first\":\n",
    "            output = hidden_states[:, 0]\n",
    "        elif self.summary_type == \"mean\":\n",
    "            output = hidden_states.mean(dim=1)\n",
    "        elif self.summary_type == \"cls_index\":\n",
    "            if cls_index is None:\n",
    "                cls_index = torch.full_like(\n",
    "                    hidden_states[..., :1, :],\n",
    "                    hidden_states.shape[-2] - 1,\n",
    "                    dtype=torch.long,\n",
    "                )\n",
    "            else:\n",
    "                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n",
    "                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n",
    "            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n",
    "            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n",
    "        elif self.summary_type == \"attn\":\n",
    "            raise NotImplementedError\n",
    "\n",
    "        output = self.first_dropout(output)\n",
    "        output = self.summary(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.last_dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def prune_linear_layer(layer: torch.nn.Linear, index: torch.LongTensor, dim: int = 0) -> torch.nn.Linear:\n",
    "    \"\"\"\n",
    "    Prune a linear layer to keep only entries in index.\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (:obj:`torch.nn.Linear`): The layer to prune.\n",
    "        index (:obj:`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (:obj:`int`, `optional`, defaults to 0): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`torch.nn.Linear`: The pruned layer as a new layer with :obj:`requires_grad=True`.\n",
    "    \"\"\"\n",
    "    index = index.to(layer.weight.device)\n",
    "    W = layer.weight.index_select(dim, index).clone().detach()\n",
    "    if layer.bias is not None:\n",
    "        if dim == 1:\n",
    "            b = layer.bias.clone().detach()\n",
    "        else:\n",
    "            b = layer.bias[index].clone().detach()\n",
    "    new_size = list(layer.weight.size())\n",
    "    new_size[dim] = len(index)\n",
    "    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n",
    "    new_layer.weight.requires_grad = False\n",
    "    new_layer.weight.copy_(W.contiguous())\n",
    "    new_layer.weight.requires_grad = True\n",
    "    if layer.bias is not None:\n",
    "        new_layer.bias.requires_grad = False\n",
    "        new_layer.bias.copy_(b.contiguous())\n",
    "        new_layer.bias.requires_grad = True\n",
    "    return new_layer\n",
    "\n",
    "\n",
    "def prune_conv1d_layer(layer: Conv1D, index: torch.LongTensor, dim: int = 1) -> Conv1D:\n",
    "    \"\"\"\n",
    "    Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear layer (see e.g. BERT) but the weights\n",
    "    are transposed.\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (:class:`~transformers.modeling_utils.Conv1D`): The layer to prune.\n",
    "        index (:obj:`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (:obj:`int`, `optional`, defaults to 1): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        :class:`~transformers.modeling_utils.Conv1D`: The pruned layer as a new layer with :obj:`requires_grad=True`.\n",
    "    \"\"\"\n",
    "    index = index.to(layer.weight.device)\n",
    "    W = layer.weight.index_select(dim, index).clone().detach()\n",
    "    if dim == 0:\n",
    "        b = layer.bias.clone().detach()\n",
    "    else:\n",
    "        b = layer.bias[index].clone().detach()\n",
    "    new_size = list(layer.weight.size())\n",
    "    new_size[dim] = len(index)\n",
    "    new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n",
    "    new_layer.weight.requires_grad = False\n",
    "    new_layer.weight.copy_(W.contiguous())\n",
    "    new_layer.weight.requires_grad = True\n",
    "    new_layer.bias.requires_grad = False\n",
    "    new_layer.bias.copy_(b.contiguous())\n",
    "    new_layer.bias.requires_grad = True\n",
    "    return new_layer\n",
    "\n",
    "\n",
    "def prune_layer(\n",
    "    layer: Union[torch.nn.Linear, Conv1D], index: torch.LongTensor, dim: Optional[int] = None\n",
    ") -> Union[torch.nn.Linear, Conv1D]:\n",
    "    \"\"\"\n",
    "    Prune a Conv1D or linear layer to keep only entries in index.\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (:obj:`Union[torch.nn.Linear, Conv1D]`): The layer to prune.\n",
    "        index (:obj:`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (:obj:`int`, `optional`): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`torch.nn.Linear` or :class:`~transformers.modeling_utils.Conv1D`: The pruned layer as a new layer with\n",
    "        :obj:`requires_grad=True`.\n",
    "    \"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n",
    "    elif isinstance(layer, Conv1D):\n",
    "        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n",
    "    else:\n",
    "        raise ValueError(\"Can't prune layer of class {}\".format(layer.__class__))\n",
    "\n",
    "\n",
    "def apply_chunking_to_forward(\n",
    "    forward_fn: Callable[..., torch.Tensor], chunk_size: int, chunk_dim: int, *input_tensors\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function chunks the :obj:`input_tensors` into smaller input tensor parts of size :obj:`chunk_size` over the\n",
    "    dimension :obj:`chunk_dim`. It then applies a layer :obj:`forward_fn` to each chunk independently to save memory.\n",
    "\n",
    "    If the :obj:`forward_fn` is independent across the :obj:`chunk_dim` this function will yield the same result as\n",
    "    directly applying :obj:`forward_fn` to :obj:`input_tensors`.\n",
    "\n",
    "    Args:\n",
    "        forward_fn (:obj:`Callable[..., torch.Tensor]`):\n",
    "            The forward function of the model.\n",
    "        chunk_size (:obj:`int`):\n",
    "            The chunk size of a chunked tensor: :obj:`num_chunks = len(input_tensors[0]) / chunk_size`.\n",
    "        chunk_dim (:obj:`int`):\n",
    "            The dimension over which the :obj:`input_tensors` should be chunked.\n",
    "        input_tensors (:obj:`Tuple[torch.Tensor]`):\n",
    "            The input tensors of ``forward_fn`` which will be chunked\n",
    "\n",
    "    Returns:\n",
    "        :obj:`torch.Tensor`: A tensor with the same shape as the :obj:`forward_fn` would have given if applied`.\n",
    "\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        # rename the usual forward() fn to forward_chunk()\n",
    "        def forward_chunk(self, hidden_states):\n",
    "            hidden_states = self.decoder(hidden_states)\n",
    "            return hidden_states\n",
    "\n",
    "        # implement a chunked forward function\n",
    "        def forward(self, hidden_states):\n",
    "            return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(input_tensors) > 0, \"{} has to be a tuple/list of tensors\".format(input_tensors)\n",
    "    tensor_shape = input_tensors[0].shape[chunk_dim]\n",
    "    assert all(\n",
    "        input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
    "    ), \"All input tenors have to be of the same shape\"\n",
    "\n",
    "    # inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\n",
    "    num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)\n",
    "    assert num_args_in_forward_chunk_fn == len(\n",
    "        input_tensors\n",
    "    ), \"forward_chunk_fn expects {} arguments, but only {} input tensors are given\".format(\n",
    "        num_args_in_forward_chunk_fn, len(input_tensors)\n",
    "    )\n",
    "\n",
    "    if chunk_size > 0:\n",
    "        assert (\n",
    "            input_tensors[0].shape[chunk_dim] % chunk_size == 0\n",
    "        ), \"The dimension to be chunked {} has to be a multiple of the chunk size {}\".format(\n",
    "            input_tensors[0].shape[chunk_dim], chunk_size\n",
    "        )\n",
    "\n",
    "        num_chunks = input_tensors[0].shape[chunk_dim] // chunk_size\n",
    "\n",
    "        # chunk input tensor into tuples\n",
    "        input_tensors_chunks = tuple(input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors)\n",
    "        # apply forward fn to every tuple\n",
    "        output_chunks = tuple(forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks))\n",
    "        # concatenate output at same dimension\n",
    "        return torch.cat(output_chunks, dim=chunk_dim)\n",
    "\n",
    "    return forward_fn(*input_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:50:20.073225Z",
     "start_time": "2021-02-10T14:50:19.926925Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"PyTorch BERT model. \"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "# from ...activations import ACT2FN\n",
    "# from ...file_utils import (\n",
    "#     ModelOutput,\n",
    "#     add_code_sample_docstrings,\n",
    "#     add_start_docstrings,\n",
    "#     add_start_docstrings_to_model_forward,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "\n",
    "# from ...modeling_outputs import (\n",
    "#     BaseModelOutputWithPastAndCrossAttentions,\n",
    "#     BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "#     CausalLMOutputWithCrossAttentions,\n",
    "#     MaskedLMOutput,\n",
    "#     MultipleChoiceModelOutput,\n",
    "#     NextSentencePredictorOutput,\n",
    "#     QuestionAnsweringModelOutput,\n",
    "#     SequenceClassifierOutput,\n",
    "#     TokenClassifierOutput,\n",
    "# )\n",
    "# from ...modeling_utils import (\n",
    "#     PreTrainedModel,\n",
    "#     apply_chunking_to_forward,\n",
    "#     find_pruneable_heads_and_indices,\n",
    "#     prune_linear_layer,\n",
    "# )\n",
    "\n",
    "\n",
    "# from ...utils import logging\n",
    "# from .configuration_bert import BertConfig\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"BertConfig\"\n",
    "_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n",
    "\n",
    "BERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"bert-large-uncased\",\n",
    "    \"bert-base-cased\",\n",
    "    \"bert-large-cased\",\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"bert-base-chinese\",\n",
    "    \"bert-base-german-cased\",\n",
    "    \"bert-large-uncased-whole-word-masking\",\n",
    "    \"bert-large-cased-whole-word-masking\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n",
    "    \"bert-base-cased-finetuned-mrpc\",\n",
    "    \"bert-base-german-dbmdz-cased\",\n",
    "    \"bert-base-german-dbmdz-uncased\",\n",
    "    \"cl-tohoku/bert-base-japanese\",\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "    \"cl-tohoku/bert-base-japanese-char\",\n",
    "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\",\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n",
    "    \"wietsedv/bert-base-dutch-cased\",\n",
    "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
    "]\n",
    "\n",
    "\n",
    "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
    "    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n",
    "    try:\n",
    "        import re\n",
    "\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        logger.error(\n",
    "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
    "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
    "        )\n",
    "        raise\n",
    "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
    "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
    "    # Load weights from TF model\n",
    "    init_vars = tf.train.list_variables(tf_path)\n",
    "    names = []\n",
    "    arrays = []\n",
    "    for name, shape in init_vars:\n",
    "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
    "        array = tf.train.load_variable(tf_path, name)\n",
    "        names.append(name)\n",
    "        arrays.append(array)\n",
    "\n",
    "    for name, array in zip(names, arrays):\n",
    "        name = name.split(\"/\")\n",
    "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
    "        # which are not required for using pretrained model\n",
    "        if any(\n",
    "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
    "            for n in name\n",
    "        ):\n",
    "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "            continue\n",
    "        pointer = model\n",
    "        for m_name in name:\n",
    "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
    "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
    "            else:\n",
    "                scope_names = [m_name]\n",
    "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
    "                pointer = getattr(pointer, \"bias\")\n",
    "            elif scope_names[0] == \"output_weights\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"squad\":\n",
    "                pointer = getattr(pointer, \"classifier\")\n",
    "            else:\n",
    "                try:\n",
    "                    pointer = getattr(pointer, scope_names[0])\n",
    "                except AttributeError:\n",
    "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "                    continue\n",
    "            if len(scope_names) >= 2:\n",
    "                num = int(scope_names[1])\n",
    "                pointer = pointer[num]\n",
    "        if m_name[-11:] == \"_embeddings\":\n",
    "            pointer = getattr(pointer, \"weight\")\n",
    "        elif m_name == \"kernel\":\n",
    "            array = np.transpose(array)\n",
    "        try:\n",
    "            assert (\n",
    "                pointer.shape == array.shape\n",
    "            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n",
    "        except AssertionError as e:\n",
    "            e.args += (pointer.shape, array.shape)\n",
    "            raise\n",
    "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
    "        pointer.data = torch.from_numpy(array)\n",
    "    return model\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
    "            self.crossattention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            assert hasattr(\n",
    "                self, \"crossattention\"\n",
    "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warn(\n",
    "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
    "                        \"`use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "\n",
    "class BertOnlyNSPHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score\n",
    "\n",
    "\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score\n",
    "\n",
    "\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BertForPreTrainingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Output type of :class:`~transformers.BertForPreTraining`.\n",
    "\n",
    "    Args:\n",
    "        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    prediction_logits: torch.FloatTensor = None\n",
    "    seq_relationship_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "BERT_START_DOCSTRING = r\"\"\"\n",
    "\n",
    "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
    "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
    "    pruning heads etc.)\n",
    "\n",
    "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
    "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
    "    general usage and behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
    "            weights.\n",
    "\"\"\"\n",
    "\n",
    "BERT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
    "            1]``:\n",
    "\n",
    "            - 0 corresponds to a `sentence A` token,\n",
    "            - 1 corresponds to a `sentence B` token.\n",
    "\n",
    "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
    "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
    "            config.max_position_embeddings - 1]``.\n",
    "\n",
    "            `What are position IDs? <../glossary.html#position-ids>`_\n",
    "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
    "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
    "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
    "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
    "            vectors than the model's internal embedding lookup matrix.\n",
    "        output_attentions (:obj:`bool`, `optional`):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (:obj:`bool`, `optional`):\n",
    "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (:obj:`bool`, `optional`):\n",
    "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
    "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
    "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
    "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
    "    input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            batch_size, seq_length = input_shape\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n",
    "    sentence prediction (classification)` head.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForPreTraining(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        next_sentence_label=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
    "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
    "        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n",
    "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
    "            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n",
    "\n",
    "            - 0 indicates sequence B is a continuation of sequence A,\n",
    "            - 1 indicates sequence B is a random sequence.\n",
    "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
    "            Used to hide legacy arguments that have been deprecated.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> from transformers import BertTokenizer, BertForPreTraining\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "            >>> outputs = model(**inputs)\n",
    "\n",
    "            >>> prediction_logits = outputs.prediction_logits\n",
    "            >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return BertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", BERT_START_DOCSTRING\n",
    ")\n",
    "class BertLMHeadModel(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if not config.is_decoder:\n",
    "            logger.warning(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
    "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
    "            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "            >>> config.is_decoder = True\n",
    "            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n",
    "\n",
    "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "            >>> outputs = model(**inputs)\n",
    "\n",
    "            >>> prediction_logits = outputs.logits\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if labels is not None:\n",
    "            use_cache = False\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
    "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
    "        return reordered_past\n",
    "\n",
    "\n",
    "@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\", BERT_START_DOCSTRING)\n",
    "class BertForMaskedLM(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.is_decoder:\n",
    "            logger.warning(\n",
    "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "                \"bi-directional self-attention.\"\n",
    "            )\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=MaskedLMOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
    "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        effective_batch_size = input_shape[0]\n",
    "\n",
    "        #  add a dummy token\n",
    "        assert self.config.pad_token_id is not None, \"The PAD token should be defined for generation\"\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n",
    "        dummy_token = torch.full(\n",
    "            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top. \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyNSPHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
    "            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n",
    "\n",
    "            - 0 indicates sequence B is a continuation of sequence A,\n",
    "            - 1 indicates sequence B is a random sequence.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            >>> model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "            >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "            >>> encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "            >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "            >>> logits = outputs.logits\n",
    "            >>> assert logits[0, 0] < logits[0, 1] # next sentence was random\n",
    "        \"\"\"\n",
    "\n",
    "        if \"next_sentence_label\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            labels = kwargs.pop(\"next_sentence_label\")\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        seq_relationship_scores = self.cls(pooled_output)\n",
    "\n",
    "        next_sentence_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (seq_relationship_scores,) + outputs[2:]\n",
    "            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n",
    "\n",
    "        return NextSentencePredictorOutput(\n",
    "            loss=next_sentence_loss,\n",
    "            logits=seq_relationship_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
    "    output) e.g. for GLUE tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=SequenceClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
    "    softmax) e.g. for RocStories/SWAG tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForMultipleChoice(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
    "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
    "            :obj:`input_ids` above)\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
    "\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
    "        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
    "        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
    "        inputs_embeds = (\n",
    "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
    "            if inputs_embeds is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        reshaped_logits = logits.view(-1, num_choices)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(reshaped_logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (reshaped_logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return MultipleChoiceModelOutput(\n",
    "            loss=loss,\n",
    "            logits=reshaped_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
    "    Named-Entity-Recognition (NER) tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=TokenClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
    "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:50:23.734928Z",
     "start_time": "2021-02-10T14:50:22.162105Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"/Users/yongdeng/Downloads/中文预训练语言模型/chinese-roberta-wwm-ext/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:50:59.144891Z",
     "start_time": "2021-02-10T14:50:59.105462Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"/Users/yongdeng/Downloads/中文预训练语言模型/chinese-roberta-wwm-ext/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
